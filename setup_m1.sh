#!/bin/bash
# M1 MacBook Setup Script - OptimalizovanÃ½ pro Llama 3.1 8B na MacBook Air M1
# Tento skript nastavÃ­ pokroÄilÃ© lokÃ¡lnÃ­ AI vÃ½vojovÃ© prostÅ™edÃ­ s vysoce kvalitnÃ­m modelem

set -e

echo "ğŸ M1 MacBook Air Research Tool Setup - Llama 3.1 8B Edition"
echo "============================================================"

# Kontrola M1 architektury
if [[ $(uname -m) != "arm64" ]]; then
    echo "âš ï¸  VarovÃ¡nÃ­: Tento skript je optimalizovÃ¡n pro M1 MacBook (arm64)"
    echo "   PokraÄuji, ale optimalizace nemusÃ­ bÃ½t ideÃ¡lnÃ­..."
fi

# Kontrola RAM (dÅ¯leÅ¾itÃ© pro 8B model)
total_ram=$(sysctl -n hw.memsize)
total_ram_gb=$((total_ram / 1024 / 1024 / 1024))

echo "ğŸ” Kontroluji systÃ©m..."
echo "   OS: $(sw_vers -productName) $(sw_vers -productVersion)"
echo "   Architektura: $(uname -m)"
echo "   RAM: ${total_ram_gb}GB"

if [ $total_ram_gb -lt 8 ]; then
    echo "âš ï¸  VAROVÃNÃ: DetekovÃ¡na RAM mÃ©nÄ› neÅ¾ 8GB!"
    echo "   Llama 3.1 8B mÅ¯Å¾e bÃ½t pomalÃ½. DoporuÄuji Phi-3 Mini pro vÃ¡Å¡ systÃ©m."
    read -p "   PokraÄovat s Llama 3.1 8B? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo "   Upravuji setup pro Phi-3 Mini..."
        USE_PHI3_MINI=true
    fi
fi

# Kontrola a instalace Homebrew
if ! command -v brew &> /dev/null; then
    echo "ğŸ“¦ Instaluji Homebrew..."
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
else
    echo "âœ… Homebrew je nainstalovÃ¡n"
fi

# Aktualizace Homebrew
echo "ğŸ”„ Aktualizuji Homebrew..."
brew update

# Kontrola a instalace Python 3.11+ (optimÃ¡lnÃ­ pro M1)
if ! python3.11 --version &> /dev/null; then
    echo "ğŸ Instaluji Python 3.11 (optimalizovanÃ½ pro M1)..."
    brew install python@3.11
else
    echo "âœ… Python 3.11 je nainstalovÃ¡n"
fi

# VytvoÅ™enÃ­ virtuÃ¡lnÃ­ho prostÅ™edÃ­
echo "ğŸ“ VytvÃ¡Å™Ã­m virtuÃ¡lnÃ­ prostÅ™edÃ­..."
python3.11 -m venv venv_llama
source venv_llama/bin/activate

echo "âœ… VirtuÃ¡lnÃ­ prostÅ™edÃ­ aktivovÃ¡no"

# Upgrade pip pro M1
echo "â¬†ï¸  Upgraduju pip..."
pip install --upgrade pip

# Instalace M1 optimalizovanÃ½ch balÃ­ÄkÅ¯
echo "ğŸš€ Instaluji M1 optimalizovanÃ© balÃ­Äky..."
pip install -r requirements.txt

# Instalace a setup Ollama
echo "ğŸ§  Nastavuji Ollama pro pokroÄilÃ© lokÃ¡lnÃ­ AI..."

if ! command -v ollama &> /dev/null; then
    echo "ğŸ“¥ Stahuji a instaluji Ollama..."
    brew install ollama
else
    echo "âœ… Ollama je nainstalovÃ¡n"
fi

# SpuÅ¡tÄ›nÃ­ Ollama serveru na pozadÃ­
echo "ğŸ”„ SpouÅ¡tÃ­m Ollama server..."
brew services start ollama

# Chvilku poÄkÃ¡me na start serveru
sleep 5

# StaÅ¾enÃ­ modelÅ¯ podle konfigurace
if [ "$USE_PHI3_MINI" = true ]; then
    echo "â¬‡ï¸  Stahuji Phi-3 Mini (optimalizovanÃ½ pro niÅ¾Å¡Ã­ RAM)..."
    ollama pull phi3:mini
    PRIMARY_MODEL="phi3:mini"
    FALLBACK_MODEL="phi3:mini"
else
    echo "â¬‡ï¸  Stahuji Llama 3.1 8B (high-quality model)..."
    ollama pull llama3.1:8b

    echo "â¬‡ï¸  Stahuji Phi-3 Mini jako fallback..."
    ollama pull phi3:mini

    echo "â¬‡ï¸  Stahuji Qwen2 7B pro kÃ³dovÃ¡nÃ­..."
    ollama pull qwen2:7b

    PRIMARY_MODEL="llama3.1:8b"
    FALLBACK_MODEL="phi3:mini"
fi

# StaÅ¾enÃ­ embedding modelu
echo "ğŸ“Š Stahuji embedding model..."
ollama pull nomic-embed-text

# Test primÃ¡rnÃ­ho modelu
echo "ğŸ§ª Testuji ${PRIMARY_MODEL}..."
if ollama run $PRIMARY_MODEL "Ahoj! Jsi pÅ™ipraven na pokroÄilÃ½ research?" --timeout 60s; then
    echo "âœ… ${PRIMARY_MODEL} funguje sprÃ¡vnÄ›!"
else
    echo "âš ï¸  ${PRIMARY_MODEL} moÅ¾nÃ¡ nefunguje sprÃ¡vnÄ›"
    if [ "$PRIMARY_MODEL" != "phi3:mini" ]; then
        echo "   ZkouÅ¡Ã­m fallback na Phi-3 Mini..."
        if ollama run phi3:mini "Test fallback" --timeout 30s; then
            echo "âœ… Fallback na Phi-3 Mini funguje"
        fi
    fi
fi

# VytvoÅ™enÃ­ optimalizovanÃ© konfigurace pro Llama 3.1 8B
echo "âš™ï¸  VytvÃ¡Å™Ã­m Llama 3.1 8B optimalizovanou konfiguraci..."
cat > .env << EOF
# M1 MacBook Optimized Configuration - Llama 3.1 8B Edition
ENVIRONMENT=development

# Ollama Configuration for Llama 3.1 8B
OLLAMA_HOST=http://localhost:11434
PRIMARY_MODEL=$PRIMARY_MODEL
FALLBACK_MODEL=$FALLBACK_MODEL
CODE_MODEL=qwen2:7b
EMBEDDING_MODEL=nomic-embed-text

# Enhanced Memory Optimizations for 8B Model
MAX_CONTEXT_LENGTH=4096
MAX_QUALITY_TOKENS=1024
MAX_CONCURRENT_REQUESTS=2
MEMORY_THRESHOLD_GB=2.5
AUTO_CLEANUP=true
SMART_MODEL_SWITCHING=true

# High-Quality AI Settings
REQUIRE_QUALITY_THRESHOLD=200
ADAPTIVE_CONTEXT=true
F16_PRECISION=true
STREAM_RESPONSES=true

# Privacy & Offline
OFFLINE_MODE=true
USE_EXTERNAL_APIs=false
CACHE_RESPONSES=true
LOG_QUERIES=false

# Performance for 8B Model
USE_GPU=true
LOW_MEMORY_MODE=true
NUM_THREADS=6
AUTO_GC=true
EOF

# VytvoÅ™enÃ­ Llama-optimalizovanÃ½ch aliasÅ¯
echo "âš¡ VytvÃ¡Å™Ã­m Llama 3.1 8B quick start skripty..."

cat > start_llama_research.sh << 'EOF'
#!/bin/bash
source venv_llama/bin/activate
echo "ğŸ§  Llama 3.1 8B Research Tool je pÅ™ipraven!"
echo "ğŸ’¡ PouÅ¾itÃ­:"
echo "   python llama_main.py --test-ai          # Test Llama 3.1 8B"
echo "   python llama_main.py --system-info      # SystÃ©movÃ© info + model stats"
echo "   python llama_main.py 'AI research'      # High-quality research"
echo "   python llama_main.py --interactive      # InteraktivnÃ­ mÃ³d"
echo ""
echo "ğŸ¯ Pro nejlepÅ¡Ã­ kvalitu pouÅ¾ij:"
echo "   python llama_main.py 'complex query' --require-quality"
EOF

chmod +x start_llama_research.sh

# Performance test pro Llama 3.1 8B
echo "âš¡ VytvÃ¡Å™Ã­m Llama 3.1 8B performance test..."
cat > llama_performance_test.py << 'EOF'
#!/usr/bin/env python3
import asyncio
import time
import psutil
from local_ai_adapter import M1OptimizedLlamaClient, quick_ai_query

async def test_llama_performance():
    print("ğŸ§ª Llama 3.1 8B Performance Test")
    print("=" * 50)

    # Memory pÅ™ed testem
    memory_before = psutil.virtual_memory()
    print(f"ğŸ’¾ RAM pÅ™ed testem: {memory_before.available / (1024**3):.1f}GB volnÃ©")

    # Test 1: Kvalita odpovÄ›di s Llama 3.1 8B
    print("\nğŸ§  Test high-quality research s Llama 3.1 8B...")
    start_time = time.time()

    research_query = """Analyzuj souÄasnÃ© trendy v oblasti umÄ›lÃ© inteligence,
    zamÄ›Å™ se na praktickÃ© aplikace a budoucÃ­ smÄ›ry vÃ½voje."""

    response = await quick_ai_query(
        research_query,
        max_tokens=800,
        require_quality=True
    )

    ai_time = time.time() - start_time

    print(f"ğŸ¤– Llama 3.1 8B odpovÄ›Ä za: {ai_time:.2f}s")
    print(f"ğŸ“ DÃ©lka odpovÄ›di: {len(response)} znakÅ¯")
    print(f"ğŸ¯ Kvalita: {'VysokÃ¡' if len(response) > 500 else 'StÅ™ednÃ­'}")

    # Test 2: Rychlost s fallback modelem
    print(f"\nâš¡ Test rychlÃ© odpovÄ›di s fallback...")
    start_time = time.time()

    quick_response = await quick_ai_query(
        "Co je machine learning?",
        max_tokens=200,
        require_quality=False
    )

    quick_time = time.time() - start_time
    print(f"ğŸš€ RychlÃ¡ odpovÄ›Ä za: {quick_time:.2f}s")

    # Memory po testech
    memory_after = psutil.virtual_memory()
    print(f"\nğŸ’¾ RAM po testu: {memory_after.available / (1024**3):.1f}GB volnÃ©")
    memory_used = (memory_before.available - memory_after.available) / (1024**2)
    print(f"ğŸ“Š SpotÅ™eba pamÄ›ti: {memory_used:.1f}MB")

    # Model stats
    async with M1OptimizedLlamaClient() as client:
        model_stats = client.get_model_stats()
        print(f"\nğŸ¯ DostupnÃ© modely:")
        for name, stats in model_stats.items():
            print(f"   â€¢ {name}: {stats['size_gb']}GB, quality:{stats['quality_score']}/10")

    # CPU info
    cpu_usage = psutil.cpu_percent(interval=1)
    print(f"\nâš¡ CPU vyuÅ¾itÃ­: {cpu_usage}%")

    print(f"\nâœ… Llama 3.1 8B performance test dokonÄen!")
    print(f"ğŸ“ˆ DoporuÄenÃ­: {'PouÅ¾Ã­vej high-quality mode' if ai_time < 15 else 'Preferuj rychlÃ© dotazy'}")

if __name__ == "__main__":
    asyncio.run(test_llama_performance())
EOF

chmod +x llama_performance_test.py

# FinÃ¡lnÃ­ test
echo "ğŸ¯ SpouÅ¡tÃ­m finÃ¡lnÃ­ test Llama 3.1 8B systÃ©mu..."
python3 -c "
import asyncio
from local_ai_adapter import quick_ai_query

async def test():
    try:
        response = await quick_ai_query('Test Llama 3.1 8B', max_tokens=50)
        print(f'âœ… Llama test ÃºspÄ›Å¡nÃ½: {response[:100]}...')
    except Exception as e:
        print(f'âŒ Llama test failed: {e}')

asyncio.run(test())
"

echo ""
echo "ğŸ‰ Llama 3.1 8B MacBook Setup dokonÄen!"
echo "=" * 50
echo "âœ… Llama 3.1 8B je nainstalovÃ¡n a nakonfigurovÃ¡n"
echo "âœ… InteligentnÃ­ model switching je aktivnÃ­"
echo "âœ… M1 optimalizace pro vÄ›tÅ¡Ã­ modely jsou aktivnÃ­"
echo "âœ… High-quality research mode je dostupnÃ½"
echo ""
echo "ğŸš€ Quick Start:"
echo "   ./start_llama_research.sh             # Aktivovat prostÅ™edÃ­"
echo "   python llama_main.py --test-ai        # Test Llama 3.1 8B"
echo "   python llama_main.py 'AI research'    # High-quality research"
echo "   python llama_performance_test.py      # Performance test"
echo ""
echo "ğŸ’¡ Pro nejlepÅ¡Ã­ vÃ½kon s Llama 3.1 8B:"
echo "   â€¢ ZavÅ™i nepotÅ™ebnÃ© aplikace (potÅ™ebuje 4GB+ RAM)"
echo "   â€¢ PouÅ¾Ã­vej --require-quality pro komplexnÃ­ dotazy"
echo "   â€¢ Fallback na Phi-3 Mini je automatickÃ½ pÅ™i nÃ­zkÃ©m RAM"
echo "   â€¢ Cache je optimalizovanÃ¡ pro vÄ›tÅ¡Ã­ odpovÄ›di"
echo ""
echo "ğŸ¯ Model hierarchie:"
echo "   Llama 3.1 8B â†’ VysokÃ¡ kvalita, komplexnÃ­ analÃ½zy"
echo "   Qwen2 7B     â†’ KÃ³dovÃ¡nÃ­ a technickÃ© dotazy"
echo "   Phi-3 Mini   â†’ RychlÃ© odpovÄ›di, fallback"
echo ""
