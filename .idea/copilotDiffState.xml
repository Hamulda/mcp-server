<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/MCP_SERVERS.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/MCP_SERVERS.md" />
              <option name="updatedContent" value="#  MCP Servers Installation Guide&#10;&#10;##  Installed MCP Servers&#10;&#10;All requested MCP servers have been successfully installed and configured:&#10;&#10;### ✅ Successfully Installed Servers&#10;&#10;1. ** Brave Search MCP Server**&#10;   - **Location**: `mcp_servers/brave-search-mcp-server/`&#10;   - **Source**: https://github.com/brave/brave-search-mcp-server&#10;   - **Features**: Web search, news search, image search, video search&#10;   - **Status**: ✅ Installed and built&#10;   - **Requirements**: Brave API Key&#10;&#10;2. **️ Puppeteer MCP Server**&#10;   - **Location**: `mcp_servers/servers-archived/src/puppeteer/`&#10;   - **Source**: https://github.com/modelcontextprotocol/servers-archived/tree/main/src/puppeteer&#10;   - **Features**: Web scraping, browser automation, screenshot capture&#10;   - **Status**: ✅ Installed and built&#10;   - **Note**: From archived servers repository&#10;&#10;3. ** Git MCP Server**&#10;   - **Location**: `mcp_servers/mcp-official-servers/src/git/`&#10;   - **Source**: https://github.com/modelcontextprotocol/servers/tree/main/src/git&#10;   - **Features**: Git repository operations, commit history, branch management&#10;   - **Status**: ✅ Installed and built&#10;&#10;4. ** Fetch MCP Server**&#10;   - **Location**: `mcp_servers/mcp-official-servers/src/fetch/`&#10;   - **Source**: https://github.com/modelcontextprotocol/servers/tree/main/src/fetch&#10;   - **Features**: HTTP requests, API calls, web content fetching&#10;   - **Status**: ✅ Installed and built&#10;&#10;5. ** GitHub MCP Server**&#10;   - **Location**: `mcp_servers/github-mcp-server/`&#10;   - **Source**: https://github.com/github/github-mcp-server&#10;   - **Features**: GitHub API integration, repository management, issues, PRs&#10;   - **Status**: ⚠️ Cloned (requires Go to build)&#10;   - **Requirements**: GitHub Personal Access Token, Go programming language&#10;&#10;##  Quick Start&#10;&#10;### Using the Management Script&#10;&#10;```bash&#10;# List all available servers&#10;./mcp-servers.sh list&#10;&#10;# Build all servers (except GitHub which needs Go)&#10;./mcp-servers.sh build&#10;&#10;# Start a specific server&#10;./mcp-servers.sh start brave-search&#10;./mcp-servers.sh start puppeteer&#10;./mcp-servers.sh start git&#10;./mcp-servers.sh start fetch&#10;```&#10;&#10;### Manual Server Startup&#10;&#10;#### Brave Search MCP Server&#10;```bash&#10;cd mcp_servers/brave-search-mcp-server&#10;export BRAVE_API_KEY=&quot;your-api-key-here&quot;&#10;node dist/index.js&#10;```&#10;&#10;#### Puppeteer MCP Server&#10;```bash&#10;cd mcp_servers/servers-archived/src/puppeteer&#10;node dist/index.js&#10;```&#10;&#10;#### Git MCP Server&#10;```bash&#10;cd mcp_servers/mcp-official-servers/src/git&#10;node dist/index.js&#10;```&#10;&#10;#### Fetch MCP Server&#10;```bash&#10;cd mcp_servers/mcp-official-servers/src/fetch&#10;node dist/index.js&#10;```&#10;&#10;#### GitHub MCP Server (requires Go)&#10;```bash&#10;# First, install Go from https://golang.org/dl/&#10;cd mcp_servers/github-mcp-server&#10;go build&#10;export GITHUB_PERSONAL_ACCESS_TOKEN=&quot;your-token-here&quot;&#10;./github-mcp-server&#10;```&#10;&#10;## ⚙️ Configuration&#10;&#10;### MCP Configuration File&#10;A complete configuration file is available at `mcp_servers/mcp-config.json`:&#10;&#10;```json&#10;{&#10;  &quot;mcpServers&quot;: {&#10;    &quot;brave-search&quot;: {&#10;      &quot;command&quot;: &quot;node&quot;,&#10;      &quot;args&quot;: [&quot;./mcp_servers/brave-search-mcp-server/dist/index.js&quot;],&#10;      &quot;env&quot;: {&#10;        &quot;BRAVE_API_KEY&quot;: &quot;your-brave-api-key-here&quot;&#10;      }&#10;    },&#10;    &quot;puppeteer&quot;: {&#10;      &quot;command&quot;: &quot;node&quot;,&#10;      &quot;args&quot;: [&quot;./mcp_servers/servers-archived/src/puppeteer/dist/index.js&quot;]&#10;    },&#10;    &quot;git&quot;: {&#10;      &quot;command&quot;: &quot;node&quot;,&#10;      &quot;args&quot;: [&quot;./mcp_servers/mcp-official-servers/src/git/dist/index.js&quot;]&#10;    },&#10;    &quot;fetch&quot;: {&#10;      &quot;command&quot;: &quot;node&quot;,&#10;      &quot;args&quot;: [&quot;./mcp_servers/mcp-official-servers/src/fetch/dist/index.js&quot;]&#10;    },&#10;    &quot;github&quot;: {&#10;      &quot;command&quot;: &quot;./mcp_servers/github-mcp-server/github-mcp-server&quot;,&#10;      &quot;env&quot;: {&#10;        &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;your-github-token-here&quot;&#10;      }&#10;    }&#10;  }&#10;}&#10;```&#10;&#10;### Environment Variables&#10;&#10;Create a `.env` file in the project root with:&#10;&#10;```bash&#10;# Brave Search API&#10;BRAVE_API_KEY=your_brave_api_key_here&#10;&#10;# GitHub API&#10;GITHUB_PERSONAL_ACCESS_TOKEN=your_github_token_here&#10;```&#10;&#10;##  API Keys Setup&#10;&#10;### Brave Search API Key&#10;1. Visit https://api.search.brave.com/&#10;2. Sign up for an account&#10;3. Get your API key&#10;4. Add it to your environment variables&#10;&#10;### GitHub Personal Access Token&#10;1. Go to GitHub Settings → Developer settings → Personal access tokens&#10;2. Generate a new token with appropriate permissions&#10;3. Add it to your environment variables&#10;&#10;## ️ Troubleshooting&#10;&#10;### GitHub MCP Server (Go Required)&#10;If you don't have Go installed:&#10;&#10;```bash&#10;# macOS with Homebrew&#10;brew install go&#10;&#10;# Or download from https://golang.org/dl/&#10;&#10;# Then build the server&#10;cd mcp_servers/github-mcp-server&#10;go build&#10;```&#10;&#10;### Puppeteer Issues&#10;If Puppeteer fails to start:&#10;&#10;```bash&#10;cd mcp_servers/servers-archived/src/puppeteer&#10;npm install&#10;npm run build&#10;```&#10;&#10;### Permission Issues&#10;Make sure the management script is executable:&#10;&#10;```bash&#10;chmod +x mcp-servers.sh&#10;```&#10;&#10;##  Server Features&#10;&#10;| Server | Web Search | API Calls | Git Ops | Browser | GitHub |&#10;|--------|------------|-----------|---------|---------|---------|&#10;| Brave Search | ✅ | ✅ | ❌ | ❌ | ❌ |&#10;| Puppeteer | ❌ | ❌ | ❌ | ✅ | ❌ |&#10;| Git | ❌ | ❌ | ✅ | ❌ | ❌ |&#10;| Fetch | ❌ | ✅ | ❌ | ❌ | ❌ |&#10;| GitHub | ❌ | ✅ | ✅ | ❌ | ✅ |&#10;&#10;##  Integration with Academic Research Tool&#10;&#10;All MCP servers are now ready to be integrated with your Academic Research Tool. They can be used by AI agents to:&#10;&#10;- **Search the web** (Brave Search)&#10;- **Scrape websites** (Puppeteer)&#10;- **Manage repositories** (Git)&#10;- **Make API calls** (Fetch)&#10;- **Interact with GitHub** (GitHub)&#10;&#10;##  Next Steps&#10;&#10;1. **Set up API keys** for Brave Search and GitHub&#10;2. **Install Go** if you want to use GitHub MCP Server&#10;3. **Test each server** individually using the management script&#10;4. **Integrate with your main application** using the MCP configuration&#10;&#10;All servers are production-ready and optimized for your M1 MacBook! " />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/PLAN.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/PLAN.md" />
              <option name="updatedContent" value="# Plán vytvoření Flask webové aplikace&#10;&#10;## Analýza současného stavu projektu&#10;- Projekt aktuálně používá FastAPI jako hlavní web framework&#10;- V requirements.txt není Flask, bude potřeba ho přidat&#10;- Struktura projektu je připravena pro webové aplikace&#10;&#10;## Kroky implementace&#10;&#10;### 1. Příprava závislostí&#10;- Přidat Flask do requirements.txt&#10;- Instalovat Flask pomocí pip&#10;&#10;### 2. Vytvoření základní Flask aplikace&#10;- Vytvořit soubor app.py v kořenovém adresáři&#10;- Implementovat základní Flask aplikaci s route &quot;/&quot;&#10;- Route bude vracet &quot;Hello, World!&quot;&#10;&#10;### 3. Struktura app.py&#10;```python&#10;from flask import Flask&#10;&#10;app = Flask(__name__)&#10;&#10;@app.route('/')&#10;def hello_world():&#10;    return 'Hello, World!'&#10;&#10;if __name__ == '__main__':&#10;    app.run(debug=True)&#10;```&#10;&#10;### 4. Kontrola kvality&#10;- Použít lint nástroj pro kontrolu syntaxe a stylu kódu&#10;- Ověřit funkčnost aplikace&#10;&#10;### 5. Dokumentace&#10;- Přidat instrukce pro spuštění do README nebo komentářů&#10;&#10;## Očekávané výsledky&#10;- Funkční Flask aplikace dostupná na localhost:5000&#10;- Zobrazení &quot;Hello, World!&quot; na hlavní stránce&#10;- Čistý a kvalitní kód odpovídající standardům" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Flask Backend - Optimized with validation, async processing, and error handling&#10;Implementuje Blueprint architecture, proper logging, CORS, a input validation&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import logging&#10;import time&#10;from concurrent.futures import ThreadPoolExecutor&#10;from functools import wraps&#10;from typing import Dict, Any, Optional&#10;&#10;from flask import Flask, Blueprint, request, jsonify, current_app&#10;from flask_cors import CORS&#10;from marshmallow import Schema, fields, ValidationError&#10;from werkzeug.exceptions import BadRequest, InternalServerError&#10;&#10;# Import našich modulů&#10;try:&#10;    from unified_config import get_config&#10;    from academic_scraper import create_scraping_orchestrator&#10;    UNIFIED_CONFIG_AVAILABLE = True&#10;except ImportError as e:&#10;    logger.warning(f&quot;Unified config not available: {e}&quot;)&#10;    UNIFIED_CONFIG_AVAILABLE = False&#10;    # Fallback configuration&#10;    class FallbackConfig:&#10;        sources = {'wikipedia': True, 'pubmed': True, 'openalex': True}&#10;    get_config = lambda: FallbackConfig()&#10;&#10;# Nastavení loggingu&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;)&#10;logger = logging.getLogger(__name__)&#10;&#10;# Validační schéma&#10;class ScrapeRequestSchema(Schema):&#10;    query = fields.Str(required=True, validate=lambda x: len(x.strip()) &gt; 0)&#10;    sources = fields.List(fields.Str(), missing=['wikipedia', 'openalex'])&#10;    max_results = fields.Int(missing=10, validate=lambda x: 1 &lt;= x &lt;= 100)&#10;    timeout = fields.Int(missing=30, validate=lambda x: 5 &lt;= x &lt;= 120)&#10;&#10;# Global thread pool pro async processing&#10;executor = ThreadPoolExecutor(max_workers=4)&#10;&#10;def async_endpoint(f):&#10;    &quot;&quot;&quot;Decorator pro asynchronní zpracování v endpointech&quot;&quot;&quot;&#10;    @wraps(f)&#10;    def wrapper(*args, **kwargs):&#10;        try:&#10;            loop = asyncio.new_event_loop()&#10;            asyncio.set_event_loop(loop)&#10;            return loop.run_until_complete(f(*args, **kwargs))&#10;        finally:&#10;            loop.close()&#10;    return wrapper&#10;&#10;def validate_json(schema_class):&#10;    &quot;&quot;&quot;Decorator pro validaci JSON inputu&quot;&quot;&quot;&#10;    def decorator(f):&#10;        @wraps(f)&#10;        def wrapper(*args, **kwargs):&#10;            try:&#10;                schema = schema_class()&#10;                validated_data = schema.load(request.get_json() or {})&#10;                return f(validated_data, *args, **kwargs)&#10;            except ValidationError as e:&#10;                logger.warning(f&quot;Validation error: {e.messages}&quot;)&#10;                return jsonify({&#10;                    'error': 'Validation failed',&#10;                    'details': e.messages&#10;                }), 400&#10;            except Exception as e:&#10;                logger.error(f&quot;Unexpected validation error: {e}&quot;)&#10;                return jsonify({'error': 'Invalid request format'}), 400&#10;        return wrapper&#10;    return decorator&#10;&#10;# Blueprint definice&#10;api_bp = Blueprint('api', __name__, url_prefix='/api')&#10;&#10;@api_bp.route('/health', methods=['GET'])&#10;def health_check():&#10;    &quot;&quot;&quot;Health check endpoint&quot;&quot;&quot;&#10;    return jsonify({&#10;        'status': 'healthy',&#10;        'timestamp': time.time(),&#10;        'version': '1.0.0'&#10;    })&#10;&#10;@api_bp.route('/scrape', methods=['POST'])&#10;@validate_json(ScrapeRequestSchema)&#10;@async_endpoint&#10;async def scrape_endpoint(validated_data: Dict[str, Any]):&#10;    &quot;&quot;&quot;&#10;    Hlavní scraping endpoint s asynchronním zpracováním&#10;    &quot;&quot;&quot;&#10;    start_time = time.time()&#10;    query = validated_data['query']&#10;    sources = validated_data['sources']&#10;    max_results = validated_data['max_results']&#10;    timeout = validated_data['timeout']&#10;&#10;    logger.info(f&quot;Processing scrape request: query='{query}', sources={sources}&quot;)&#10;&#10;    try:&#10;        # Inicializace scraperu&#10;        scraper = EnhancedAcademicScraper()&#10;&#10;        # Asynchronní scraping&#10;        results = await scraper.scrape_multiple_sources(&#10;            query=query,&#10;            sources=sources,&#10;            max_results=max_results,&#10;            timeout=timeout&#10;        )&#10;&#10;        processing_time = time.time() - start_time&#10;&#10;        # Sestavení odpovědi&#10;        response = {&#10;            'success': True,&#10;            'query': query,&#10;            'sources_requested': sources,&#10;            'processing_time': round(processing_time, 2),&#10;            'results': results,&#10;            'summary': {&#10;                'total_sources': len(results),&#10;                'successful_sources': len([r for r in results.values() if r.get('success', False)]),&#10;                'total_papers': sum(len(r.get('data', {}).get('papers', [])) for r in results.values())&#10;            }&#10;        }&#10;&#10;        logger.info(f&quot;Scrape completed in {processing_time:.2f}s: {len(results)} sources processed&quot;)&#10;        return jsonify(response)&#10;&#10;    except asyncio.TimeoutError:&#10;        logger.warning(f&quot;Scrape request timed out after {timeout}s&quot;)&#10;        return jsonify({&#10;            'error': 'Request timed out',&#10;            'timeout': timeout&#10;        }), 408&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Scrape request failed: {e}&quot;, exc_info=True)&#10;        return jsonify({&#10;            'error': 'Internal server error',&#10;            'message': str(e) if current_app.debug else 'Processing failed'&#10;        }), 500&#10;&#10;@api_bp.route('/sources', methods=['GET'])&#10;def get_available_sources():&#10;    &quot;&quot;&quot;Vrátí seznam dostupných zdrojů&quot;&quot;&quot;&#10;    try:&#10;        if UNIFIED_CONFIG_AVAILABLE:&#10;            config = get_config()&#10;            sources = list(config.sources.keys())&#10;        else:&#10;            from config import BaseConfig&#10;            sources = list(BaseConfig.SOURCES.keys())&#10;&#10;        return jsonify({&#10;            'sources': sources,&#10;            'count': len(sources)&#10;        })&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to get sources: {e}&quot;)&#10;        return jsonify({'error': 'Failed to retrieve sources'}), 500&#10;&#10;@api_bp.errorhandler(404)&#10;def not_found(error):&#10;    return jsonify({'error': 'Endpoint not found'}), 404&#10;&#10;@api_bp.errorhandler(405)&#10;def method_not_allowed(error):&#10;    return jsonify({'error': 'Method not allowed'}), 405&#10;&#10;@api_bp.errorhandler(500)&#10;def internal_error(error):&#10;    logger.error(f&quot;Internal server error: {error}&quot;)&#10;    return jsonify({'error': 'Internal server error'}), 500&#10;&#10;def create_app(config_name='development'):&#10;    &quot;&quot;&quot;Application factory&quot;&quot;&quot;&#10;    app = Flask(__name__)&#10;&#10;    # Konfigurace - použij náš nový config systém&#10;    try:&#10;        # Pokus se použít unified config pouze pokud je dostupný&#10;        if UNIFIED_CONFIG_AVAILABLE:&#10;            config = get_config()&#10;            # Mapování unified_config na náš systém&#10;            app.config['DEBUG'] = getattr(config, 'debug', False)&#10;            app.config['TESTING'] = getattr(config, 'testing', False)&#10;        else:&#10;            # Použij náš nový config systém&#10;            if config_name == 'development':&#10;                from config import DevelopmentConfig&#10;                config_obj = DevelopmentConfig()&#10;                app.config['DEBUG'] = config_obj.APP.debug&#10;                app.config['TESTING'] = config_obj.APP.testing&#10;            elif config_name == 'testing':&#10;                from config import TestingConfig&#10;                config_obj = TestingConfig()&#10;                app.config['DEBUG'] = config_obj.APP.debug&#10;                app.config['TESTING'] = config_obj.APP.testing&#10;            else:&#10;                from config import ProductionConfig&#10;                config_obj = ProductionConfig()&#10;                app.config['DEBUG'] = config_obj.APP.debug&#10;                app.config['TESTING'] = config_obj.APP.testing&#10;    except Exception as e:&#10;        # Fallback na defaults&#10;        logger.warning(f&quot;Config loading failed, using defaults: {e}&quot;)&#10;        app.config['DEBUG'] = config_name == 'development'&#10;        app.config['TESTING'] = config_name == 'testing'&#10;&#10;    # CORS setup&#10;    CORS(app, resources={&#10;        r&quot;/api/*&quot;: {&#10;            &quot;origins&quot;: [&quot;http://localhost:3000&quot;, &quot;http://localhost:8501&quot;],&#10;            &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;, &quot;OPTIONS&quot;],&#10;            &quot;allow_headers&quot;: [&quot;Content-Type&quot;, &quot;Authorization&quot;]&#10;        }&#10;    })&#10;&#10;    # Registrace blueprintů&#10;    app.register_blueprint(api_bp)&#10;&#10;    # Logging setup&#10;    if not app.testing:&#10;        handler = logging.StreamHandler()&#10;        handler.setLevel(logging.INFO)&#10;        formatter = logging.Formatter(&#10;            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;        )&#10;        handler.setFormatter(formatter)&#10;        app.logger.addHandler(handler)&#10;        app.logger.setLevel(logging.INFO)&#10;&#10;    return app&#10;&#10;# Pro development server&#10;if __name__ == '__main__':&#10;    app = create_app()&#10;    app.run(&#10;        host='0.0.0.0',&#10;        port=5000,&#10;        debug=True,&#10;        threaded=True&#10;    )&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Flask Backend - Optimized with validation, async processing, and error handling&#10;Implementuje Blueprint architecture, proper logging, CORS, a input validation&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import logging&#10;import time&#10;from concurrent.futures import ThreadPoolExecutor&#10;from functools import wraps&#10;from typing import Dict, Any, Optional&#10;&#10;from flask import Flask, Blueprint, request, jsonify, current_app&#10;from flask_cors import CORS&#10;from marshmallow import Schema, fields, ValidationError&#10;from werkzeug.exceptions import BadRequest, InternalServerError&#10;&#10;# Import našich modulů&#10;try:&#10;    from unified_config import get_config&#10;    from academic_scraper import create_scraping_orchestrator&#10;    UNIFIED_CONFIG_AVAILABLE = True&#10;except ImportError as e:&#10;    logger.warning(f&quot;Unified config not available: {e}&quot;)&#10;    UNIFIED_CONFIG_AVAILABLE = False&#10;    # Fallback configuration&#10;    class FallbackConfig:&#10;        sources = {'wikipedia': True, 'pubmed': True, 'openalex': True}&#10;    get_config = lambda: FallbackConfig()&#10;&#10;# Nastavení loggingu&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;)&#10;logger = logging.getLogger(__name__)&#10;&#10;# Validační schéma&#10;class ScrapeRequestSchema(Schema):&#10;    query = fields.Str(required=True, validate=lambda x: len(x.strip()) &gt; 0)&#10;    sources = fields.List(fields.Str(), missing=['wikipedia', 'openalex'])&#10;    max_results = fields.Int(missing=10, validate=lambda x: 1 &lt;= x &lt;= 100)&#10;    timeout = fields.Int(missing=30, validate=lambda x: 5 &lt;= x &lt;= 120)&#10;&#10;# Global thread pool pro async processing&#10;executor = ThreadPoolExecutor(max_workers=4)&#10;&#10;def async_endpoint(f):&#10;    &quot;&quot;&quot;Decorator pro asynchronní zpracování v endpointech&quot;&quot;&quot;&#10;    @wraps(f)&#10;    def wrapper(*args, **kwargs):&#10;        try:&#10;            loop = asyncio.new_event_loop()&#10;            asyncio.set_event_loop(loop)&#10;            return loop.run_until_complete(f(*args, **kwargs))&#10;        finally:&#10;            loop.close()&#10;    return wrapper&#10;&#10;def validate_json(schema_class):&#10;    &quot;&quot;&quot;Decorator pro validaci JSON inputu&quot;&quot;&quot;&#10;    def decorator(f):&#10;        @wraps(f)&#10;        def wrapper(*args, **kwargs):&#10;            try:&#10;                schema = schema_class()&#10;                validated_data = schema.load(request.get_json() or {})&#10;                return f(validated_data, *args, **kwargs)&#10;            except ValidationError as e:&#10;                logger.warning(f&quot;Validation error: {e.messages}&quot;)&#10;                return jsonify({&#10;                    'error': 'Validation failed',&#10;                    'details': e.messages&#10;                }), 400&#10;            except Exception as e:&#10;                logger.error(f&quot;Unexpected validation error: {e}&quot;)&#10;                return jsonify({'error': 'Invalid request format'}), 400&#10;        return wrapper&#10;    return decorator&#10;&#10;# Blueprint definice&#10;api_bp = Blueprint('api', __name__, url_prefix='/api')&#10;&#10;@api_bp.route('/health', methods=['GET'])&#10;def health_check():&#10;    &quot;&quot;&quot;Health check endpoint&quot;&quot;&quot;&#10;    return jsonify({&#10;        'status': 'healthy',&#10;        'timestamp': time.time(),&#10;        'version': '1.0.0'&#10;    })&#10;&#10;@api_bp.route('/scrape', methods=['POST'])&#10;@validate_json(ScrapeRequestSchema)&#10;@async_endpoint&#10;async def scrape_endpoint(validated_data: Dict[str, Any]):&#10;    &quot;&quot;&quot;&#10;    Hlavní scraping endpoint s asynchronním zpracováním&#10;    &quot;&quot;&quot;&#10;    start_time = time.time()&#10;    query = validated_data['query']&#10;    sources = validated_data['sources']&#10;    max_results = validated_data['max_results']&#10;    timeout = validated_data['timeout']&#10;&#10;    logger.info(f&quot;Processing scrape request: query='{query}', sources={sources}&quot;)&#10;&#10;    try:&#10;        # Inicializace scraperu - použij správnou třídu z academic_scraper.py&#10;        orchestrator = create_scraping_orchestrator()&#10;&#10;        # Asynchronní scraping&#10;        results = await orchestrator.scrape_all_sources(&#10;            query=query,&#10;            sources=sources&#10;        )&#10;&#10;        processing_time = time.time() - start_time&#10;&#10;        # Sestavení odpovědi&#10;        response = {&#10;            'success': True,&#10;            'query': query,&#10;            'sources_requested': sources,&#10;            'processing_time': round(processing_time, 2),&#10;            'results': [&#10;                {&#10;                    'source': r.source,&#10;                    'success': r.success,&#10;                    'data': r.data,&#10;                    'error': r.error,&#10;                    'response_time': r.response_time&#10;                }&#10;                for r in results&#10;            ],&#10;            'summary': {&#10;                'total_sources': len(results),&#10;                'successful_sources': len([r for r in results if r.success]),&#10;                'total_papers': sum(len(r.data.get('papers', [])) for r in results if r.success)&#10;            }&#10;        }&#10;&#10;        # Cleanup&#10;        await orchestrator.cleanup()&#10;&#10;        logger.info(f&quot;Scrape completed in {processing_time:.2f}s: {len(results)} sources processed&quot;)&#10;        return jsonify(response)&#10;&#10;    except asyncio.TimeoutError:&#10;        logger.warning(f&quot;Scrape request timed out after {timeout}s&quot;)&#10;        return jsonify({&#10;            'error': 'Request timed out',&#10;            'timeout': timeout&#10;        }), 408&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Scrape request failed: {e}&quot;, exc_info=True)&#10;        return jsonify({&#10;            'error': 'Internal server error',&#10;            'message': str(e) if current_app.debug else 'Processing failed'&#10;        }), 500&#10;&#10;@api_bp.route('/sources', methods=['GET'])&#10;def get_available_sources():&#10;    &quot;&quot;&quot;Vrátí seznam dostupných zdrojů&quot;&quot;&quot;&#10;    try:&#10;        if UNIFIED_CONFIG_AVAILABLE:&#10;            config = get_config()&#10;            sources = list(config.sources.keys())&#10;        else:&#10;            from config import BaseConfig&#10;            sources = list(BaseConfig.SOURCES.keys())&#10;&#10;        return jsonify({&#10;            'sources': sources,&#10;            'count': len(sources)&#10;        })&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to get sources: {e}&quot;)&#10;        return jsonify({'error': 'Failed to retrieve sources'}), 500&#10;&#10;@api_bp.errorhandler(404)&#10;def not_found(error):&#10;    return jsonify({'error': 'Endpoint not found'}), 404&#10;&#10;@api_bp.errorhandler(405)&#10;def method_not_allowed(error):&#10;    return jsonify({'error': 'Method not allowed'}), 405&#10;&#10;@api_bp.errorhandler(500)&#10;def internal_error(error):&#10;    logger.error(f&quot;Internal server error: {error}&quot;)&#10;    return jsonify({'error': 'Internal server error'}), 500&#10;&#10;def create_app(config_name='development'):&#10;    &quot;&quot;&quot;Application factory&quot;&quot;&quot;&#10;    app = Flask(__name__)&#10;&#10;    # Konfigurace - použij náš nový config systém&#10;    try:&#10;        # Pokus se použít unified config pouze pokud je dostupný&#10;        if UNIFIED_CONFIG_AVAILABLE:&#10;            config = get_config()&#10;            # Mapování unified_config na náš systém&#10;            app.config['DEBUG'] = getattr(config, 'debug', False)&#10;            app.config['TESTING'] = getattr(config, 'testing', False)&#10;        else:&#10;            # Použij náš nový config systém&#10;            if config_name == 'development':&#10;                from config import DevelopmentConfig&#10;                config_obj = DevelopmentConfig()&#10;                app.config['DEBUG'] = config_obj.APP.debug&#10;                app.config['TESTING'] = config_obj.APP.testing&#10;            elif config_name == 'testing':&#10;                from config import TestingConfig&#10;                config_obj = TestingConfig()&#10;                app.config['DEBUG'] = config_obj.APP.debug&#10;                app.config['TESTING'] = config_obj.APP.testing&#10;            else:&#10;                from config import ProductionConfig&#10;                config_obj = ProductionConfig()&#10;                app.config['DEBUG'] = config_obj.APP.debug&#10;                app.config['TESTING'] = config_obj.APP.testing&#10;    except Exception as e:&#10;        # Fallback na defaults&#10;        logger.warning(f&quot;Config loading failed, using defaults: {e}&quot;)&#10;        app.config['DEBUG'] = config_name == 'development'&#10;        app.config['TESTING'] = config_name == 'testing'&#10;&#10;    # CORS setup&#10;    CORS(app, resources={&#10;        r&quot;/api/*&quot;: {&#10;            &quot;origins&quot;: [&quot;http://localhost:3000&quot;, &quot;http://localhost:8501&quot;],&#10;            &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;, &quot;OPTIONS&quot;],&#10;            &quot;allow_headers&quot;: [&quot;Content-Type&quot;, &quot;Authorization&quot;]&#10;        }&#10;    })&#10;&#10;    # Registrace blueprintů&#10;    app.register_blueprint(api_bp)&#10;&#10;    # Logging setup&#10;    if not app.testing:&#10;        handler = logging.StreamHandler()&#10;        handler.setLevel(logging.INFO)&#10;        formatter = logging.Formatter(&#10;            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;        )&#10;        handler.setFormatter(formatter)&#10;        app.logger.addHandler(handler)&#10;        app.logger.setLevel(logging.INFO)&#10;&#10;    return app&#10;&#10;# Pro development server&#10;if __name__ == '__main__':&#10;    app = create_app()&#10;    app.run(&#10;        host='0.0.0.0',&#10;        port=5000,&#10;        debug=True,&#10;        threaded=True&#10;    )" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/cleanup_project.sh">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/cleanup_project.sh" />
              <option name="updatedContent" value="#!/bin/bash&#10;# Project Cleanup Script - Odstranění deprecated souborů&#10;# Spouští se po úspěšné migraci na unified architekturu&#10;&#10;echo &quot; ZAČÍNÁM ČIŠTĚNÍ PROJEKTU...&quot;&#10;echo &quot;======================================&quot;&#10;&#10;# Vytvoř backup složku&#10;mkdir -p deprecated_backup&#10;echo &quot;✅ Vytvořena backup složka&quot;&#10;&#10;# 1. DEPRECATED MAIN SOUBORY&#10;echo &quot;️ Odstraňuji deprecated main soubory...&quot;&#10;mv main.py deprecated_backup/ 2&gt;/dev/null || echo &quot;main.py již neexistuje&quot;&#10;mv main_fast.py deprecated_backup/ 2&gt;/dev/null || echo &quot;main_fast.py již neexistuje&quot;&#10;&#10;# 2. DEPRECATED KONFIGURACE  &#10;echo &quot;️ Odstraňuji deprecated config soubory...&quot;&#10;mv config_personal.py deprecated_backup/ 2&gt;/dev/null || echo &quot;config_personal.py již neexistuje&quot;&#10;mv settings.py deprecated_backup/ 2&gt;/dev/null || echo &quot;settings.py již neexistuje&quot;&#10;mv external_sources_config.py deprecated_backup/ 2&gt;/dev/null || echo &quot;external_sources_config.py již neexistuje&quot;&#10;&#10;# 3. DEPRECATED SERVERY&#10;echo &quot;️ Odstraňuji deprecated server soubory...&quot;&#10;mv fastapi_app.py deprecated_backup/ 2&gt;/dev/null || echo &quot;fastapi_app.py již neexistuje&quot;&#10;mv streamlit_app.py deprecated_backup/ 2&gt;/dev/null || echo &quot;streamlit_app.py již neexistuje&quot;&#10;&#10;# 4. DEPRECATED RESEARCH ENGINES&#10;echo &quot;️ Odstraňuji deprecated research engines...&quot;&#10;mv research_engine.py deprecated_backup/ 2&gt;/dev/null || echo &quot;research_engine.py již neexistuje&quot;&#10;mv simple_research_engine.py deprecated_backup/ 2&gt;/dev/null || echo &quot;simple_research_engine.py již neexistuje&quot;&#10;mv research_strategies.py deprecated_backup/ 2&gt;/dev/null || echo &quot;research_strategies.py již neexistuje&quot;&#10;&#10;# 5. DEPRECATED SCRAPERY&#10;echo &quot;️ Odstraňuji deprecated scrapery...&quot;&#10;mv web_scraper.py deprecated_backup/ 2&gt;/dev/null || echo &quot;web_scraper.py již neexistuje&quot;&#10;&#10;# 6. DEPRECATED DATABASE&#10;echo &quot;️ Odstraňuji deprecated database soubory...&quot;&#10;mv database_manager.py deprecated_backup/ 2&gt;/dev/null || echo &quot;database_manager.py již neexistuje&quot;&#10;mv database_migrations.py deprecated_backup/ 2&gt;/dev/null || echo &quot;database_migrations.py již neexistuje&quot;&#10;&#10;# 7. DEPRECATED TESTY&#10;echo &quot;️ Odstraňuji deprecated test soubory...&quot;&#10;mv test_app.py deprecated_backup/ 2&gt;/dev/null || echo &quot;test_app.py již neexistuje&quot;&#10;mv test_core_components.py deprecated_backup/ 2&gt;/dev/null || echo &quot;test_core_components.py již neexistuje&quot;&#10;mv test_db.py deprecated_backup/ 2&gt;/dev/null || echo &quot;test_db.py již neexistuje&quot;&#10;mv test_todo_manager.py deprecated_backup/ 2&gt;/dev/null || echo &quot;test_todo_manager.py již neexistuje&quot;&#10;&#10;# 8. DEPRECATED UTILITY SOUBORY&#10;echo &quot;️ Odstraňuji deprecated utility soubory...&quot;&#10;mv text_analyzer.py deprecated_backup/ 2&gt;/dev/null || echo &quot;text_analyzer.py již neexistuje&quot;&#10;mv text_processing_utils.py deprecated_backup/ 2&gt;/dev/null || echo &quot;text_processing_utils.py již neexistuje&quot;&#10;mv benchmarking_tools.py deprecated_backup/ 2&gt;/dev/null || echo &quot;benchmarking_tools.py již neexistuje&quot;&#10;&#10;# 9. MIGRATION SOUBORY (už nepotřebné)&#10;echo &quot;️ Odstraňuji migration soubory...&quot;&#10;mv MIGRATION_PLAN.py deprecated_backup/ 2&gt;/dev/null || echo &quot;MIGRATION_PLAN.py již neexistuje&quot;&#10;mv MIGRATION_COMPLETED.py deprecated_backup/ 2&gt;/dev/null || echo &quot;MIGRATION_COMPLETED.py již neexistuje&quot;&#10;mv MANUAL_CLEANUP_INSTRUCTIONS.py deprecated_backup/ 2&gt;/dev/null || echo &quot;MANUAL_CLEANUP_INSTRUCTIONS.py již neexistuje&quot;&#10;mv manual_cleanup.py deprecated_backup/ 2&gt;/dev/null || echo &quot;manual_cleanup.py již neexistuje&quot;&#10;mv deprecation_manager.py deprecated_backup/ 2&gt;/dev/null || echo &quot;deprecation_manager.py již neexistuje&quot;&#10;mv project_cleanup_analyzer.py deprecated_backup/ 2&gt;/dev/null || echo &quot;project_cleanup_analyzer.py již neexistuje&quot;&#10;&#10;# 10. PERFORMANCE/COST SOUBORY (integrovány do unified systému)&#10;echo &quot;️ Odstraňuji deprecated performance soubory...&quot;&#10;mv cost_optimizer.py deprecated_backup/ 2&gt;/dev/null || echo &quot;cost_optimizer.py již neexistuje&quot;&#10;mv cost_tracker.py deprecated_backup/ 2&gt;/dev/null || echo &quot;cost_tracker.py již neexistuje&quot;&#10;mv performance_optimizer.py deprecated_backup/ 2&gt;/dev/null || echo &quot;performance_optimizer.py již neexistuje&quot;&#10;mv monitoring_metrics.py deprecated_backup/ 2&gt;/dev/null || echo &quot;monitoring_metrics.py již neexistuje&quot;&#10;&#10;# 11. SPECIALIZOVANÉ SOUBORY (integrovány)&#10;echo &quot;️ Odstraňuji specializované deprecated soubory...&quot;&#10;mv gemini_manager.py deprecated_backup/ 2&gt;/dev/null || echo &quot;gemini_manager.py již neexistuje&quot;&#10;mv high_performance_cache.py deprecated_backup/ 2&gt;/dev/null || echo &quot;high_performance_cache.py již neexistuje&quot;&#10;mv token_distillation_pipeline.py deprecated_backup/ 2&gt;/dev/null || echo &quot;token_distillation_pipeline.py již neexistuje&quot;&#10;&#10;# 12. DUPLICATE SOUBORY&#10;echo &quot;️ Odstraňuji duplicitní soubory...&quot;&#10;mv optimized_academic_scraper.py deprecated_backup/ 2&gt;/dev/null || echo &quot;optimized_academic_scraper.py již neexistuje&quot;&#10;mv optimized_database_manager.py deprecated_backup/ 2&gt;/dev/null || echo &quot;optimized_database_manager.py již neexistuje&quot;&#10;&#10;# 13. TEST INTEGRATION&#10;echo &quot;️ Odstraňuji staré integration testy...&quot;&#10;mv integration_test.py deprecated_backup/ 2&gt;/dev/null || echo &quot;integration_test.py již neexistuje&quot;&#10;mv final_system_validator.py deprecated_backup/ 2&gt;/dev/null || echo &quot;final_system_validator.py již neexistuje&quot;&#10;&#10;# 14. VYČISTI TODO MANAGER (pokud je integrovný)&#10;echo &quot;️ Kontroluji todo_manager...&quot;&#10;if [ -f &quot;unified_main.py&quot; ] &amp;&amp; grep -q &quot;todo&quot; unified_main.py; then&#10;    echo &quot;Todo manager je integrován, odstraňujem samostatnou složku&quot;&#10;    mv todo_manager deprecated_backup/ 2&gt;/dev/null || echo &quot;todo_manager již není&quot;&#10;fi&#10;&#10;echo &quot;&quot;&#10;echo &quot;✅ ČIŠTĚNÍ DOKONČENO!&quot;&#10;echo &quot;======================================&quot;&#10;echo &quot; Všechny deprecated soubory byly přesunuty do deprecated_backup/&quot;&#10;echo &quot; Zkontroluj výsledek:&quot;&#10;ls -la&#10;echo &quot;&quot;&#10;echo &quot; ZBÝVAJÍCÍ CORE SOUBORY:&quot;&#10;echo &quot;- unified_main.py (hlavní entry point)&quot;&#10;echo &quot;- unified_config.py (konfigurace)&quot;&#10;echo &quot;- unified_research_engine.py (research logika)&quot;&#10;echo &quot;- unified_server.py (server komponenty)&quot;&#10;echo &quot;- academic_scraper.py (optimized scraper)&quot;&#10;echo &quot;- app.py (optimized Flask API)&quot;&#10;echo &quot;- config.py (optimized config)&quot;&#10;echo &quot;- test_optimized_components.py (moderní testy)&quot;&#10;echo &quot;&quot;&#10;echo &quot; Projekt je nyní vyčištěný a optimalizovaný!&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/config.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/config.py" />
              <option name="originalContent" value="# Research Tool Configuration for Azure App Service&#10;import os&#10;from pathlib import Path&#10;import logging&#10;from typing import Optional, Dict, Any, List&#10;from dataclasses import dataclass&#10;from urllib.parse import urljoin&#10;&#10;# Základní nastavení&#10;BASE_DIR = Path(__file__).parent&#10;DATA_DIR = BASE_DIR / &quot;data&quot;&#10;&#10;# Vytvořit složky pokud neexistují (jen pro lokální development)&#10;if not os.getenv('WEBSITE_SITE_NAME'):  # Azure App Service check&#10;    for dir_path in [DATA_DIR]:&#10;        dir_path.mkdir(exist_ok=True)&#10;&#10;@dataclass&#10;class ScrapingConfig:&#10;    &quot;&quot;&quot;Konfigurace pro scraping&quot;&quot;&quot;&#10;    request_timeout: int = 30&#10;    max_retries: int = 3&#10;    retry_delay: float = 1.0&#10;    concurrent_requests: int = 5&#10;    user_agents: List[str] = None&#10;&#10;    def __post_init__(self):&#10;        if self.user_agents is None:&#10;            self.user_agents = [&#10;                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',&#10;                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',&#10;                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'&#10;            ]&#10;&#10;@dataclass&#10;class SourceConfig:&#10;    &quot;&quot;&quot;Konfigurace pro jednotlivé zdroje&quot;&quot;&quot;&#10;    name: str&#10;    base_url: str&#10;    api_key_env: Optional[str] = None&#10;    rate_limit_delay: float = 1.0&#10;    custom_headers: Dict[str, str] = None&#10;    parser_config: Dict[str, Any] = None&#10;&#10;class BaseConfig:&#10;    &quot;&quot;&quot;Základní konfigurace&quot;&quot;&quot;&#10;    DEBUG = False&#10;    TESTING = False&#10;&#10;    # Scraping konfigurace&#10;    SCRAPING = ScrapingConfig()&#10;&#10;    # Definice zdrojů&#10;    SOURCES = {&#10;        'wikipedia': SourceConfig(&#10;            name='Wikipedia',&#10;            base_url='https://en.wikipedia.org',&#10;            rate_limit_delay=0.5,&#10;            parser_config={&#10;                'content_selector': '#mw-content-text',&#10;                'title_selector': 'h1.firstHeading',&#10;                'summary_selector': '.mw-parser-output &gt; p:first-of-type'&#10;            }&#10;        ),&#10;        'openalex': SourceConfig(&#10;            name='OpenAlex',&#10;            base_url='https://api.openalex.org',&#10;            rate_limit_delay=0.1,&#10;            custom_headers={'User-Agent': 'Research-Tool (mailto:your-email@example.com)'},&#10;            parser_config={&#10;                'works_endpoint': '/works',&#10;                'authors_endpoint': '/authors',&#10;                'institutions_endpoint': '/institutions'&#10;            }&#10;        ),&#10;        'semantic_scholar': SourceConfig(&#10;            name='Semantic Scholar',&#10;            base_url='https://api.semanticscholar.org',&#10;            api_key_env='SEMANTIC_SCHOLAR_API_KEY',&#10;            rate_limit_delay=1.0,&#10;            parser_config={&#10;                'paper_endpoint': '/graph/v1/paper',&#10;                'author_endpoint': '/graph/v1/author',&#10;                'search_endpoint': '/graph/v1/paper/search'&#10;            }&#10;        ),&#10;        'google_scholar': SourceConfig(&#10;            name='Google Scholar',&#10;            base_url='https://scholar.google.com',&#10;            rate_limit_delay=2.0,&#10;            parser_config={&#10;                'search_selector': '.gs_rt a',&#10;                'citation_selector': '.gs_fl a',&#10;                'snippet_selector': '.gs_rs'&#10;            }&#10;        )&#10;    }&#10;&#10;class DevelopmentConfig(BaseConfig):&#10;    &quot;&quot;&quot;Vývojová konfigurace&quot;&quot;&quot;&#10;    DEBUG = True&#10;    SCRAPING = ScrapingConfig(&#10;        request_timeout=10,&#10;        max_retries=2,&#10;        concurrent_requests=3&#10;    )&#10;&#10;class ProductionConfig(BaseConfig):&#10;    &quot;&quot;&quot;Produkční konfigurace&quot;&quot;&quot;&#10;    DEBUG = False&#10;    SCRAPING = ScrapingConfig(&#10;        request_timeout=30,&#10;        max_retries=5,&#10;        retry_delay=2.0,&#10;        concurrent_requests=10&#10;    )&#10;&#10;class TestingConfig(BaseConfig):&#10;    &quot;&quot;&quot;Testovací konfigurace&quot;&quot;&quot;&#10;    TESTING = True&#10;    SCRAPING = ScrapingConfig(&#10;        request_timeout=5,&#10;        max_retries=1,&#10;        concurrent_requests=1&#10;    )&#10;&#10;class AppServiceConfig:&#10;    &quot;&quot;&quot;Zjednodušená konfigurace pro Azure App Service - bez Key Vault komplexity&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.logger = logging.getLogger(__name__)&#10;&#10;        # Automatická detekce prostředí&#10;        self.is_azure_app_service = bool(os.getenv('WEBSITE_SITE_NAME'))&#10;        self.is_development = not self.is_azure_app_service&#10;&#10;        if self.is_azure_app_service:&#10;            self.logger.info(&quot;✅ Running on Azure App Service&quot;)&#10;            self.config = ProductionConfig()&#10;        else:&#10;            self.logger.info(&quot; Running in development mode&quot;)&#10;            self.config = DevelopmentConfig()&#10;&#10;    def get_setting(self, setting_name: str, default: str = None, required: bool = False) -&gt; Optional[str]:&#10;        &quot;&quot;&quot;&#10;        Získání nastavení z App Service Application Settings nebo environment variables&#10;        &quot;&quot;&quot;&#10;        value = os.getenv(setting_name, default)&#10;&#10;        if required and not value:&#10;            error_msg = f&quot;❌ Required setting '{setting_name}' not found&quot;&#10;            self.logger.error(error_msg)&#10;            if self.is_azure_app_service:&#10;                raise ValueError(f&quot;Setting '{setting_name}' must be configured in App Service Application Settings&quot;)&#10;            else:&#10;                raise ValueError(f&quot;Environment variable '{setting_name}' is required&quot;)&#10;&#10;        return value&#10;&#10;# Factory function pro výběr konfigurace&#10;def get_config() -&gt; BaseConfig:&#10;    &quot;&quot;&quot;Vrátí konfiguraci podle prostředí&quot;&quot;&quot;&#10;    env = os.getenv('FLASK_ENV', 'development').lower()&#10;&#10;    if env == 'production':&#10;        return ProductionConfig()&#10;    elif env == 'testing':&#10;        return TestingConfig()&#10;    else:&#10;        return DevelopmentConfig()&#10;&#10;# Globální instance&#10;app_config = AppServiceConfig()&#10;config = get_config()&#10;&#10;# === KRITICKÉ NASTAVENÍ ===&#10;# API klíče (nakonfigurované v App Service Application Settings)&#10;GEMINI_API_KEY = app_config.get_setting(&quot;GEMINI_API_KEY&quot;, required=True)&#10;OPENAI_API_KEY = app_config.get_setting(&quot;OPENAI_API_KEY&quot;)  # Volitelné&#10;SERP_API_KEY = app_config.get_setting(&quot;SERP_API_KEY&quot;)     # Volitelné&#10;NEWS_API_KEY = app_config.get_setting(&quot;NEWS_API_KEY&quot;)     # Volitelné&#10;&#10;# Cosmos DB připojení&#10;COSMOS_DB_ENDPOINT = app_config.get_setting(&quot;COSMOS_DB_ENDPOINT&quot;, required=True)&#10;COSMOS_DB_KEY = app_config.get_setting(&quot;COSMOS_DB_KEY&quot;, required=True)&#10;COSMOS_DB_NAME = app_config.get_setting(&quot;COSMOS_DB_NAME&quot;, &quot;research_tool&quot;)&#10;&#10;# === APLIKAČNÍ NASTAVENÍ ===&#10;APP_NAME = app_config.get_setting(&quot;APP_NAME&quot;, &quot;ResearchTool&quot;)&#10;DEBUG = app_config.get_setting(&quot;DEBUG&quot;, &quot;false&quot;).lower() == &quot;true&quot;&#10;LOG_LEVEL = app_config.get_setting(&quot;LOG_LEVEL&quot;, &quot;INFO&quot;)&#10;ENVIRONMENT = app_config.get_setting(&quot;ENVIRONMENT&quot;, &quot;development&quot; if app_config.is_development else &quot;production&quot;)&#10;&#10;# === ULTRA ÚSPORNÉ NASTAVENÍ PRO MINIMÁLNÍ NÁKLADY ===&#10;# Cíl: být levnější než Perplexity ($20/měsíc)&#10;DAILY_COST_LIMIT = float(app_config.get_setting(&quot;DAILY_COST_LIMIT&quot;, &quot;0.50&quot;))  # Pouze $0.50/den = $15/měsíc&#10;MONTHLY_TARGET_COST = 15.0  # Úspora $5/měsíc oproti Perplexity&#10;MAX_CONCURRENT_REQUESTS = int(app_config.get_setting(&quot;MAX_CONCURRENT_REQUESTS&quot;, &quot;1&quot;))  # Sekvenční zpracování&#10;RATE_LIMIT_PER_MINUTE = int(app_config.get_setting(&quot;RATE_LIMIT_PER_MINUTE&quot;, &quot;3&quot;))  # Velmi konzervativní&#10;&#10;# === AGRESIVNÍ OPTIMALIZACE TOKENŮ ===&#10;ANALYSIS_CONFIG = {&#10;    'max_input_tokens': int(app_config.get_setting(&quot;MAX_INPUT_TOKENS&quot;, &quot;800&quot;)),    # Radikálně sníženo&#10;    'max_output_tokens': int(app_config.get_setting(&quot;MAX_OUTPUT_TOKENS&quot;, &quot;300&quot;)),  # Radikálně sníženo&#10;    'temperature': float(app_config.get_setting(&quot;ANALYSIS_TEMPERATURE&quot;, &quot;0.1&quot;)),   # Nízká pro konzistenci&#10;    'top_p': float(app_config.get_setting(&quot;ANALYSIS_TOP_P&quot;, &quot;0.7&quot;))&#10;}&#10;&#10;# === RESEARCH STRATEGIE - POUZE JEDNA OPTIMALIZOVANÁ ===&#10;RESEARCH_CONFIG = {&#10;    'max_sources': int(app_config.get_setting(&quot;MAX_SOURCES&quot;, &quot;5&quot;)),        # Pouze 5 zdrojů&#10;    'token_budget': int(app_config.get_setting(&quot;TOKEN_BUDGET&quot;, &quot;400&quot;)),    # Pouze 400 tokenů&#10;    'parallel_requests': 1,  # Vždy sekvenční&#10;    'cache_priority': 'highest',  # Maximální využití cache&#10;    'analysis_type': 'essential',   # Pouze nejnutnější analýza&#10;    'batch_size': int(app_config.get_setting(&quot;BATCH_SIZE&quot;, &quot;3&quot;))  # Malé batche&#10;}&#10;&#10;# === RESEARCH DEPTHS - ULTRA ÚSPORNÉ ===&#10;RESEARCH_DEPTHS = {&#10;    'quick': {&#10;        'max_sources': 3,&#10;        'token_budget': 200,&#10;        'analysis_type': 'basic',&#10;        'max_results': 8&#10;    },&#10;    'standard': {&#10;        'max_sources': 5,&#10;        'token_budget': 400,&#10;        'analysis_type': 'summary',&#10;        'max_results': 12&#10;    },&#10;    'thorough': {&#10;        'max_sources': 8,&#10;        'token_budget': 600,&#10;        'analysis_type': 'detailed',&#10;        'max_results': 20&#10;    }&#10;}&#10;&#10;# === UNIVERZÁLNÍ DOMAIN SUPPORT ===&#10;# Specializované klíčové slova pro různé domény&#10;DOMAIN_KEYWORDS = {&#10;    'medical': {&#10;        'nootropics', 'peptides', 'cognitive', 'depression', 'anxiety', 'adhd',&#10;        'bipolar', 'ptsd', 'autism', 'medication', 'therapy', 'clinical_trial',&#10;        'neurotransmitter', 'dopamine', 'serotonin', 'gaba', 'acetylcholine',&#10;        'treatment', 'drug', 'dosage', 'side_effects', 'efficacy', 'safety'&#10;    },&#10;    'technology': {&#10;        'artificial_intelligence', 'machine_learning', 'blockchain', 'quantum',&#10;        'cybersecurity', 'cloud_computing', 'api', 'framework', 'algorithm',&#10;        'programming', 'software', 'hardware', 'innovation', 'development'&#10;    },&#10;    'science': {&#10;        'research', 'experiment', 'hypothesis', 'methodology', 'data_analysis',&#10;        'peer_review', 'publication', 'journal', 'conference', 'discovery',&#10;        'theory', 'observation', 'measurement', 'statistical', 'significant'&#10;    },&#10;    'business': {&#10;        'strategy', 'marketing', 'finance', 'investment', 'startup', 'innovation',&#10;        'management', 'leadership', 'productivity', 'growth', 'revenue',&#10;        'market_analysis', 'competitive', 'customer', 'brand', 'digital'&#10;    },&#10;    'environment': {&#10;        'sustainability', 'climate_change', 'renewable_energy', 'carbon',&#10;        'ecosystem', 'biodiversity', 'pollution', 'conservation', 'green_tech',&#10;        'environmental_impact', 'recycling', 'solar', 'wind_energy'&#10;    },&#10;    'education': {&#10;        'learning', 'pedagogy', 'curriculum', 'assessment', 'educational_technology',&#10;        'student_engagement', 'teaching_methods', 'academic_performance',&#10;        'distance_learning', 'online_education', 'classroom', 'university'&#10;    },&#10;    'general': {&#10;        'research', 'study', 'analysis', 'investigation', 'findings', 'results',&#10;        'conclusion', 'evidence', 'data', 'information', 'knowledge', 'insight'&#10;    }&#10;}&#10;&#10;# Zpětná kompatibilita - medical keywords jako hlavní&#10;MEDICAL_KEYWORDS = DOMAIN_KEYWORDS['medical']&#10;&#10;# === CACHE NASTAVENÍ - MAXIMÁLNÍ ÚSPORA ===&#10;ENABLE_CACHE = app_config.get_setting(&quot;ENABLE_CACHE&quot;, &quot;true&quot;).lower() == &quot;true&quot;&#10;CACHE_TTL_HOURS = int(app_config.get_setting(&quot;CACHE_TTL_HOURS&quot;, &quot;168&quot;))  # 7 dní - velmi dlouhá cache&#10;PREFER_CACHE_OVER_API = True&#10;MIN_CACHE_HIT_RATE = 0.8  # Minimálně 80% cache hit rate&#10;&#10;# === SCRAPING NASTAVENÍ - KONZERVATIVNÍ ===&#10;SCHOLAR_DELAY = float(app_config.get_setting(&quot;SCHOLAR_DELAY&quot;, &quot;12.0&quot;))  # Velmi pomalejší&#10;PUBMED_DELAY = float(app_config.get_setting(&quot;PUBMED_DELAY&quot;, &quot;4.0&quot;))     # Pomalejší&#10;REQUEST_TIMEOUT = int(app_config.get_setting(&quot;REQUEST_TIMEOUT&quot;, &quot;45&quot;))  # Delší timeout&#10;MAX_RETRY_ATTEMPTS = 1  # Pouze jeden pokus&#10;&#10;# === DOMAIN-SPECIFIC SOURCE MAPPING ===&#10;DOMAIN_SOURCES = {&#10;    'medical': ['pubmed', 'scholar', 'web'],&#10;    'technology': ['scholar', 'web', 'arxiv'],&#10;    'science': ['pubmed', 'scholar', 'arxiv', 'web'],&#10;    'business': ['scholar', 'web'],&#10;    'environment': ['scholar', 'web'],&#10;    'education': ['scholar', 'web'],&#10;    'general': ['scholar', 'web']&#10;}&#10;&#10;# === LOGGING KONFIGURACE ===&#10;LOGGING_CONFIG = {&#10;    'version': 1,&#10;    'disable_existing_loggers': False,&#10;    'formatters': {&#10;        'standard': {&#10;            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'&#10;        },&#10;    },&#10;    'handlers': {&#10;        'default': {&#10;            'level': LOG_LEVEL,&#10;            'formatter': 'standard',&#10;            'class': 'logging.StreamHandler',&#10;        },&#10;    },&#10;    'loggers': {&#10;        '': {&#10;            'handlers': ['default'],&#10;            'level': LOG_LEVEL,&#10;            'propagate': False&#10;        }&#10;    }&#10;}&#10;&#10;" />
              <option name="updatedContent" value="# Research Tool Configuration for Azure App Service&#10;import os&#10;from pathlib import Path&#10;from typing import Optional, Dict, Any, List, Union&#10;from dataclasses import dataclass, field&#10;from enum import Enum&#10;&#10;# Základní nastavení&#10;BASE_DIR = Path(__file__).parent&#10;DATA_DIR = BASE_DIR / &quot;data&quot;&#10;&#10;# Vytvořit složky pokud neexistují (jen pro lokální development)&#10;if not os.getenv('WEBSITE_SITE_NAME'):  # Azure App Service check&#10;    for dir_path in [DATA_DIR]:&#10;        dir_path.mkdir(exist_ok=True)&#10;&#10;class Environment(Enum):&#10;    &quot;&quot;&quot;Prostředí aplikace&quot;&quot;&quot;&#10;    DEVELOPMENT = &quot;development&quot;&#10;    TESTING = &quot;testing&quot;&#10;    PRODUCTION = &quot;production&quot;&#10;&#10;@dataclass&#10;class ScrapingConfig:&#10;    &quot;&quot;&quot;Konfigurace pro scraping s rozšířenými možnostmi&quot;&quot;&quot;&#10;    request_timeout: int = 30&#10;    max_retries: int = 3&#10;    retry_delay: float = 1.0&#10;    concurrent_requests: int = 5&#10;    user_agents: List[str] = field(default_factory=list)&#10;    max_concurrent_sources: int = 10&#10;    global_rate_limit: float = 0.1  # Minimální delay mezi requesty&#10;    cache_ttl_seconds: int = 3600  # TTL pro cache&#10;&#10;    def __post_init__(self):&#10;        if not self.user_agents:&#10;            self.user_agents = [&#10;                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',&#10;                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',&#10;                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',&#10;                'Mozilla/5.0 (compatible; AcademicBot/1.0; +http://example.com/bot)'&#10;            ]&#10;&#10;@dataclass&#10;class SourceConfig:&#10;    &quot;&quot;&quot;Rozšířená konfigurace pro jednotlivé zdroje&quot;&quot;&quot;&#10;    name: str&#10;    base_url: str&#10;    api_key_env: Optional[str] = None&#10;    rate_limit_delay: float = 1.0&#10;    max_retries: int = 3&#10;    timeout: int = 30&#10;    custom_headers: Dict[str, str] = field(default_factory=dict)&#10;    parser_config: Dict[str, Any] = field(default_factory=dict)&#10;    requires_proxy: bool = False&#10;    priority: int = 1  # 1 = highest priority&#10;    enabled: bool = True&#10;&#10;@dataclass&#10;class DatabaseConfig:&#10;    &quot;&quot;&quot;Konfigurace databáze&quot;&quot;&quot;&#10;    url: str = &quot;sqlite:///./research_data.db&quot;&#10;    echo: bool = False&#10;    pool_size: int = 5&#10;    max_overflow: int = 10&#10;    pool_timeout: int = 30&#10;&#10;@dataclass&#10;class CacheConfig:&#10;    &quot;&quot;&quot;Konfigurace cache systému&quot;&quot;&quot;&#10;    enabled: bool = True&#10;    backend: str = &quot;memory&quot;  # memory, redis, disk&#10;    ttl_seconds: int = 3600&#10;    max_size: int = 1000&#10;    redis_url: Optional[str] = None&#10;&#10;@dataclass&#10;class AppConfig:&#10;    &quot;&quot;&quot;Konfigurace Flask aplikace&quot;&quot;&quot;&#10;    debug: bool = False&#10;    testing: bool = False&#10;    secret_key: str = &quot;dev-secret-key-change-in-production&quot;&#10;    host: str = &quot;0.0.0.0&quot;&#10;    port: int = 5000&#10;    cors_origins: List[str] = field(default_factory=lambda: [&quot;http://localhost:3000&quot;, &quot;http://localhost:8501&quot;])&#10;&#10;class BaseConfig:&#10;    &quot;&quot;&quot;Základní konfigurace s rozšířenými možnostmi&quot;&quot;&quot;&#10;    ENVIRONMENT = Environment.DEVELOPMENT&#10;    &#10;    # Aplikační konfigurace&#10;    APP = AppConfig()&#10;    &#10;    # Scraping konfigurace&#10;    SCRAPING = ScrapingConfig()&#10;    &#10;    # Database konfigurace&#10;    DATABASE = DatabaseConfig()&#10;    &#10;    # Cache konfigurace  &#10;    CACHE = CacheConfig()&#10;&#10;    # Definice zdrojů s rozšířenými možnostmi&#10;    SOURCES = {&#10;        'wikipedia': SourceConfig(&#10;            name='Wikipedia',&#10;            base_url='https://en.wikipedia.org',&#10;            rate_limit_delay=0.5,&#10;            timeout=20,&#10;            priority=2,&#10;            parser_config={&#10;                'api_endpoint': '/w/api.php',&#10;                'content_selector': '#mw-content-text',&#10;                'title_selector': 'h1.firstHeading',&#10;                'summary_selector': '.mw-parser-output &gt; p:first-of-type',&#10;                'search_params': {&#10;                    'action': 'query',&#10;                    'format': 'json',&#10;                    'list': 'search',&#10;                    'srprop': 'snippet|titlesnippet|size|timestamp'&#10;                }&#10;            }&#10;        ),&#10;        'openalex': SourceConfig(&#10;            name='OpenAlex',&#10;            base_url='https://api.openalex.org',&#10;            rate_limit_delay=0.1,&#10;            timeout=25,&#10;            priority=1,&#10;            custom_headers={&#10;                'User-Agent': 'Research-Tool (mailto:your-email@example.com)',&#10;                'Accept': 'application/json'&#10;            },&#10;            parser_config={&#10;                'works_endpoint': '/works',&#10;                'authors_endpoint': '/authors',&#10;                'institutions_endpoint': '/institutions',&#10;                'default_params': {&#10;                    'per-page': 10,&#10;                    'sort': 'relevance_score:desc',&#10;                    'select': 'id,title,abstract,publication_year,doi,open_access,authorships,cited_by_count'&#10;                }&#10;            }&#10;        ),&#10;        'semantic_scholar': SourceConfig(&#10;            name='Semantic Scholar',&#10;            base_url='https://api.semanticscholar.org',&#10;            api_key_env='SEMANTIC_SCHOLAR_API_KEY',&#10;            rate_limit_delay=1.0,&#10;            timeout=30,&#10;            priority=1,&#10;            parser_config={&#10;                'paper_endpoint': '/graph/v1/paper',&#10;                'author_endpoint': '/graph/v1/author',&#10;                'search_endpoint': '/graph/v1/paper/search',&#10;                'default_fields': 'paperId,title,abstract,year,authors,citationCount,openAccessPdf,url'&#10;            }&#10;        ),&#10;        'google_scholar': SourceConfig(&#10;            name='Google Scholar',&#10;            base_url='https://scholar.google.com',&#10;            rate_limit_delay=2.0,&#10;            timeout=20,&#10;            priority=3,&#10;            requires_proxy=True,  # Může vyžadovat proxy&#10;            enabled=False,  # Disabled by default kvůli rate limiting&#10;            custom_headers={&#10;                'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'&#10;            },&#10;            parser_config={&#10;                'search_url': '/scholar',&#10;                'search_selector': '.gs_rt a',&#10;                'citation_selector': '.gs_fl a',&#10;                'snippet_selector': '.gs_rs',&#10;                'next_page_selector': '.gs_ico_nav_next'&#10;            }&#10;        ),&#10;        'arxiv': SourceConfig(&#10;            name='arXiv',&#10;            base_url='http://export.arxiv.org',&#10;            rate_limit_delay=3.0,  # arXiv má strict rate limiting&#10;            timeout=30,&#10;            priority=2,&#10;            parser_config={&#10;                'api_endpoint': '/api/query',&#10;                'default_params': {&#10;                    'search_query': 'all:{}',&#10;                    'start': 0,&#10;                    'max_results': 10,&#10;                    'sortBy': 'relevance',&#10;                    'sortOrder': 'descending'&#10;                }&#10;            }&#10;        )&#10;    }&#10;&#10;    @classmethod&#10;    def get_source_config(cls, source_name: str) -&gt; Optional[SourceConfig]:&#10;        &quot;&quot;&quot;Získá konfiguraci pro daný zdroj&quot;&quot;&quot;&#10;        return cls.SOURCES.get(source_name)&#10;    &#10;    @classmethod&#10;    def get_enabled_sources(cls) -&gt; Dict[str, SourceConfig]:&#10;        &quot;&quot;&quot;Vrátí pouze povolené zdroje&quot;&quot;&quot;&#10;        return {name: config for name, config in cls.SOURCES.items() if config.enabled}&#10;    &#10;    @classmethod&#10;    def get_sources_by_priority(cls) -&gt; List[tuple[str, SourceConfig]]:&#10;        &quot;&quot;&quot;Vrátí zdroje seřazené podle priority&quot;&quot;&quot;&#10;        enabled_sources = cls.get_enabled_sources()&#10;        return sorted(enabled_sources.items(), key=lambda x: x[1].priority)&#10;&#10;class DevelopmentConfig(BaseConfig):&#10;    &quot;&quot;&quot;Vývojová konfigurace&quot;&quot;&quot;&#10;    ENVIRONMENT = Environment.DEVELOPMENT&#10;    &#10;    def __init__(self):&#10;        super().__init__()&#10;        self.APP.debug = True&#10;        self.DATABASE.echo = True&#10;        self.CACHE.enabled = True&#10;        self.SCRAPING.concurrent_requests = 3&#10;&#10;class TestingConfig(BaseConfig):&#10;    &quot;&quot;&quot;Testovací konfigurace&quot;&quot;&quot;&#10;    ENVIRONMENT = Environment.TESTING&#10;    &#10;    def __init__(self):&#10;        super().__init__()&#10;        self.APP.testing = True&#10;        self.APP.debug = True&#10;        self.DATABASE.url = &quot;sqlite:///:memory:&quot;&#10;        self.CACHE.enabled = False&#10;        self.SCRAPING.request_timeout = 10&#10;        self.SCRAPING.max_retries = 1&#10;&#10;class ProductionConfig(BaseConfig):&#10;    &quot;&quot;&quot;Produkční konfigurace&quot;&quot;&quot;&#10;    ENVIRONMENT = Environment.PRODUCTION&#10;    &#10;    def __init__(self):&#10;        super().__init__()&#10;        self.APP.debug = False&#10;        self.APP.secret_key = os.getenv('SECRET_KEY', 'change-me-in-production')&#10;        &#10;        # Database z ENV variables&#10;        self.DATABASE.url = os.getenv('DATABASE_URL', 'sqlite:///./research_data.db')&#10;        &#10;        # Cache konfigurace&#10;        redis_url = os.getenv('REDIS_URL')&#10;        if redis_url:&#10;            self.CACHE.backend = &quot;redis&quot;&#10;            self.CACHE.redis_url = redis_url&#10;        &#10;        # Scraping optimizations for production&#10;        self.SCRAPING.concurrent_requests = 8&#10;        self.SCRAPING.max_concurrent_sources = 15&#10;&#10;# Config factory&#10;CONFIG_MAP = {&#10;    Environment.DEVELOPMENT: DevelopmentConfig,&#10;    Environment.TESTING: TestingConfig,&#10;    Environment.PRODUCTION: ProductionConfig&#10;}&#10;&#10;def get_config(env: Union[str, Environment] = None) -&gt; BaseConfig:&#10;    &quot;&quot;&quot;&#10;    Factory pro získání konfigurace podle prostředí&#10;    &quot;&quot;&quot;&#10;    if env is None:&#10;        env_str = os.getenv('FLASK_ENV', 'development').lower()&#10;        if env_str in ['dev', 'development']:&#10;            env = Environment.DEVELOPMENT&#10;        elif env_str in ['test', 'testing']:&#10;            env = Environment.TESTING&#10;        elif env_str in ['prod', 'production']:&#10;            env = Environment.PRODUCTION&#10;        else:&#10;            env = Environment.DEVELOPMENT&#10;    elif isinstance(env, str):&#10;        env = Environment(env.lower())&#10;    &#10;    config_class = CONFIG_MAP.get(env, DevelopmentConfig)&#10;    return config_class()&#10;&#10;# Validace konfigurace&#10;def validate_config(config: BaseConfig) -&gt; List[str]:&#10;    &quot;&quot;&quot;Validuje konfiguraci a vrací seznam chyb&quot;&quot;&quot;&#10;    errors = []&#10;    &#10;    # Validace zdrojů&#10;    for source_name, source_config in config.SOURCES.items():&#10;        if not source_config.base_url:&#10;            errors.append(f&quot;Source {source_name}: missing base_url&quot;)&#10;        &#10;        if source_config.rate_limit_delay &lt; 0:&#10;            errors.append(f&quot;Source {source_name}: rate_limit_delay must be &gt;= 0&quot;)&#10;            &#10;        if source_config.api_key_env and not os.getenv(source_config.api_key_env):&#10;            errors.append(f&quot;Source {source_name}: API key {source_config.api_key_env} not found in environment&quot;)&#10;    &#10;    # Validace app konfigurace&#10;    if config.APP.port &lt; 1 or config.APP.port &gt; 65535:&#10;        errors.append(&quot;APP.port must be between 1 and 65535&quot;)&#10;    &#10;    return errors&#10;&#10;# Export hlavních objektů&#10;__all__ = [&#10;    'BaseConfig', 'DevelopmentConfig', 'TestingConfig', 'ProductionConfig',&#10;    'ScrapingConfig', 'SourceConfig', 'DatabaseConfig', 'CacheConfig', 'AppConfig',&#10;    'Environment', 'get_config', 'validate_config'&#10;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/config_optimized.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/config_optimized.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Cost-Optimized Configuration - Minimální náklady, maximální efektivita&#10;Zaměřeno na free API sources a ultra-low cost operations&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;from dataclasses import dataclass&#10;from typing import Dict, List&#10;&#10;@dataclass&#10;class CostOptimizedConfig:&#10;    &quot;&quot;&quot;Ultra-optimalizovaná konfigurace pro minimální náklady&quot;&quot;&quot;&#10;    &#10;    # Free sources only - no API keys needed&#10;    FREE_SOURCES = {&#10;        'wikipedia': {&#10;            'enabled': True,&#10;            'rate_limit': 0.5,  # 2 requests per second&#10;            'max_concurrent': 3,&#10;            'cost_per_request': 0.0  # FREE&#10;        },&#10;        'pubmed': {&#10;            'enabled': True,&#10;            'rate_limit': 0.3,  # NCBI guidelines&#10;            'max_concurrent': 2,&#10;            'cost_per_request': 0.0  # FREE&#10;        }&#10;    }&#10;    &#10;    # Cost optimization settings&#10;    COST_OPTIMIZATION = {&#10;        'max_requests_per_query': 10,  # Limit total requests&#10;        'cache_ttl_hours': 24,         # Aggressive caching&#10;        'prefer_free_sources': True,   # Always prefer free APIs&#10;        'batch_processing': True,      # Batch requests for efficiency&#10;        'compression_enabled': True    # Compress responses&#10;    }&#10;    &#10;    # Performance vs cost balance&#10;    PERFORMANCE_PROFILE = 'cost_optimized'  # vs 'balanced' or 'performance'&#10;    &#10;    # Monitoring for cost control&#10;    COST_LIMITS = {&#10;        'max_daily_requests': 1000,    # Stay under free tier limits&#10;        'max_monthly_cost': 5.0,       # $5/month target vs $20 Perplexity&#10;        'alert_threshold': 0.8         # Alert at 80% of limits&#10;    }&#10;&#10;# Global config instance&#10;config = CostOptimizedConfig()&#10;&#10;def get_optimized_config():&#10;    &quot;&quot;&quot;Get cost-optimized configuration&quot;&quot;&quot;&#10;    return config" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/config_personal.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/config_personal.py" />
              <option name="updatedContent" value="# === ÚSPORNÉ NASTAVENÍ PRO MINIMÁLNÍ NÁKLADY ===&#10;DAILY_COST_LIMIT = 2.0  # Pouze 2 USD denně - úspora oproti Perplexity ($20/měsíc)&#10;MONTHLY_TARGET_COST = 15.0  # Cílové náklady za měsíc&#10;GEMINI_RATE_LIMIT = 30  # Pomalejší ale levnější&#10;MAX_CONCURRENT_REQUESTS = 3  # Méně paralelních požadavků = nižší náklady&#10;&#10;# === AGRESIVNÍ ÚSPORY ===&#10;BATCH_SIZE = 8  # Menší batche = levnější&#10;MAX_TOKEN_LIMIT = 2000  # Nižší limit tokenů&#10;PREFER_CACHE_OVER_API = True  # Preferuj cache před API voláními&#10;MIN_CACHE_HIT_RATE = 0.7  # Minimálně 70% cache hit rate&#10;&#10;# === OPTIMALIZACE PRO ÚSPORY ===&#10;CACHE_EXPIRY_HOURS = 72  # Delší cache = méně API volání&#10;ENABLE_AGGRESSIVE_DEDUPLICATION = True  # Odstranění duplikátů&#10;SMART_QUERY_REDUCTION = True  # Inteligentní zkracování dotazů&#10;&#10;# === TIMEOUT NASTAVENÍ PRO ÚSPORY ===&#10;GOOGLE_SCHOLAR_TIMEOUT = 8  # Delší timeout pro méně opakování&#10;PUBMED_TIMEOUT = 8&#10;MAX_RETRY_ATTEMPTS = 1  # Pouze 1 pokus = nižší náklady&#10;&#10;# === VÝSLEDKY - MÉNĚ JE VÍCE ===&#10;MAX_RESULTS_PER_SOURCE = 30  # Méně výsledků = levnější&#10;DEFAULT_MAX_RESULTS = 15  # Standardně jen 15 výsledků&#10;QUALITY_OVER_QUANTITY = True  # Kvalita před kvantitou&#10;&#10;# === RESEARCH DEPTHS - ÚSPORNÉ ===&#10;RESEARCH_DEPTHS = {&#10;    'shallow': {&#10;        'max_sources': 10,&#10;        'token_budget': 1000,&#10;        'analysis_type': 'quick'&#10;    },&#10;    'medium': {&#10;        'max_sources': 20,&#10;        'token_budget': 2000,&#10;        'analysis_type': 'summary'&#10;    },&#10;    'deep': {&#10;        'max_sources': 30,  # Sníženo z 100&#10;        'token_budget': 3000,  # Sníženo z 8000&#10;        'analysis_type': 'comprehensive'&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/database_manager.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/database_manager.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Azure-optimalizovaný Database Manager s PostgreSQL a connection pooling&#10;import asyncpg&#10;import asyncpg&#10;import asyncpg&#10;import logging&#10;from typing import List, Dict, Any, Optional&#10;from datetime import datetime, timedelta&#10;import json&#10;import os&#10;class AzureDatabaseManager:&#10;class AzureDatabaseManager:&#10;    &quot;&quot;&quot;Database manager optimalizovaný pro Azure Database for PostgreSQL&quot;&quot;&quot;&#10;            raise ValueError(&quot;DATABASE_URL environment variable is required for Azure deployment&quot;)&#10;                command_timeout=60,&#10;            # Parsování connection string pro asyncpg&#10;            self.pool = await asyncpg.create_pool(&#10;                self.connection_string,&#10;                min_size=2,&#10;                max_size=10,&#10;                command_timeout=60,&#10;                server_settings={&#10;                    'jit': 'off',  # Optimalizace pro Azure&#10;        self.pool: Optional[asyncpg.Pool] = None&#10;        self.connection_string = os.getenv('DATABASE_URL')&#10;class AzureDatabaseManager:&#10;        if not self.connection_string:&#10;            raise ValueError(&quot;DATABASE_URL environment variable is required for Azure deployment&quot;)&#10;                command_timeout=60,&#10;class AzureDatabaseManager:&#10;                command_timeout=60,&#10;    async def _create_tables(self):&#10;        &quot;&quot;&quot;Vytvoření databázových tabulek&quot;&quot;&quot;&#10;        &quot;&quot;&quot;Inicializace connection pool&quot;&quot;&quot;&#10;            # Research queries table&#10;            await conn.execute(&quot;&quot;&quot;&#10;                CREATE TABLE IF NOT EXISTS research_queries (&#10;                    id SERIAL PRIMARY KEY,&#10;                    query TEXT NOT NULL,&#10;                    domain VARCHAR(50) DEFAULT 'general',&#10;            # Parsování connection string pro asyncpg&#10;            self.pool = await asyncpg.create_pool(&#10;                self.connection_string,&#10;                min_size=2,&#10;                max_size=10,&#10;                command_timeout=60,&#10;                server_settings={&#10;                    'jit': 'off',  # Optimalizace pro Azure&#10;                    'application_name': 'ResearchTool'&#10;                }&#10;            )&#10;&#10;            self.logger.info(&quot;✅ Azure PostgreSQL connection pool initialized&quot;)&#10;&#10;            # Vytvoření tabulek&#10;            await self._create_tables()&#10;&#10;                    result_data JSONB NOT NULL,&#10;            self.logger.error(f&quot;❌ Failed to initialize Azure database: {e}&quot;)&#10;                    expires_at TIMESTAMP WITH TIME ZONE,&#10;&#10;    async def _create_tables(self):&#10;        &quot;&quot;&quot;Vytvoření databázových tabulek&quot;&quot;&quot;&#10;        async with self.pool.acquire() as conn:&#10;            # Research queries table&#10;            await conn.execute(&quot;&quot;&quot;&#10;                CREATE TABLE IF NOT EXISTS research_queries (&#10;                    id SERIAL PRIMARY KEY,&#10;                    query TEXT NOT NULL,&#10;                    domain VARCHAR(50) DEFAULT 'general',&#10;                    strategy VARCHAR(20) DEFAULT 'balanced',&#10;                    sources JSONB,&#10;                    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),&#10;                    user_id VARCHAR(100),&#10;                    status VARCHAR(20) DEFAULT 'pending'&#10;                )&#10;            &quot;&quot;&quot;)&#10;&#10;            # Research results table&#10;            await conn.execute(&quot;&quot;&quot;&#10;                CREATE TABLE IF NOT EXISTS research_results (&#10;                    id SERIAL PRIMARY KEY,&#10;                    query_id INTEGER REFERENCES research_queries(id),&#10;                    query TEXT NOT NULL,&#10;                    sources_found INTEGER DEFAULT 0,&#10;                    total_tokens_used INTEGER DEFAULT 0,&#10;                    cost DECIMAL(10,6) DEFAULT 0,&#10;                    execution_time DECIMAL(8,3) DEFAULT 0,&#10;                    summary TEXT,&#10;                    key_findings JSONB,&#10;                    sources JSONB,&#10;                    strategy_used VARCHAR(20),&#10;                    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),&#10;                    metadata JSONB&#10;                )&#10;            &quot;&quot;&quot;)&#10;&#10;            # Cost tracking table&#10;            await conn.execute(&quot;&quot;&quot;&#10;                CREATE TABLE IF NOT EXISTS cost_tracking (&#10;                    id SERIAL PRIMARY KEY,&#10;                    date DATE DEFAULT CURRENT_DATE,&#10;                    operation_type VARCHAR(50),&#10;                    tokens_used INTEGER DEFAULT 0,&#10;                    cost DECIMAL(10,6) DEFAULT 0,&#10;                    model VARCHAR(50),&#10;                    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),&#10;                    metadata JSONB&#10;                )&#10;            &quot;&quot;&quot;)&#10;&#10;            # Cache table for research results&#10;            await conn.execute(&quot;&quot;&quot;&#10;                CREATE TABLE IF NOT EXISTS research_cache (&#10;                    id SERIAL PRIMARY KEY,&#10;                    cache_key VARCHAR(255) UNIQUE NOT NULL,&#10;                    query_hash VARCHAR(64) NOT NULL,&#10;                    domain VARCHAR(50),&#10;                    result_data JSONB NOT NULL,&#10;                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),&#10;                    expires_at TIMESTAMP WITH TIME ZONE,&#10;                    hit_count INTEGER DEFAULT 0,&#10;                    last_accessed TIMESTAMP WITH TIME ZONE DEFAULT NOW()&#10;                )&#10;        async with self.pool.acquire() as conn:&#10;            result_id = await conn.fetchval(&quot;&quot;&quot;&#10;                INSERT INTO research_results (&#10;                    query, sources_found, total_tokens_used, cost, execution_time,&#10;                    summary, key_findings, sources, strategy_used, metadata&#10;                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)&#10;                RETURNING id&#10;            &quot;&quot;&quot;,&#10;                result_data.get('query'),&#10;                result_data.get('sources_found', 0),&#10;                result_data.get('total_tokens_used', 0),&#10;                result_data.get('cost', 0),&#10;                result_data.get('execution_time', 0),&#10;                result_data.get('summary'),&#10;        async with self.pool.acquire() as conn:&#10;            query = &quot;&quot;&quot;&#10;                SELECT rq.*, rr.sources_found, rr.cost, rr.execution_time, rr.strategy_used&#10;                FROM research_queries rq&#10;                LEFT JOIN research_results rr ON rq.id = rr.query_id&#10;                WHERE ($2::VARCHAR IS NULL OR rq.user_id = $2)&#10;                ORDER BY rq.timestamp DESC&#10;                LIMIT $1&#10;            &quot;&quot;&quot;&#10;&#10;            rows = await conn.fetch(query, limit, user_id)&#10;&#10;            history = []&#10;            for row in rows:&#10;                history.append({&#10;                    'id': row['id'],&#10;                    'query': row['query'],&#10;                    'domain': row['domain'],&#10;                    'strategy': row['strategy'],&#10;                    'sources_found': row['sources_found'],&#10;                    'cost': float(row['cost']) if row['cost'] else 0,&#10;                    'execution_time': float(row['execution_time']) if row['execution_time'] else 0,&#10;                    'timestamp': row['timestamp'].isoformat(),&#10;                    'strategy_used': row['strategy_used']&#10;    async def save_research_result(self, result_data: Dict[str, Any]) -&gt; int:&#10;        &quot;&quot;&quot;Uložení výsledků výzkumu&quot;&quot;&quot;&#10;        async with self.pool.acquire() as conn:&#10;            result_id = await conn.fetchval(&quot;&quot;&quot;&#10;                INSERT INTO research_results (&#10;                    query, sources_found, total_tokens_used, cost, execution_time,&#10;                    summary, key_findings, sources, strategy_used, metadata&#10;                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)&#10;                RETURNING id&#10;            &quot;&quot;&quot;,&#10;                result_data.get('query'),&#10;                result_data.get('sources_found', 0),&#10;                result_data.get('total_tokens_used', 0),&#10;                result_data.get('cost', 0),&#10;&#10;            return history&#10;&#10;    async def add_cost_entry(self, operation_type: str, tokens_used: int,&#10;&#10;            self.logger.debug(f&quot;Saved research result with ID: {result_id}&quot;)&#10;            return result_id&#10;&#10;    async def get_research_history(self, limit: int = 50, user_id: str = None) -&gt; List[Dict[str, Any]]:&#10;        &quot;&quot;&quot;Získání historie výzkumů&quot;&quot;&quot;&#10;        async with self.pool.acquire() as conn:&#10;            query = &quot;&quot;&quot;&#10;        async with self.pool.acquire() as conn:&#10;            await conn.execute(&quot;&quot;&quot;&#10;                INSERT INTO cost_tracking (operation_type, tokens_used, cost, model, metadata)&#10;                VALUES ($1, $2, $3, $4, $5)&#10;            &quot;&quot;&quot;, operation_type, tokens_used, cost, model, json.dumps(metadata or {}))&#10;&#10;                    'execution_time': float(row['execution_time']) if row['execution_time'] else 0,&#10;                    'timestamp': row['timestamp'].isoformat(),&#10;                    'strategy_used': row['strategy_used']&#10;                })&#10;&#10;            return history&#10;&#10;&#10;        async with self.pool.acquire() as conn:&#10;            result = await conn.fetchval(&quot;&quot;&quot;&#10;                SELECT COALESCE(SUM(cost), 0)&#10;                FROM cost_tracking&#10;                WHERE date = $1&#10;            &quot;&quot;&quot;, date)&#10;&#10;            return float(result) if result else 0.0&#10;&#10;                return json.loads(row['result_data'])&#10;&#10;            return None&#10;&#10;        async with self.pool.acquire() as conn:&#10;            result = await conn.fetchval(&quot;&quot;&quot;&#10;                SELECT COALESCE(SUM(cost), 0)&#10;                FROM cost_tracking&#10;                WHERE EXTRACT(YEAR FROM date) = $1 AND EXTRACT(MONTH FROM date) = $2&#10;            &quot;&quot;&quot;, year, month)&#10;&#10;            return float(result) if result else 0.0&#10;&#10;    async def store_cache_result(self, cache_key: str, query_hash: str,&#10;            self.logger.error(f&quot;Database health check failed: {e}&quot;)&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Azure Cosmos DB Database Manager - optimalizovaný pro Free Tier&#10;Využívá NoSQL API s minimálními náklady na RU/s&#10;&quot;&quot;&quot;&#10;import asyncio&#10;import logging&#10;from typing import List, Dict, Any, Optional&#10;from datetime import datetime, timedelta&#10;import json&#10;import os&#10;import hashlib&#10;&#10;# Azure Cosmos DB imports&#10;try:&#10;    from azure.cosmos.aio import CosmosClient&#10;    from azure.cosmos import PartitionKey, exceptions&#10;    COSMOS_AVAILABLE = True&#10;except ImportError:&#10;    COSMOS_AVAILABLE = False&#10;    CosmosClient = None&#10;&#10;class CosmosDBManager:&#10;    &quot;&quot;&quot;Database manager optimalizovaný pro Azure Cosmos DB Free Tier&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.logger = logging.getLogger(__name__)&#10;        self.client: Optional[CosmosClient] = None&#10;        self.database = None&#10;        &#10;        # Konfigurace z environment variables (App Service settings)&#10;        self.endpoint = os.getenv('COSMOS_DB_ENDPOINT')&#10;        self.key = os.getenv('COSMOS_DB_KEY')&#10;        self.database_name = os.getenv('COSMOS_DB_NAME', 'research_tool')&#10;        &#10;        if not self.endpoint or not self.key:&#10;            raise ValueError(&quot;COSMOS_DB_ENDPOINT and COSMOS_DB_KEY must be set in App Service settings&quot;)&#10;    &#10;    async def initialize(self):&#10;        &quot;&quot;&quot;Inicializace Cosmos DB klienta a kontejnerů&quot;&quot;&quot;&#10;        try:&#10;            # Vytvoření klienta&#10;            self.client = CosmosClient(self.endpoint, self.key)&#10;            &#10;            # Vytvoření databáze (pokud neexistuje)&#10;            self.database = await self.client.create_database_if_not_exists(&#10;                id=self.database_name,&#10;                offer_throughput=400  # Minimální RU/s pro sdílenou databázi&#10;            )&#10;            &#10;            self.logger.info(&quot;✅ Cosmos DB client initialized&quot;)&#10;            &#10;            # Vytvoření kontejnerů&#10;            await self._create_containers()&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;❌ Failed to initialize Cosmos DB: {e}&quot;)&#10;            raise&#10;    &#10;    async def _create_containers(self):&#10;        &quot;&quot;&quot;Vytvoření Cosmos DB kontejnerů (ekvivalent SQL tabulek)&quot;&quot;&quot;&#10;        containers = [&#10;            {&#10;                'id': 'research_queries',&#10;                'partition_key': PartitionKey(path=&quot;/user_id&quot;),&#10;                'unique_key_policy': None&#10;            },&#10;            {&#10;                'id': 'research_results', &#10;                'partition_key': PartitionKey(path=&quot;/strategy&quot;),&#10;                'unique_key_policy': None&#10;            },&#10;            {&#10;                'id': 'cost_tracking',&#10;                'partition_key': PartitionKey(path=&quot;/date&quot;),&#10;                'unique_key_policy': None&#10;            },&#10;            {&#10;                'id': 'research_cache',&#10;                'partition_key': PartitionKey(path=&quot;/domain&quot;),&#10;                'unique_key_policy': None&#10;            }&#10;        ]&#10;        &#10;        for container_config in containers:&#10;            try:&#10;                await self.database.create_container_if_not_exists(&#10;                    id=container_config['id'],&#10;                    partition_key=container_config['partition_key'],&#10;                    offer_throughput=None  # Používá sdílenou databázovou RU/s&#10;                )&#10;                self.logger.debug(f&quot;Container '{container_config['id']}' ready&quot;)&#10;            except Exception as e:&#10;                self.logger.error(f&quot;Failed to create container {container_config['id']}: {e}&quot;)&#10;                raise&#10;        &#10;        self.logger.info(&quot;✅ All Cosmos DB containers created/verified&quot;)&#10;    &#10;    async def save_research_query(self, query: str, domain: str = &quot;general&quot;, &#10;                                strategy: str = &quot;balanced&quot;, sources: List[str] = None,&#10;                                user_id: str = &quot;default&quot;) -&gt; str:&#10;        &quot;&quot;&quot;Uložení research dotazu do Cosmos DB&quot;&quot;&quot;&#10;        container = self.database.get_container_client('research_queries')&#10;        &#10;        document = {&#10;            'id': self._generate_id('query'),&#10;            'query': query,&#10;            'domain': domain,&#10;            'strategy': strategy,&#10;            'sources': sources or [],&#10;            'timestamp': datetime.now().isoformat(),&#10;            'user_id': user_id,&#10;            'status': 'pending'&#10;        }&#10;        &#10;        try:&#10;            result = await container.create_item(body=document)&#10;            self.logger.debug(f&quot;Saved research query with ID: {result['id']}&quot;)&#10;            return result['id']&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to save research query: {e}&quot;)&#10;            raise&#10;    &#10;    async def save_research_result(self, result_data: Dict[str, Any]) -&gt; str:&#10;        &quot;&quot;&quot;Uložení výsledků výzkumu&quot;&quot;&quot;&#10;        container = self.database.get_container_client('research_results')&#10;        &#10;        document = {&#10;            'id': self._generate_id('result'),&#10;            'query': result_data.get('query'),&#10;            'sources_found': result_data.get('sources_found', 0),&#10;            'total_tokens_used': result_data.get('total_tokens_used', 0),&#10;            'cost': result_data.get('cost', 0),&#10;            'execution_time': result_data.get('execution_time', 0),&#10;            'summary': result_data.get('summary'),&#10;            'key_findings': result_data.get('key_findings', []),&#10;            'sources': result_data.get('sources', []),&#10;            'strategy': result_data.get('strategy_used', 'unknown'),&#10;            'timestamp': datetime.now().isoformat(),&#10;            'metadata': result_data.get('metadata', {})&#10;        }&#10;        &#10;        try:&#10;            result = await container.create_item(body=document)&#10;            self.logger.debug(f&quot;Saved research result with ID: {result['id']}&quot;)&#10;            return result['id']&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to save research result: {e}&quot;)&#10;            raise&#10;    &#10;    async def get_research_history(self, limit: int = 50, user_id: str = &quot;default&quot;) -&gt; List[Dict[str, Any]]:&#10;        &quot;&quot;&quot;Získání historie výzkumů&quot;&quot;&quot;&#10;        container = self.database.get_container_client('research_queries')&#10;        &#10;        # NoSQL dotaz s optimalizací pro minimální RU consumption&#10;        query = &quot;&quot;&quot;&#10;        SELECT TOP @limit q.id, q.query, q.domain, q.strategy, q.timestamp, q.status&#10;        FROM research_queries q &#10;        WHERE q.user_id = @user_id &#10;        ORDER BY q.timestamp DESC&#10;        &quot;&quot;&quot;&#10;        &#10;        parameters = [&#10;            {&quot;name&quot;: &quot;@limit&quot;, &quot;value&quot;: limit},&#10;            {&quot;name&quot;: &quot;@user_id&quot;, &quot;value&quot;: user_id}&#10;        ]&#10;        &#10;        try:&#10;            items = []&#10;            async for item in container.query_items(&#10;                query=query, &#10;                parameters=parameters,&#10;                partition_key=user_id,&#10;                enable_cross_partition_query=False  # Optimalizace RU&#10;            ):&#10;                items.append({&#10;                    'id': item['id'],&#10;                    'query': item['query'],&#10;                    'domain': item['domain'],&#10;                    'strategy': item['strategy'],&#10;                    'timestamp': item['timestamp'],&#10;                    'status': item.get('status', 'completed')&#10;                })&#10;            &#10;            return items&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get research history: {e}&quot;)&#10;            return []&#10;    &#10;    async def add_cost_entry(self, operation_type: str, tokens_used: int, &#10;                           cost: float, model: str = &quot;gemini&quot;, metadata: Dict = None):&#10;        &quot;&quot;&quot;Přidání záznamu o nákladech&quot;&quot;&quot;&#10;        container = self.database.get_container_client('cost_tracking')&#10;        &#10;        today = datetime.now().date().isoformat()&#10;        document = {&#10;            'id': self._generate_id('cost'),&#10;            'date': today,&#10;            'operation_type': operation_type,&#10;            'tokens_used': tokens_used,&#10;            'cost': cost,&#10;            'model': model,&#10;            'timestamp': datetime.now().isoformat(),&#10;            'metadata': metadata or {}&#10;        }&#10;        &#10;        try:&#10;            await container.create_item(body=document)&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to add cost entry: {e}&quot;)&#10;    &#10;    async def get_daily_cost(self, date: datetime = None) -&gt; float:&#10;        &quot;&quot;&quot;Získání denních nákladů&quot;&quot;&quot;&#10;        if date is None:&#10;            date = datetime.now().date()&#10;        &#10;        date_str = date.isoformat()&#10;        container = self.database.get_container_client('cost_tracking')&#10;        &#10;        query = &quot;&quot;&quot;&#10;        SELECT VALUE SUM(c.cost)&#10;        FROM cost_tracking c&#10;        WHERE c.date = @date&#10;        &quot;&quot;&quot;&#10;        &#10;        parameters = [{&quot;name&quot;: &quot;@date&quot;, &quot;value&quot;: date_str}]&#10;        &#10;        try:&#10;            async for result in container.query_items(&#10;                query=query,&#10;                parameters=parameters,&#10;                partition_key=date_str,&#10;                enable_cross_partition_query=False&#10;            ):&#10;                return float(result) if result else 0.0&#10;            return 0.0&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get daily cost: {e}&quot;)&#10;            return 0.0&#10;    &#10;    async def get_monthly_cost(self, year: int = None, month: int = None) -&gt; float:&#10;        &quot;&quot;&quot;Získání měsíčních nákladů&quot;&quot;&quot;&#10;        now = datetime.now()&#10;        year = year or now.year&#10;        month = month or now.month&#10;        &#10;        # Formát YYYY-MM pro prefix match&#10;        month_prefix = f&quot;{year:04d}-{month:02d}&quot;&#10;        &#10;        container = self.database.get_container_client('cost_tracking')&#10;        &#10;        query = &quot;&quot;&quot;&#10;        SELECT VALUE SUM(c.cost)&#10;        FROM cost_tracking c&#10;        WHERE STARTSWITH(c.date, @month_prefix)&#10;        &quot;&quot;&quot;&#10;        &#10;        parameters = [{&quot;name&quot;: &quot;@month_prefix&quot;, &quot;value&quot;: month_prefix}]&#10;        &#10;        try:&#10;            async for result in container.query_items(&#10;                query=query,&#10;                parameters=parameters,&#10;                enable_cross_partition_query=True  # Nutné pro cross-partition query&#10;            ):&#10;                return float(result) if result else 0.0&#10;            return 0.0&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get monthly cost: {e}&quot;)&#10;            return 0.0&#10;    &#10;    async def store_cache_result(self, cache_key: str, query_hash: str, &#10;                               domain: str, result_data: Dict, ttl_hours: int = 24):&#10;        &quot;&quot;&quot;Uložení do cache&quot;&quot;&quot;&#10;        container = self.database.get_container_client('research_cache')&#10;        &#10;        expires_at = datetime.now() + timedelta(hours=ttl_hours)&#10;        &#10;        document = {&#10;            'id': cache_key,&#10;            'query_hash': query_hash,&#10;            'domain': domain,&#10;            'result_data': result_data,&#10;            'created_at': datetime.now().isoformat(),&#10;            'expires_at': expires_at.isoformat(),&#10;            'hit_count': 1,&#10;            'last_accessed': datetime.now().isoformat()&#10;        }&#10;        &#10;        try:&#10;            # Upsert - vytvoří nebo aktualizuje&#10;            await container.upsert_item(body=document)&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to store cache result: {e}&quot;)&#10;    &#10;    async def get_cached_result(self, cache_key: str, domain: str) -&gt; Optional[Dict]:&#10;        &quot;&quot;&quot;Získání z cache&quot;&quot;&quot;&#10;        container = self.database.get_container_client('research_cache')&#10;        &#10;        try:&#10;            item = await container.read_item(item=cache_key, partition_key=domain)&#10;            &#10;            # Kontrola expirace&#10;            expires_at = datetime.fromisoformat(item['expires_at'])&#10;            if expires_at &lt; datetime.now():&#10;                # Expired - smazat&#10;                await container.delete_item(item=cache_key, partition_key=domain)&#10;                return None&#10;            &#10;            # Aktualizace hit count&#10;            item['hit_count'] = item.get('hit_count', 0) + 1&#10;            item['last_accessed'] = datetime.now().isoformat()&#10;            await container.replace_item(item=cache_key, body=item)&#10;            &#10;            return item['result_data']&#10;            &#10;        except exceptions.CosmosResourceNotFoundError:&#10;            return None&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get cached result: {e}&quot;)&#10;            return None&#10;    &#10;    async def cleanup_expired_cache(self):&#10;        &quot;&quot;&quot;Vyčištění expired cache záznamů&quot;&quot;&quot;&#10;        container = self.database.get_container_client('research_cache')&#10;        &#10;        now = datetime.now().isoformat()&#10;        query = &quot;&quot;&quot;&#10;        SELECT c.id, c.domain&#10;        FROM research_cache c&#10;        WHERE c.expires_at &lt; @now&#10;        &quot;&quot;&quot;&#10;        &#10;        parameters = [{&quot;name&quot;: &quot;@now&quot;, &quot;value&quot;: now}]&#10;        &#10;        try:&#10;            deleted_count = 0&#10;            async for item in container.query_items(&#10;                query=query,&#10;                parameters=parameters,&#10;                enable_cross_partition_query=True&#10;            ):&#10;                await container.delete_item(item=item['id'], partition_key=item['domain'])&#10;                deleted_count += 1&#10;            &#10;            if deleted_count &gt; 0:&#10;                self.logger.info(f&quot;Cleaned up {deleted_count} expired cache entries&quot;)&#10;                &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to cleanup expired cache: {e}&quot;)&#10;    &#10;    async def health_check(self) -&gt; bool:&#10;        &quot;&quot;&quot;Health check pro Cosmos DB&quot;&quot;&quot;&#10;        try:&#10;            # Jednoduchý test čtení database info&#10;            database_properties = await self.database.read()&#10;            return True&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Cosmos DB health check failed: {e}&quot;)&#10;            return False&#10;    &#10;    def _generate_id(self, prefix: str) -&gt; str:&#10;        &quot;&quot;&quot;Generování unikátního ID&quot;&quot;&quot;&#10;        timestamp = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)&#10;        import uuid&#10;        unique_part = str(uuid.uuid4())[:8]&#10;        return f&quot;{prefix}_{timestamp}_{unique_part}&quot;&#10;    &#10;    async def close(self):&#10;        &quot;&quot;&quot;Uzavření Cosmos DB klienta&quot;&quot;&quot;&#10;        if self.client:&#10;            await self.client.close()&#10;            self.logger.info(&quot;Cosmos DB client closed&quot;)&#10;&#10;# Backward compatibility&#10;DatabaseManager = CosmosDBManager&#10;AzureDatabaseManager = CosmosDBManager" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/database_migrations.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/database_migrations.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Databázové migrace pro ResearchTool&#10;Podporuje SQLite i PostgreSQL&#10;&quot;&quot;&quot;&#10;from alembic import command&#10;from alembic.config import Config&#10;from alembic.script import ScriptDirectory&#10;from pathlib import Path&#10;import logging&#10;import asyncio&#10;from typing import Optional&#10;&#10;from settings import config, DatabaseType&#10;&#10;class DatabaseMigrationManager:&#10;    &quot;&quot;&quot;Správce databázových migrací&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.logger = logging.getLogger(__name__)&#10;        self.base_dir = config.base_dir&#10;        self.migrations_dir = self.base_dir / &quot;migrations&quot;&#10;        self.alembic_cfg_path = self.base_dir / &quot;alembic.ini&quot;&#10;        &#10;    async def initialize_migrations(self):&#10;        &quot;&quot;&quot;Inicializace migračního systému&quot;&quot;&quot;&#10;        try:&#10;            if not self.migrations_dir.exists():&#10;                self.logger.info(&quot;Inicializuji Alembic migrace...&quot;)&#10;                &#10;                # Vytvoření alembic.ini&#10;                self._create_alembic_config()&#10;                &#10;                # Inicializace Alembic&#10;                alembic_cfg = Config(str(self.alembic_cfg_path))&#10;                command.init(alembic_cfg, str(self.migrations_dir))&#10;                &#10;                # Vytvoření první migrace&#10;                command.revision(&#10;                    alembic_cfg, &#10;                    message=&quot;Initial database schema&quot;,&#10;                    autogenerate=True&#10;                )&#10;                &#10;                self.logger.info(&quot;✅ Migrace úspěšně inicializovány&quot;)&#10;            else:&#10;                self.logger.info(&quot;Migrace již existují&quot;)&#10;                &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Chyba při inicializaci migrací: {e}&quot;)&#10;            raise&#10;    &#10;    def _create_alembic_config(self):&#10;        &quot;&quot;&quot;Vytvoření alembic.ini konfigurace&quot;&quot;&quot;&#10;        alembic_config = f&quot;&quot;&quot;# Alembic configuration file&#10;&#10;[alembic]&#10;script_location = migrations&#10;prepend_sys_path = .&#10;version_path_separator = os&#10;&#10;# Database URL - will be set programmatically&#10;sqlalchemy.url = {config.database_url}&#10;&#10;[post_write_hooks]&#10;&#10;[loggers]&#10;keys = root,sqlalchemy,alembic&#10;&#10;[handlers]&#10;keys = console&#10;&#10;[formatters]&#10;keys = generic&#10;&#10;[logger_root]&#10;level = WARN&#10;handlers = console&#10;qualname =&#10;&#10;[logger_sqlalchemy]&#10;level = WARN&#10;handlers =&#10;qualname = sqlalchemy.engine&#10;&#10;[logger_alembic]&#10;level = INFO&#10;handlers =&#10;qualname = alembic&#10;&#10;[handler_console]&#10;class = StreamHandler&#10;args = (sys.stderr,)&#10;level = NOTSET&#10;formatter = generic&#10;&#10;[formatter_generic]&#10;format = %(levelname)-5.5s [%(name)s] %(message)s&#10;datefmt = %H:%M:%S&#10;&quot;&quot;&quot;&#10;        with open(self.alembic_cfg_path, 'w', encoding='utf-8') as f:&#10;            f.write(alembic_config)&#10;    &#10;    async def run_migrations(self, target_revision: str = &quot;head&quot;):&#10;        &quot;&quot;&quot;Spuštění migrací&quot;&quot;&quot;&#10;        try:&#10;            self.logger.info(f&quot;Spouštím migrace do verze: {target_revision}&quot;)&#10;            &#10;            alembic_cfg = Config(str(self.alembic_cfg_path))&#10;            alembic_cfg.set_main_option(&quot;sqlalchemy.url&quot;, config.database_url)&#10;            &#10;            command.upgrade(alembic_cfg, target_revision)&#10;            &#10;            self.logger.info(&quot;✅ Migrace úspěšně dokončeny&quot;)&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Chyba při spouštění migrací: {e}&quot;)&#10;            raise&#10;    &#10;    async def create_migration(self, message: str, autogenerate: bool = True):&#10;        &quot;&quot;&quot;Vytvoření nové migrace&quot;&quot;&quot;&#10;        try:&#10;            self.logger.info(f&quot;Vytvářím novou migraci: {message}&quot;)&#10;            &#10;            alembic_cfg = Config(str(self.alembic_cfg_path))&#10;            alembic_cfg.set_main_option(&quot;sqlalchemy.url&quot;, config.database_url)&#10;            &#10;            command.revision(&#10;                alembic_cfg,&#10;                message=message,&#10;                autogenerate=autogenerate&#10;            )&#10;            &#10;            self.logger.info(&quot;✅ Nová migrace vytvořena&quot;)&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Chyba při vytváření migrace: {e}&quot;)&#10;            raise&#10;    &#10;    def get_current_revision(self) -&gt; Optional[str]:&#10;        &quot;&quot;&quot;Získání aktuální verze databáze&quot;&quot;&quot;&#10;        try:&#10;            alembic_cfg = Config(str(self.alembic_cfg_path))&#10;            alembic_cfg.set_main_option(&quot;sqlalchemy.url&quot;, config.database_url)&#10;            &#10;            script_dir = ScriptDirectory.from_config(alembic_cfg)&#10;            &#10;            with script_dir.env_context() as env_context:&#10;                return env_context.get_current_revision()&#10;                &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Chyba při získávání aktuální verze: {e}&quot;)&#10;            return None&#10;    &#10;    def get_migration_history(self) -&gt; list:&#10;        &quot;&quot;&quot;Získání historie migrací&quot;&quot;&quot;&#10;        try:&#10;            alembic_cfg = Config(str(self.alembic_cfg_path))&#10;            history = []&#10;            &#10;            script_dir = ScriptDirectory.from_config(alembic_cfg)&#10;            &#10;            for revision in script_dir.walk_revisions():&#10;                history.append({&#10;                    'revision': revision.revision,&#10;                    'down_revision': revision.down_revision,&#10;                    'message': revision.doc,&#10;                    'branch_labels': revision.branch_labels&#10;                })&#10;            &#10;            return history&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Chyba při získávání historie: {e}&quot;)&#10;            return []&#10;&#10;# Globální instance&#10;migration_manager = DatabaseMigrationManager()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/external_sources_config.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/external_sources_config.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Centralizovaná konfigurace externích zdrojů a RSS feedů&#10;Nahrazuje hardcoded hodnoty ve web_scraper.py&#10;&quot;&quot;&quot;&#10;from typing import Dict, List, Any&#10;from dataclasses import dataclass&#10;&#10;@dataclass&#10;class SourceConfig:&#10;    &quot;&quot;&quot;Konfigurace externího zdroje&quot;&quot;&quot;&#10;    name: str&#10;    base_url: str&#10;    type: str  # 'rss', 'api', 'scrape'&#10;    headers: Dict[str, str] = None&#10;    rate_limit: float = 1.0  # seconds between requests&#10;    quality_score: int = 5  # 1-10 scale&#10;    reliability: str = &quot;medium&quot;  # low, medium, high&#10;    requires_auth: bool = False&#10;&#10;# Medical RSS feeds - vysoce kvalitní zdroje&#10;MEDICAL_RSS_FEEDS = [&#10;    SourceConfig(&#10;        name=&quot;PubMed Latest&quot;,&#10;        base_url=&quot;https://pubmed.ncbi.nlm.nih.gov/rss/search/&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=9,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;New England Journal of Medicine&quot;,&#10;        base_url=&quot;https://www.nejm.org/action/showFeed?type=etoc&amp;feed=rss&quot;,&#10;        type=&quot;rss&quot;, &#10;        quality_score=10,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Nature Medicine&quot;,&#10;        base_url=&quot;https://feeds.nature.com/nm/rss/current&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=9,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;The Lancet&quot;,&#10;        base_url=&quot;https://www.thelancet.com/rssfeed/lancet_current.xml&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=10,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;BMJ Latest Research&quot;,&#10;        base_url=&quot;https://feeds.bmj.com/bmj/research&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=9,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Cochrane Library&quot;,&#10;        base_url=&quot;https://www.cochranelibrary.com/cdsr/rss&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=10,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Journal of Clinical Medicine&quot;,&#10;        base_url=&quot;https://www.mdpi.com/rss/journal/jcm&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=7,&#10;        reliability=&quot;medium&quot;&#10;    )&#10;]&#10;&#10;# Nootropics &amp; Cognitive Enhancement&#10;NOOTROPICS_RSS_FEEDS = [&#10;    SourceConfig(&#10;        name=&quot;Nootropics Expert&quot;,&#10;        base_url=&quot;https://nootropicsexpert.com/feed/&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=6,&#10;        reliability=&quot;medium&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Examine.com Nutrition&quot;,&#10;        base_url=&quot;https://examine.com/rss/&quot;,&#10;        type=&quot;rss&quot;, &#10;        quality_score=8,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Neurohacker Collective&quot;,&#10;        base_url=&quot;https://neurohacker.com/feed&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=6,&#10;        reliability=&quot;medium&quot;&#10;    )&#10;]&#10;&#10;# Academic &amp; Research feeds&#10;ACADEMIC_RSS_FEEDS = [&#10;    SourceConfig(&#10;        name=&quot;arXiv Neuroscience&quot;,&#10;        base_url=&quot;http://export.arxiv.org/rss/q-bio.NC&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=8,&#10;        reliability=&quot;high&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Science Daily - Neuroscience&quot;,&#10;        base_url=&quot;https://www.sciencedaily.com/rss/mind_brain/psychology.xml&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=7,&#10;        reliability=&quot;medium&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Frontiers in Neuroscience&quot;,&#10;        base_url=&quot;https://www.frontiersin.org/journals/neuroscience/rss&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=8,&#10;        reliability=&quot;high&quot;&#10;    )&#10;]&#10;&#10;# News feeds pro aktuální informace&#10;NEWS_RSS_FEEDS = [&#10;    SourceConfig(&#10;        name=&quot;Medical News Today&quot;,&#10;        base_url=&quot;https://www.medicalnewstoday.com/rss&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=6,&#10;        reliability=&quot;medium&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Healthline News&quot;,&#10;        base_url=&quot;https://www.healthline.com/health-news/rss&quot;,&#10;        type=&quot;rss&quot;, &#10;        quality_score=6,&#10;        reliability=&quot;medium&quot;&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Reuters Health&quot;,&#10;        base_url=&quot;https://www.reuters.com/news/health/rss&quot;,&#10;        type=&quot;rss&quot;,&#10;        quality_score=8,&#10;        reliability=&quot;high&quot;&#10;    )&#10;]&#10;&#10;# API endpoints pro strukturované data&#10;API_SOURCES = [&#10;    SourceConfig(&#10;        name=&quot;PubMed API&quot;,&#10;        base_url=&quot;https://eutils.ncbi.nlm.nih.gov/entrez/eutils/&quot;,&#10;        type=&quot;api&quot;,&#10;        quality_score=10,&#10;        reliability=&quot;high&quot;,&#10;        rate_limit=0.34  # 3 requests per second max&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;Semantic Scholar API&quot;, &#10;        base_url=&quot;https://api.semanticscholar.org/graph/v1/&quot;,&#10;        type=&quot;api&quot;,&#10;        quality_score=9,&#10;        reliability=&quot;high&quot;,&#10;        rate_limit=1.0&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;CrossRef API&quot;,&#10;        base_url=&quot;https://api.crossref.org/&quot;,&#10;        type=&quot;api&quot;,&#10;        quality_score=9,&#10;        reliability=&quot;high&quot;,&#10;        rate_limit=1.0&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;ClinicalTrials.gov API&quot;,&#10;        base_url=&quot;https://clinicaltrials.gov/api/&quot;,&#10;        type=&quot;api&quot;,&#10;        quality_score=10,&#10;        reliability=&quot;high&quot;,&#10;        rate_limit=1.0&#10;    )&#10;]&#10;&#10;# Scraping targets (use sparingly)&#10;SCRAPING_TARGETS = [&#10;    SourceConfig(&#10;        name=&quot;Google Scholar&quot;,&#10;        base_url=&quot;https://scholar.google.com/&quot;,&#10;        type=&quot;scrape&quot;,&#10;        quality_score=9,&#10;        reliability=&quot;medium&quot;,  # Can be blocked&#10;        rate_limit=5.0,  # Very conservative&#10;        headers={&#10;            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'&#10;        }&#10;    ),&#10;    SourceConfig(&#10;        name=&quot;ResearchGate&quot;,&#10;        base_url=&quot;https://www.researchgate.net/&quot;,&#10;        type=&quot;scrape&quot;,&#10;        quality_score=7,&#10;        reliability=&quot;low&quot;,  # Often blocks bots&#10;        rate_limit=10.0&#10;    )&#10;]&#10;&#10;class SourceRegistry:&#10;    &quot;&quot;&quot;Centrální registr všech zdrojů&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self._sources = {}&#10;        self._register_all_sources()&#10;    &#10;    def _register_all_sources(self):&#10;        &quot;&quot;&quot;Registrace všech zdrojů&quot;&quot;&quot;&#10;        all_sources = (&#10;            MEDICAL_RSS_FEEDS + &#10;            NOOTROPICS_RSS_FEEDS + &#10;            ACADEMIC_RSS_FEEDS + &#10;            NEWS_RSS_FEEDS + &#10;            API_SOURCES + &#10;            SCRAPING_TARGETS&#10;        )&#10;        &#10;        for source in all_sources:&#10;            self._sources[source.name.lower().replace(' ', '_')] = source&#10;    &#10;    def get_sources_by_domain(self, domain: str) -&gt; List[SourceConfig]:&#10;        &quot;&quot;&quot;Získání zdrojů podle domény&quot;&quot;&quot;&#10;        domain_mapping = {&#10;            'medical': MEDICAL_RSS_FEEDS + [s for s in API_SOURCES if 'pubmed' in s.name.lower() or 'clinical' in s.name.lower()],&#10;            'nootropics': NOOTROPICS_RSS_FEEDS + MEDICAL_RSS_FEEDS[:3],  # Top medical sources&#10;            'academic': ACADEMIC_RSS_FEEDS + API_SOURCES,&#10;            'news': NEWS_RSS_FEEDS,&#10;            'general': MEDICAL_RSS_FEEDS[:3] + ACADEMIC_RSS_FEEDS[:2] + NEWS_RSS_FEEDS[:2]&#10;        }&#10;        &#10;        return domain_mapping.get(domain, [])&#10;    &#10;    def get_source(self, name: str) -&gt; SourceConfig:&#10;        &quot;&quot;&quot;Získání konkrétního zdroje&quot;&quot;&quot;&#10;        key = name.lower().replace(' ', '_')&#10;        return self._sources.get(key)&#10;    &#10;    def get_high_quality_sources(self, min_quality: int = 8) -&gt; List[SourceConfig]:&#10;        &quot;&quot;&quot;Získání vysoce kvalitních zdrojů&quot;&quot;&quot;&#10;        return [source for source in self._sources.values() &#10;                if source.quality_score &gt;= min_quality]&#10;    &#10;    def get_reliable_sources(self, min_reliability: str = &quot;medium&quot;) -&gt; List[SourceConfig]:&#10;        &quot;&quot;&quot;Získání spolehlivých zdrojů&quot;&quot;&quot;&#10;        reliability_order = {&quot;low&quot;: 0, &quot;medium&quot;: 1, &quot;high&quot;: 2}&#10;        min_level = reliability_order.get(min_reliability, 1)&#10;        &#10;        return [source for source in self._sources.values()&#10;                if reliability_order.get(source.reliability, 0) &gt;= min_level]&#10;    &#10;    def get_sources_by_type(self, source_type: str) -&gt; List[SourceConfig]:&#10;        &quot;&quot;&quot;Získání zdrojů podle typu&quot;&quot;&quot;&#10;        return [source for source in self._sources.values() &#10;                if source.type == source_type]&#10;    &#10;    def get_recommended_sources(self, domain: str, strategy: str = &quot;balanced&quot;) -&gt; List[SourceConfig]:&#10;        &quot;&quot;&quot;Získání doporučených zdrojů podle domény a strategie&quot;&quot;&quot;&#10;        domain_sources = self.get_sources_by_domain(domain)&#10;        &#10;        if strategy == &quot;cheap&quot;:&#10;            # Pouze RSS a API, žádné scraping&#10;            return [s for s in domain_sources if s.type in ['rss', 'api']][:5]&#10;        &#10;        elif strategy == &quot;fast&quot;:&#10;            # Preferuj API a rychlé RSS&#10;            api_sources = [s for s in domain_sources if s.type == 'api']&#10;            rss_sources = [s for s in domain_sources if s.type == 'rss'][:3]&#10;            return api_sources + rss_sources&#10;        &#10;        elif strategy == &quot;deep&quot;:&#10;            # Všechny dostupné zdroje včetně scrapingu&#10;            return domain_sources + [s for s in SCRAPING_TARGETS if s.quality_score &gt;= 7]&#10;        &#10;        else:  # balanced&#10;            # Mix kvalitních zdrojů&#10;            high_quality = [s for s in domain_sources if s.quality_score &gt;= 8]&#10;            return high_quality[:10]&#10;&#10;# Globální instance&#10;source_registry = SourceRegistry()&#10;&#10;# Convenience funkce pro zpětnou kompatibilitu&#10;def get_rss_feeds_for_domain(domain: str) -&gt; List[str]:&#10;    &quot;&quot;&quot;Získání RSS URL pro doménu (zpětná kompatibilita)&quot;&quot;&quot;&#10;    sources = source_registry.get_sources_by_domain(domain)&#10;    rss_sources = [s for s in sources if s.type == 'rss']&#10;    return [s.base_url for s in rss_sources]&#10;&#10;def get_api_endpoints_for_domain(domain: str) -&gt; List[str]:&#10;    &quot;&quot;&quot;Získání API endpoints pro doménu&quot;&quot;&quot;&#10;    sources = source_registry.get_sources_by_domain(domain)&#10;    api_sources = [s for s in sources if s.type == 'api']&#10;    return [s.base_url for s in api_sources]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/high_performance_cache.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/high_performance_cache.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Agresivní cache systém pro maximální výkon&#10;Optimalizováno pro soukromé použití&#10;&quot;&quot;&quot;&#10;import pickle&#10;import hashlib&#10;import time&#10;from pathlib import Path&#10;from typing import Any, Dict, Optional&#10;import logging&#10;from datetime import datetime, timedelta&#10;&#10;class HighPerformanceCache:&#10;    &quot;&quot;&quot;Vysoce výkonný cache systém s persistencí&quot;&quot;&quot;&#10;    &#10;    def __init__(self, cache_dir: Path, max_size_mb: int = 500):&#10;        self.cache_dir = Path(cache_dir)&#10;        self.cache_dir.mkdir(exist_ok=True)&#10;        self.max_size_bytes = max_size_mb * 1024 * 1024&#10;        self.memory_cache = {}  # Hot cache v paměti&#10;        self.access_times = {}  # Pro LRU&#10;        self.logger = logging.getLogger(__name__)&#10;        &#10;    def _get_cache_key(self, key: str) -&gt; str:&#10;        &quot;&quot;&quot;Rychlé generování cache klíče&quot;&quot;&quot;&#10;        return hashlib.md5(key.encode()).hexdigest()[:16]&#10;    &#10;    def _get_cache_path(self, cache_key: str) -&gt; Path:&#10;        &quot;&quot;&quot;Cesta k cache souboru&quot;&quot;&quot;&#10;        return self.cache_dir / f&quot;{cache_key}.cache&quot;&#10;    &#10;    def get(self, key: str, default=None) -&gt; Any:&#10;        &quot;&quot;&quot;Rychlé získání z cache&quot;&quot;&quot;&#10;        cache_key = self._get_cache_key(key)&#10;        &#10;        # 1. Zkus memory cache (nejrychlejší)&#10;        if cache_key in self.memory_cache:&#10;            self.access_times[cache_key] = time.time()&#10;            return self.memory_cache[cache_key]&#10;        &#10;        # 2. Zkus disk cache&#10;        cache_path = self._get_cache_path(cache_key)&#10;        if cache_path.exists():&#10;            try:&#10;                with open(cache_path, 'rb') as f:&#10;                    data = pickle.load(f)&#10;                &#10;                # Kontrola expiraci&#10;                if data['expires'] &gt; time.time():&#10;                    # Přidej do memory cache pro příště&#10;                    self.memory_cache[cache_key] = data['value']&#10;                    self.access_times[cache_key] = time.time()&#10;                    return data['value']&#10;                else:&#10;                    # Smaž expirované&#10;                    cache_path.unlink()&#10;            except:&#10;                # Pokud je cache poškozená, smaž ji&#10;                cache_path.unlink(missing_ok=True)&#10;        &#10;        return default&#10;    &#10;    def set(self, key: str, value: Any, ttl_hours: int = 24):&#10;        &quot;&quot;&quot;Rychlé uložení do cache&quot;&quot;&quot;&#10;        cache_key = self._get_cache_key(key)&#10;        expires = time.time() + (ttl_hours * 3600)&#10;        &#10;        # Ulož do memory cache&#10;        self.memory_cache[cache_key] = value&#10;        self.access_times[cache_key] = time.time()&#10;        &#10;        # Ulož na disk asynchronně (neblokuj)&#10;        cache_path = self._get_cache_path(cache_key)&#10;        try:&#10;            data = {'value': value, 'expires': expires}&#10;            with open(cache_path, 'wb') as f:&#10;                pickle.dump(data, f)&#10;        except Exception as e:&#10;            self.logger.warning(f&quot;Cache write failed: {e}&quot;)&#10;        &#10;        # Cleanup pokud je cache příliš velká&#10;        self._cleanup_if_needed()&#10;    &#10;    def _cleanup_if_needed(self):&#10;        &quot;&quot;&quot;Rychlý cleanup při překročení limitu&quot;&quot;&quot;&#10;        # Cleanup memory cache&#10;        if len(self.memory_cache) &gt; 1000:  # Max 1000 items v paměti&#10;            # Odstraň nejstarší&#10;            oldest_keys = sorted(self.access_times.items(), key=lambda x: x[1])[:100]&#10;            for key, _ in oldest_keys:&#10;                self.memory_cache.pop(key, None)&#10;                self.access_times.pop(key, None)&#10;        &#10;        # Cleanup disk cache (méně často)&#10;        if time.time() % 300 &lt; 1:  # Každých 5 minut&#10;            self._cleanup_disk_cache()&#10;    &#10;    def _cleanup_disk_cache(self):&#10;        &quot;&quot;&quot;Cleanup disk cache&quot;&quot;&quot;&#10;        try:&#10;            total_size = sum(f.stat().st_size for f in self.cache_dir.glob(&quot;*.cache&quot;))&#10;            &#10;            if total_size &gt; self.max_size_bytes:&#10;                # Seřaď podle access time a smaž nejstarší&#10;                cache_files = [(f, f.stat().st_mtime) for f in self.cache_dir.glob(&quot;*.cache&quot;)]&#10;                cache_files.sort(key=lambda x: x[1])&#10;                &#10;                for cache_file, _ in cache_files[:len(cache_files)//4]:  # Smaž 25%&#10;                    cache_file.unlink(missing_ok=True)&#10;        except Exception as e:&#10;            self.logger.warning(f&quot;Cache cleanup failed: {e}&quot;)&#10;&#10;# Globální instance pro celou aplikaci&#10;high_perf_cache = HighPerformanceCache(Path(&quot;cache&quot;))&#10;&#10;def cache_result(ttl_hours: int = 24):&#10;    &quot;&quot;&quot;Decorator pro cache výsledků funkcí&quot;&quot;&quot;&#10;    def decorator(func):&#10;        async def async_wrapper(*args, **kwargs):&#10;            # Vytvoř cache klíč z funkce a argumentů&#10;            cache_key = f&quot;{func.__name__}:{str(args)}:{str(sorted(kwargs.items()))}&quot;&#10;            &#10;            # Zkus cache&#10;            result = high_perf_cache.get(cache_key)&#10;            if result is not None:&#10;                return result&#10;            &#10;            # Spočítej a ulož&#10;            result = await func(*args, **kwargs)&#10;            high_perf_cache.set(cache_key, result, ttl_hours)&#10;            return result&#10;        &#10;        def sync_wrapper(*args, **kwargs):&#10;            cache_key = f&quot;{func.__name__}:{str(args)}:{str(sorted(kwargs.items()))}&quot;&#10;            &#10;            result = high_perf_cache.get(cache_key)&#10;            if result is not None:&#10;                return result&#10;            &#10;            result = func(*args, **kwargs)&#10;            high_perf_cache.set(cache_key, result, ttl_hours)&#10;            return result&#10;        &#10;        return async_wrapper if hasattr(func, '__code__') and func.__code__.co_flags &amp; 0x80 else sync_wrapper&#10;    return decorator" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/monitoring_metrics.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/monitoring_metrics.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Kompletní monitoring metriky s funkčními health checky&#10;&quot;&quot;&quot;&#10;import asyncio&#10;import logging&#10;import time&#10;from datetime import datetime, timedelta&#10;from typing import Dict, Any, List, Optional&#10;from dataclasses import dataclass, asdict&#10;import psutil&#10;import aiohttp&#10;&#10;from prometheus_client import Counter, Histogram, Gauge, start_http_server&#10;from settings import config&#10;&#10;@dataclass&#10;class HealthStatus:&#10;    &quot;&quot;&quot;Status zdraví komponenty&quot;&quot;&quot;&#10;    component: str&#10;    healthy: bool&#10;    message: str&#10;    response_time: float&#10;    last_check: datetime&#10;    details: Dict[str, Any] = None&#10;&#10;class ComponentHealthChecker:&#10;    &quot;&quot;&quot;Skutečné health checky komponent&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.logger = logging.getLogger(__name__)&#10;    &#10;    async def check_database(self) -&gt; HealthStatus:&#10;        &quot;&quot;&quot;Kontrola databázového připojení&quot;&quot;&quot;&#10;        start_time = time.time()&#10;        try:&#10;            from database_manager import DatabaseManager&#10;            db_manager = DatabaseManager()&#10;            &#10;            # Skutečný test databáze&#10;            await db_manager.execute_query(&quot;SELECT 1&quot;)&#10;            &#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;database&quot;,&#10;                healthy=True,&#10;                message=&quot;Database connection OK&quot;,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&quot;database_type&quot;: config.database_type.value}&#10;            )&#10;            &#10;        except Exception as e:&#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;database&quot;,&#10;                healthy=False,&#10;                message=f&quot;Database error: {str(e)[:100]}&quot;,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&quot;error&quot;: str(e)}&#10;            )&#10;    &#10;    async def check_cache(self) -&gt; HealthStatus:&#10;        &quot;&quot;&quot;Kontrola cache systému&quot;&quot;&quot;&#10;        start_time = time.time()&#10;        try:&#10;            from high_performance_cache import AdvancedCache&#10;            cache = AdvancedCache()&#10;            &#10;            # Test write/read&#10;            test_key = f&quot;health_check_{int(time.time())}&quot;&#10;            test_value = {&quot;test&quot;: True, &quot;timestamp&quot;: time.time()}&#10;            &#10;            await cache.set(test_key, test_value, ttl=60)&#10;            retrieved = await cache.get(test_key)&#10;            &#10;            if retrieved != test_value:&#10;                raise Exception(&quot;Cache read/write mismatch&quot;)&#10;            &#10;            # Cleanup&#10;            await cache.delete(test_key)&#10;            &#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;cache&quot;,&#10;                healthy=True,&#10;                message=&quot;Cache system OK&quot;,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&#10;                    &quot;cache_size&quot;: await cache.get_size(),&#10;                    &quot;hit_rate&quot;: await cache.get_hit_rate()&#10;                }&#10;            )&#10;            &#10;        except Exception as e:&#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;cache&quot;,&#10;                healthy=False,&#10;                message=f&quot;Cache error: {str(e)[:100]}&quot;,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&quot;error&quot;: str(e)}&#10;            )&#10;    &#10;    async def check_gemini_api(self) -&gt; HealthStatus:&#10;        &quot;&quot;&quot;Kontrola Gemini API&quot;&quot;&quot;&#10;        start_time = time.time()&#10;        try:&#10;            if not config.gemini_api_key:&#10;                raise Exception(&quot;Gemini API key not configured&quot;)&#10;            &#10;            from gemini_manager import GeminiAIManager&#10;            gemini = GeminiAIManager()&#10;            &#10;            # Jednoduchý test API&#10;            response = await gemini.generate_text(&quot;Test&quot;, max_tokens=10)&#10;            &#10;            if not response:&#10;                raise Exception(&quot;Empty response from API&quot;)&#10;            &#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;gemini_api&quot;,&#10;                healthy=True,&#10;                message=&quot;Gemini API OK&quot;,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&quot;api_version&quot;: &quot;gemini-pro&quot;}&#10;            )&#10;            &#10;        except Exception as e:&#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;gemini_api&quot;,&#10;                healthy=False,&#10;                message=f&quot;Gemini API error: {str(e)[:100]}&quot;,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&quot;error&quot;: str(e)}&#10;            )&#10;    &#10;    async def check_system_resources(self) -&gt; HealthStatus:&#10;        &quot;&quot;&quot;Kontrola systémových prostředků&quot;&quot;&quot;&#10;        start_time = time.time()&#10;        try:&#10;            # CPU usage&#10;            cpu_percent = psutil.cpu_percent(interval=1)&#10;            &#10;            # Memory usage&#10;            memory = psutil.virtual_memory()&#10;            memory_percent = memory.percent&#10;            &#10;            # Disk usage&#10;            disk = psutil.disk_usage('/')&#10;            disk_percent = disk.percent&#10;            &#10;            # Health thresholds&#10;            warnings = []&#10;            if cpu_percent &gt; 80:&#10;                warnings.append(f&quot;High CPU usage: {cpu_percent}%&quot;)&#10;            if memory_percent &gt; 85:&#10;                warnings.append(f&quot;High memory usage: {memory_percent}%&quot;)&#10;            if disk_percent &gt; 90:&#10;                warnings.append(f&quot;High disk usage: {disk_percent}%&quot;)&#10;            &#10;            healthy = len(warnings) == 0&#10;            message = &quot;System resources OK&quot; if healthy else f&quot;Warnings: {'; '.join(warnings)}&quot;&#10;            &#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;system_resources&quot;,&#10;                healthy=healthy,&#10;                message=message,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&#10;                    &quot;cpu_percent&quot;: cpu_percent,&#10;                    &quot;memory_percent&quot;: memory_percent,&#10;                    &quot;disk_percent&quot;: disk_percent,&#10;                    &quot;available_memory_gb&quot;: memory.available / (1024**3)&#10;                }&#10;            )&#10;            &#10;        except Exception as e:&#10;            response_time = time.time() - start_time&#10;            return HealthStatus(&#10;                component=&quot;system_resources&quot;,&#10;                healthy=False,&#10;                message=f&quot;System check error: {str(e)[:100]}&quot;,&#10;                response_time=response_time,&#10;                last_check=datetime.now(),&#10;                details={&quot;error&quot;: str(e)}&#10;            )&#10;&#10;class EnhancedMetricsCollector:&#10;    &quot;&quot;&quot;Vylepšený metrics collector s Prometheus metrikami&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.logger = logging.getLogger(__name__)&#10;        self.health_checker = ComponentHealthChecker()&#10;        &#10;        # Prometheus metriky&#10;        self.research_requests = Counter(&#10;            'research_requests_total', &#10;            'Total research requests',&#10;            ['strategy', 'domain', 'status']&#10;        )&#10;        &#10;        self.research_duration = Histogram(&#10;            'research_duration_seconds',&#10;            'Research request duration',&#10;            ['strategy']&#10;        )&#10;        &#10;        self.research_cost = Histogram(&#10;            'research_cost_dollars',&#10;            'Research request cost in dollars',&#10;            ['strategy']&#10;        )&#10;        &#10;        self.sources_found = Histogram(&#10;            'sources_found_count',&#10;            'Number of sources found',&#10;            ['strategy', 'domain']&#10;        )&#10;        &#10;        self.tokens_used = Histogram(&#10;            'tokens_used_count',&#10;            'Number of tokens used',&#10;            ['strategy', 'model']&#10;        )&#10;        &#10;        self.cache_operations = Counter(&#10;            'cache_operations_total',&#10;            'Cache operations',&#10;            ['operation', 'result']&#10;        )&#10;        &#10;        self.health_status = Gauge(&#10;            'component_health_status',&#10;            'Component health status (1=healthy, 0=unhealthy)',&#10;            ['component']&#10;        )&#10;        &#10;        self.system_cpu_usage = Gauge('system_cpu_usage_percent', 'CPU usage percentage')&#10;        self.system_memory_usage = Gauge('system_memory_usage_percent', 'Memory usage percentage')&#10;        self.daily_cost = Gauge('daily_cost_dollars', 'Total daily cost in dollars')&#10;        &#10;        # Interní statistiky&#10;        self.health_history: List[List[HealthStatus]] = []&#10;        self.prometheus_server_started = False&#10;    &#10;    async def start_prometheus_server(self, port: int = None):&#10;        &quot;&quot;&quot;Spuštění Prometheus serveru&quot;&quot;&quot;&#10;        if self.prometheus_server_started:&#10;            return&#10;        &#10;        port = port or config.prometheus_port&#10;        try:&#10;            start_http_server(port)&#10;            self.prometheus_server_started = True&#10;            self.logger.info(f&quot;Prometheus server spuštěn na portu {port}&quot;)&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Chyba při spouštění Prometheus serveru: {e}&quot;)&#10;            raise&#10;    &#10;    def record_research_completed(self, strategy: str, cost: float, &#10;                                sources_found: int, execution_time: float,&#10;                                domain: str = &quot;general&quot;, tokens_used: int = 0):&#10;        &quot;&quot;&quot;Záznam dokončeného výzkumu&quot;&quot;&quot;&#10;        self.research_requests.labels(&#10;            strategy=strategy, &#10;            domain=domain, &#10;            status='success'&#10;        ).inc()&#10;        &#10;        self.research_duration.labels(strategy=strategy).observe(execution_time)&#10;        self.research_cost.labels(strategy=strategy).observe(cost)&#10;        self.sources_found.labels(strategy=strategy, domain=domain).observe(sources_found)&#10;        &#10;        if tokens_used &gt; 0:&#10;            self.tokens_used.labels(strategy=strategy, model='gemini').observe(tokens_used)&#10;    &#10;    def record_research_error(self, error: str, strategy: str = &quot;unknown&quot;, domain: str = &quot;general&quot;):&#10;        &quot;&quot;&quot;Záznam chyby při výzkumu&quot;&quot;&quot;&#10;        self.research_requests.labels(&#10;            strategy=strategy,&#10;            domain=domain,&#10;            status='error'&#10;        ).inc()&#10;        &#10;        self.logger.error(f&quot;Research error recorded: {error}&quot;)&#10;    &#10;    def record_cache_operation(self, operation: str, result: str):&#10;        &quot;&quot;&quot;Záznam cache operace&quot;&quot;&quot;&#10;        self.cache_operations.labels(operation=operation, result=result).inc()&#10;    &#10;    async def update_system_metrics(self):&#10;        &quot;&quot;&quot;Aktualizace systémových metrik&quot;&quot;&quot;&#10;        try:&#10;            # CPU a paměť&#10;            cpu_percent = psutil.cpu_percent(interval=0.1)&#10;            memory_percent = psutil.virtual_memory().percent&#10;            &#10;            self.system_cpu_usage.set(cpu_percent)&#10;            self.system_memory_usage.set(memory_percent)&#10;            &#10;            # Denní náklady&#10;            from cost_tracker import CostTracker&#10;            cost_tracker = CostTracker()&#10;            daily_cost = await cost_tracker.get_daily_cost()&#10;            self.daily_cost.set(daily_cost)&#10;            &#10;        except Exception as e:&#10;            self.logger.warning(f&quot;Chyba při aktualizaci systémových metrik: {e}&quot;)&#10;    &#10;    async def perform_health_checks(self) -&gt; Dict[str, HealthStatus]:&#10;        &quot;&quot;&quot;Provedení všech health checků&quot;&quot;&quot;&#10;        checks = {&#10;            'database': self.health_checker.check_database(),&#10;            'cache': self.health_checker.check_cache(),&#10;            'gemini_api': self.health_checker.check_gemini_api(),&#10;            'system_resources': self.health_checker.check_system_resources()&#10;        }&#10;        &#10;        # Spuštění všech checků paralelně&#10;        results = {}&#10;        for name, check_coro in checks.items():&#10;            try:&#10;                result = await check_coro&#10;                results[name] = result&#10;                &#10;                # Aktualizace Prometheus metriky&#10;                self.health_status.labels(component=name).set(1 if result.healthy else 0)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Health check failed for {name}: {e}&quot;)&#10;                results[name] = HealthStatus(&#10;                    component=name,&#10;                    healthy=False,&#10;                    message=f&quot;Health check failed: {e}&quot;,&#10;                    response_time=0.0,&#10;                    last_check=datetime.now()&#10;                )&#10;                self.health_status.labels(component=name).set(0)&#10;        &#10;        # Uložení do historie&#10;        self.health_history.append(list(results.values()))&#10;        &#10;        # Udržování pouze posledních 100 záznamů&#10;        if len(self.health_history) &gt; 100:&#10;            self.health_history = self.health_history[-100:]&#10;        &#10;        return results&#10;    &#10;    def get_health_summary(self) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Získání souhrnu zdraví systému&quot;&quot;&quot;&#10;        if not self.health_history:&#10;            return {&quot;status&quot;: &quot;unknown&quot;, &quot;components&quot;: {}}&#10;        &#10;        latest_checks = {check.component: check for check in self.health_history[-1]}&#10;        &#10;        all_healthy = all(check.healthy for check in latest_checks.values())&#10;        &#10;        return {&#10;            &quot;status&quot;: &quot;healthy&quot; if all_healthy else &quot;unhealthy&quot;,&#10;            &quot;timestamp&quot;: datetime.now().isoformat(),&#10;            &quot;components&quot;: {name: asdict(check) for name, check in latest_checks.items()},&#10;            &quot;overall_health&quot;: all_healthy&#10;        }&#10;    &#10;    async def start_background_monitoring(self, interval: int = 60):&#10;        &quot;&quot;&quot;Spuštění background monitoringu&quot;&quot;&quot;&#10;        self.logger.info(f&quot;Spouštím background monitoring (interval: {interval}s)&quot;)&#10;        &#10;        while True:&#10;            try:&#10;                await self.perform_health_checks()&#10;                await self.update_system_metrics()&#10;                await asyncio.sleep(interval)&#10;                &#10;            except asyncio.CancelledError:&#10;                self.logger.info(&quot;Background monitoring ukončen&quot;)&#10;                break&#10;            except Exception as e:&#10;                self.logger.error(f&quot;Chyba v background monitoringu: {e}&quot;)&#10;                await asyncio.sleep(interval)&#10;&#10;# Globální instance&#10;metrics_collector = EnhancedMetricsCollector()&#10;&#10;# Backward compatibility&#10;MetricsCollector = EnhancedMetricsCollector&#10;HealthChecker = ComponentHealthChecker" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/optimized_academic_scraper.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/optimized_academic_scraper.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Optimalizovaný Academic Scraper - Řeší memory leaks a session management&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import aiohttp&#10;import logging&#10;from typing import Dict, List, Optional, Any&#10;from dataclasses import dataclass&#10;from abc import ABC, abstractmethod&#10;import time&#10;from contextlib import asynccontextmanager&#10;&#10;from unified_config import get_config&#10;&#10;@dataclass&#10;class ScrapingResult:&#10;    source: str&#10;    query: str&#10;    success: bool&#10;    data: Dict[str, Any]&#10;    error: Optional[str] = None&#10;    response_time: Optional[float] = None&#10;    cached: bool = False&#10;&#10;class SessionPool:&#10;    &quot;&quot;&quot;Singleton session pool pro efektivní správu HTTP connections&quot;&quot;&quot;&#10;    _instance = None&#10;    _session = None&#10;&#10;    def __new__(cls):&#10;        if cls._instance is None:&#10;            cls._instance = super().__new__(cls)&#10;        return cls._instance&#10;&#10;    async def get_session(self) -&gt; aiohttp.ClientSession:&#10;        &quot;&quot;&quot;Získá shared session nebo vytvoří novou&quot;&quot;&quot;&#10;        if self._session is None or self._session.closed:&#10;            config = get_config()&#10;            connector = aiohttp.TCPConnector(&#10;                limit=config.scraping.max_concurrent,&#10;                limit_per_host=5,&#10;                ttl_dns_cache=300,&#10;                use_dns_cache=True,&#10;            )&#10;&#10;            timeout = aiohttp.ClientTimeout(total=config.scraping.timeout)&#10;&#10;            self._session = aiohttp.ClientSession(&#10;                connector=connector,&#10;                timeout=timeout,&#10;                headers={'User-Agent': config.scraping.user_agents[0]}&#10;            )&#10;&#10;        return self._session&#10;&#10;    async def close(self):&#10;        &quot;&quot;&quot;Uzavře session&quot;&quot;&quot;&#10;        if self._session and not self._session.closed:&#10;            await self._session.close()&#10;            self._session = None&#10;&#10;class RateLimiter:&#10;    &quot;&quot;&quot;Thread-safe rate limiter&quot;&quot;&quot;&#10;    def __init__(self):&#10;        self._last_requests = {}&#10;        self._locks = {}&#10;        self.config = get_config()&#10;&#10;    async def acquire(self, source: str):&#10;        &quot;&quot;&quot;Získá permission pro request&quot;&quot;&quot;&#10;        if source not in self._locks:&#10;            self._locks[source] = asyncio.Lock()&#10;&#10;        async with self._locks[source]:&#10;            delay = self.config.scraping.rate_limits.get(source, 1.0)&#10;            last_time = self._last_requests.get(source, 0)&#10;            current_time = time.time()&#10;&#10;            if current_time - last_time &lt; delay:&#10;                wait_time = delay - (current_time - last_time)&#10;                await asyncio.sleep(wait_time)&#10;&#10;            self._last_requests[source] = time.time()&#10;&#10;class BaseScraper(ABC):&#10;    &quot;&quot;&quot;Optimalizovaný base scraper s shared resources&quot;&quot;&quot;&#10;&#10;    def __init__(self, source_name: str):&#10;        self.source_name = source_name&#10;        self.config = get_config()&#10;        self.source_config = self.config.get_source_config(source_name)&#10;        self.session_pool = SessionPool()&#10;        self.rate_limiter = RateLimiter()&#10;        self.logger = logging.getLogger(f&quot;{__name__}.{source_name}&quot;)&#10;&#10;    @abstractmethod&#10;    async def scrape(self, query: str) -&gt; ScrapingResult:&#10;        &quot;&quot;&quot;Hlavní scraping metoda&quot;&quot;&quot;&#10;        pass&#10;&#10;    @asynccontextmanager&#10;    async def get_session(self):&#10;        &quot;&quot;&quot;Context manager pro session&quot;&quot;&quot;&#10;        session = await self.session_pool.get_session()&#10;        try:&#10;            yield session&#10;        finally:&#10;            # Session se neuzavírá - zůstává v poolu&#10;            pass&#10;&#10;    async def make_request(self, url: str, **kwargs) -&gt; aiohttp.ClientResponse:&#10;        &quot;&quot;&quot;Robustní HTTP request s retry logikou&quot;&quot;&quot;&#10;        await self.rate_limiter.acquire(self.source_name)&#10;&#10;        async with self.get_session() as session:&#10;            for attempt in range(self.config.scraping.max_retries):&#10;                try:&#10;                    async with session.get(url, **kwargs) as response:&#10;                        response.raise_for_status()&#10;                        return response&#10;                except aiohttp.ClientError as e:&#10;                    if attempt == self.config.scraping.max_retries - 1:&#10;                        raise&#10;&#10;                    wait_time = self.config.scraping.retry_delay * (2 ** attempt)&#10;                    self.logger.warning(f&quot;Request failed, retrying in {wait_time}s: {e}&quot;)&#10;                    await asyncio.sleep(wait_time)&#10;&#10;class OptimizedWikipediaScraper(BaseScraper):&#10;    &quot;&quot;&quot;Optimalizovaný Wikipedia scraper&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__('wikipedia')&#10;&#10;    async def scrape(self, query: str) -&gt; ScrapingResult:&#10;        start_time = time.time()&#10;&#10;        try:&#10;            # Použij Wikipedia API místo scrapingu HTML&#10;            api_url = f&quot;{self.source_config['base_url']}/api/rest_v1/page/summary/{query.replace(' ', '_')}&quot;&#10;&#10;            async with self.get_session() as session:&#10;                await self.rate_limiter.acquire(self.source_name)&#10;&#10;                async with session.get(api_url) as response:&#10;                    if response.status == 200:&#10;                        data = await response.json()&#10;                        parsed_data = self._parse_api_response(data)&#10;                    else:&#10;                        # Fallback na HTML scraping&#10;                        parsed_data = await self._fallback_html_scraping(query)&#10;&#10;            response_time = time.time() - start_time&#10;&#10;            return ScrapingResult(&#10;                source=self.source_name,&#10;                query=query,&#10;                success=True,&#10;                data=parsed_data,&#10;                response_time=response_time&#10;            )&#10;&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Wikipedia scraping failed: {e}&quot;)&#10;            return ScrapingResult(&#10;                source=self.source_name,&#10;                query=query,&#10;                success=False,&#10;                data={},&#10;                error=str(e)&#10;            )&#10;&#10;    def _parse_api_response(self, data: dict) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Parse Wikipedia API response&quot;&quot;&quot;&#10;        return {&#10;            'title': data.get('title', ''),&#10;            'summary': data.get('extract', ''),&#10;            'url': data.get('content_urls', {}).get('desktop', {}).get('page', ''),&#10;            'thumbnail': data.get('thumbnail', {}).get('source', ''),&#10;            'source_type': 'api'&#10;        }&#10;&#10;    async def _fallback_html_scraping(self, query: str) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Fallback HTML scraping pokud API selže&quot;&quot;&quot;&#10;        # Implementace HTML scrapingu jako backup&#10;        return {&#10;            'title': query,&#10;            'summary': 'Content retrieved via fallback method',&#10;            'source_type': 'html_fallback'&#10;        }&#10;&#10;class OptimizedScrapingOrchestrator:&#10;    &quot;&quot;&quot;Optimalizovaný orchestrátor s connection pooling a error recovery&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.scrapers = {&#10;            'wikipedia': OptimizedWikipediaScraper(),&#10;            # Další scrapery...&#10;        }&#10;        self.session_pool = SessionPool()&#10;        self.logger = logging.getLogger(__name__)&#10;        self._semaphore = asyncio.Semaphore(get_config().scraping.max_concurrent)&#10;&#10;    async def scrape_all_sources(self, query: str, sources: Optional[List[str]] = None) -&gt; List[ScrapingResult]:&#10;        &quot;&quot;&quot;Optimalizované scraping s concurrency control&quot;&quot;&quot;&#10;        if sources is None:&#10;            sources = list(self.scrapers.keys())&#10;&#10;        # Filtruj dostupné scrapery&#10;        available_sources = [s for s in sources if s in self.scrapers]&#10;&#10;        # Použij semaphore pro omezení concurrent requests&#10;        async def limited_scrape(source: str):&#10;            async with self._semaphore:&#10;                return await self.scrapers[source].scrape(query)&#10;&#10;        # Spusť všechny scrapery současně&#10;        tasks = [limited_scrape(source) for source in available_sources]&#10;        results = await asyncio.gather(*tasks, return_exceptions=True)&#10;&#10;        # Zpracuj výsledky a exceptions&#10;        processed_results = []&#10;        for i, result in enumerate(results):&#10;            if isinstance(result, Exception):&#10;                source = available_sources[i]&#10;                self.logger.error(f&quot;Scraper {source} failed: {result}&quot;)&#10;                processed_results.append(ScrapingResult(&#10;                    source=source,&#10;                    query=query,&#10;                    success=False,&#10;                    data={},&#10;                    error=str(result)&#10;                ))&#10;            else:&#10;                processed_results.append(result)&#10;&#10;        return processed_results&#10;&#10;    async def cleanup(self):&#10;        &quot;&quot;&quot;Cleanup resources&quot;&quot;&quot;&#10;        await self.session_pool.close()&#10;&#10;# Factory functions&#10;def create_optimized_orchestrator() -&gt; OptimizedScrapingOrchestrator:&#10;    &quot;&quot;&quot;Factory pro optimalizovaný orchestrátor&quot;&quot;&quot;&#10;    return OptimizedScrapingOrchestrator()&#10;&#10;# Async context manager pro clean resource management&#10;@asynccontextmanager&#10;async def scraping_session():&#10;    &quot;&quot;&quot;Context manager pro scraping session&quot;&quot;&quot;&#10;    orchestrator = create_optimized_orchestrator()&#10;    try:&#10;        yield orchestrator&#10;    finally:&#10;        await orchestrator.cleanup()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Optimalizovaný Academic Scraper - Řeší memory leaks a session management&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import aiohttp&#10;import logging&#10;from typing import Dict, List, Optional, Any&#10;from dataclasses import dataclass&#10;from abc import ABC, abstractmethod&#10;import time&#10;from contextlib import asynccontextmanager&#10;&#10;from unified_config import get_config&#10;&#10;@dataclass&#10;class ScrapingResult:&#10;    source: str&#10;    query: str&#10;    success: bool&#10;    data: Dict[str, Any]&#10;    error: Optional[str] = None&#10;    response_time: Optional[float] = None&#10;    cached: bool = False&#10;&#10;class SessionPool:&#10;    &quot;&quot;&quot;Singleton session pool pro efektivní správu HTTP connections&quot;&quot;&quot;&#10;    _instance = None&#10;    _session = None&#10;&#10;    def __new__(cls):&#10;        if cls._instance is None:&#10;            cls._instance = super().__new__(cls)&#10;        return cls._instance&#10;&#10;    async def get_session(self) -&gt; aiohttp.ClientSession:&#10;        &quot;&quot;&quot;Získá shared session nebo vytvoří novou&quot;&quot;&quot;&#10;        if self._session is None or self._session.closed:&#10;            config = get_config()&#10;            connector = aiohttp.TCPConnector(&#10;                limit=config.scraping.max_concurrent,&#10;                limit_per_host=5,&#10;                ttl_dns_cache=300,&#10;                use_dns_cache=True,&#10;            )&#10;&#10;            timeout = aiohttp.ClientTimeout(total=config.scraping.timeout)&#10;&#10;            self._session = aiohttp.ClientSession(&#10;                connector=connector,&#10;                timeout=timeout,&#10;                headers={'User-Agent': config.scraping.user_agents[0]}&#10;            )&#10;&#10;        return self._session&#10;&#10;    async def close(self):&#10;        &quot;&quot;&quot;Uzavře session&quot;&quot;&quot;&#10;        if self._session and not self._session.closed:&#10;            await self._session.close()&#10;            self._session = None&#10;&#10;class RateLimiter:&#10;    &quot;&quot;&quot;Thread-safe rate limiter&quot;&quot;&quot;&#10;    def __init__(self):&#10;        self._last_requests = {}&#10;        self._locks = {}&#10;        self.config = get_config()&#10;&#10;    async def acquire(self, source: str):&#10;        &quot;&quot;&quot;Získá permission pro request&quot;&quot;&quot;&#10;        if source not in self._locks:&#10;            self._locks[source] = asyncio.Lock()&#10;&#10;        async with self._locks[source]:&#10;            delay = self.config.scraping.rate_limits.get(source, 1.0)&#10;            last_time = self._last_requests.get(source, 0)&#10;            current_time = time.time()&#10;&#10;            if current_time - last_time &lt; delay:&#10;                wait_time = delay - (current_time - last_time)&#10;                await asyncio.sleep(wait_time)&#10;&#10;            self._last_requests[source] = time.time()&#10;&#10;class BaseScraper(ABC):&#10;    &quot;&quot;&quot;Optimalizovaný base scraper s shared resources&quot;&quot;&quot;&#10;&#10;    def __init__(self, source_name: str):&#10;        self.source_name = source_name&#10;        self.config = get_config()&#10;        self.source_config = self.config.get_source_config(source_name)&#10;        self.session_pool = SessionPool()&#10;        self.rate_limiter = RateLimiter()&#10;        self.logger = logging.getLogger(f&quot;{__name__}.{source_name}&quot;)&#10;&#10;    @abstractmethod&#10;    async def scrape(self, query: str) -&gt; ScrapingResult:&#10;        &quot;&quot;&quot;Hlavní scraping metoda&quot;&quot;&quot;&#10;        pass&#10;&#10;    @asynccontextmanager&#10;    async def get_session(self):&#10;        &quot;&quot;&quot;Context manager pro session&quot;&quot;&quot;&#10;        session = await self.session_pool.get_session()&#10;        try:&#10;            yield session&#10;        finally:&#10;            # Session se neuzavírá - zůstává v poolu&#10;            pass&#10;&#10;    async def make_request(self, url: str, **kwargs) -&gt; dict:&#10;        &quot;&quot;&quot;Robustní HTTP request s retry logikou&quot;&quot;&quot;&#10;        await self.rate_limiter.acquire(self.source_name)&#10;        &#10;        async with self.get_session() as session:&#10;            for attempt in range(self.config.scraping.max_retries):&#10;                try:&#10;                    async with session.get(url, **kwargs) as response:&#10;                        response.raise_for_status()&#10;                        return await response.json()&#10;                except aiohttp.ClientError as e:&#10;                    if attempt == self.config.scraping.max_retries - 1:&#10;                        raise&#10;                    &#10;                    wait_time = self.config.scraping.retry_delay * (2 ** attempt)&#10;                    self.logger.warning(f&quot;Request failed, retrying in {wait_time}s: {e}&quot;)&#10;                    await asyncio.sleep(wait_time)&#10;&#10;class OptimizedWikipediaScraper(BaseScraper):&#10;    &quot;&quot;&quot;Optimalizovaný Wikipedia scraper&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__('wikipedia')&#10;&#10;    async def scrape(self, query: str) -&gt; ScrapingResult:&#10;        start_time = time.time()&#10;&#10;        try:&#10;            # Použij Wikipedia API místo scrapingu HTML&#10;            api_url = f&quot;{self.source_config['base_url']}/api/rest_v1/page/summary/{query.replace(' ', '_')}&quot;&#10;&#10;            async with self.get_session() as session:&#10;                await self.rate_limiter.acquire(self.source_name)&#10;&#10;                async with session.get(api_url) as response:&#10;                    if response.status == 200:&#10;                        data = await response.json()&#10;                        parsed_data = self._parse_api_response(data)&#10;                    else:&#10;                        # Fallback na HTML scraping&#10;                        parsed_data = await self._fallback_html_scraping(query)&#10;&#10;            response_time = time.time() - start_time&#10;&#10;            return ScrapingResult(&#10;                source=self.source_name,&#10;                query=query,&#10;                success=True,&#10;                data=parsed_data,&#10;                response_time=response_time&#10;            )&#10;&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Wikipedia scraping failed: {e}&quot;)&#10;            return ScrapingResult(&#10;                source=self.source_name,&#10;                query=query,&#10;                success=False,&#10;                data={},&#10;                error=str(e)&#10;            )&#10;&#10;    def _parse_api_response(self, data: dict) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Parse Wikipedia API response&quot;&quot;&quot;&#10;        return {&#10;            'title': data.get('title', ''),&#10;            'summary': data.get('extract', ''),&#10;            'url': data.get('content_urls', {}).get('desktop', {}).get('page', ''),&#10;            'thumbnail': data.get('thumbnail', {}).get('source', ''),&#10;            'source_type': 'api'&#10;        }&#10;&#10;    async def _fallback_html_scraping(self, query: str) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Fallback HTML scraping pokud API selže&quot;&quot;&quot;&#10;        # Implementace HTML scrapingu jako backup&#10;        return {&#10;            'title': query,&#10;            'summary': 'Content retrieved via fallback method',&#10;            'source_type': 'html_fallback'&#10;        }&#10;&#10;class OptimizedScrapingOrchestrator:&#10;    &quot;&quot;&quot;Optimalizovaný orchestrátor s connection pooling a error recovery&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.scrapers = {&#10;            'wikipedia': OptimizedWikipediaScraper(),&#10;            # Další scrapery...&#10;        }&#10;        self.session_pool = SessionPool()&#10;        self.logger = logging.getLogger(__name__)&#10;        self._semaphore = asyncio.Semaphore(get_config().scraping.max_concurrent)&#10;&#10;    async def scrape_all_sources(self, query: str, sources: Optional[List[str]] = None) -&gt; List[ScrapingResult]:&#10;        &quot;&quot;&quot;Optimalizované scraping s concurrency control&quot;&quot;&quot;&#10;        if sources is None:&#10;            sources = list(self.scrapers.keys())&#10;&#10;        # Filtruj dostupné scrapery&#10;        available_sources = [s for s in sources if s in self.scrapers]&#10;&#10;        # Použij semaphore pro omezení concurrent requests&#10;        async def limited_scrape(source: str):&#10;            async with self._semaphore:&#10;                return await self.scrapers[source].scrape(query)&#10;&#10;        # Spusť všechny scrapery současně&#10;        tasks = [limited_scrape(source) for source in available_sources]&#10;        results = await asyncio.gather(*tasks, return_exceptions=True)&#10;&#10;        # Zpracuj výsledky a exceptions&#10;        processed_results = []&#10;        for i, result in enumerate(results):&#10;            if isinstance(result, Exception):&#10;                source = available_sources[i]&#10;                self.logger.error(f&quot;Scraper {source} failed: {result}&quot;)&#10;                processed_results.append(ScrapingResult(&#10;                    source=source,&#10;                    query=query,&#10;                    success=False,&#10;                    data={},&#10;                    error=str(result)&#10;                ))&#10;            else:&#10;                processed_results.append(result)&#10;&#10;        return processed_results&#10;&#10;    async def cleanup(self):&#10;        &quot;&quot;&quot;Cleanup resources&quot;&quot;&quot;&#10;        await self.session_pool.close()&#10;&#10;# Factory functions&#10;def create_optimized_orchestrator() -&gt; OptimizedScrapingOrchestrator:&#10;    &quot;&quot;&quot;Factory pro optimalizovaný orchestrátor&quot;&quot;&quot;&#10;    return OptimizedScrapingOrchestrator()&#10;&#10;# Async context manager pro clean resource management&#10;@asynccontextmanager&#10;async def scraping_session():&#10;    &quot;&quot;&quot;Context manager pro scraping session&quot;&quot;&quot;&#10;    orchestrator = create_optimized_orchestrator()&#10;    try:&#10;        yield orchestrator&#10;    finally:&#10;        await orchestrator.cleanup()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/optimized_database_manager.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/optimized_database_manager.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Optimalized Database Manager - Řeší connection pooling, batch operations a cost optimization&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import logging&#10;from typing import List, Dict, Any, Optional, AsyncContextManager&#10;from datetime import datetime, timedelta&#10;import json&#10;import hashlib&#10;from contextlib import asynccontextmanager&#10;from dataclasses import dataclass, asdict&#10;&#10;from unified_config import get_config&#10;&#10;try:&#10;    from azure.cosmos.aio import CosmosClient&#10;    from azure.cosmos import PartitionKey, exceptions&#10;    COSMOS_AVAILABLE = True&#10;except ImportError:&#10;    COSMOS_AVAILABLE = False&#10;    import sqlite3&#10;    import aiosqlite&#10;&#10;@dataclass&#10;class QueryRecord:&#10;    &quot;&quot;&quot;Standardizovaný záznam pro research query&quot;&quot;&quot;&#10;    id: str&#10;    query: str&#10;    domain: str&#10;    strategy: str&#10;    sources: List[str]&#10;    user_id: str&#10;    timestamp: datetime&#10;    cost_estimate: float&#10;&#10;@dataclass&#10;class ResultRecord:&#10;    &quot;&quot;&quot;Standardizovaný záznam pro research result&quot;&quot;&quot;&#10;    id: str&#10;    query_id: str&#10;    strategy: str&#10;    sources_found: int&#10;    total_tokens: int&#10;    cost: float&#10;    execution_time: float&#10;    summary: str&#10;    key_findings: List[str]&#10;    timestamp: datetime&#10;&#10;class ConnectionPool:&#10;    &quot;&quot;&quot;Optimalizovaný connection pool&quot;&quot;&quot;&#10;    &#10;    def __init__(self, max_connections: int = 10):&#10;        self.max_connections = max_connections&#10;        self._connections = asyncio.Queue(maxsize=max_connections)&#10;        self._created_connections = 0&#10;        self.config = get_config()&#10;        &#10;    async def get_connection(self):&#10;        &quot;&quot;&quot;Získá connection z poolu&quot;&quot;&quot;&#10;        if self._connections.empty() and self._created_connections &lt; self.max_connections:&#10;            # Vytvoř novou connection&#10;            if COSMOS_AVAILABLE and self.config.database.type == &quot;cosmos&quot;:&#10;                connection = await self._create_cosmos_connection()&#10;            else:&#10;                connection = await self._create_sqlite_connection()&#10;            &#10;            self._created_connections += 1&#10;            return connection&#10;        else:&#10;            # Čekej na volnou connection&#10;            return await self._connections.get()&#10;    &#10;    async def return_connection(self, connection):&#10;        &quot;&quot;&quot;Vrátí connection do poolu&quot;&quot;&quot;&#10;        await self._connections.put(connection)&#10;    &#10;    async def _create_cosmos_connection(self):&#10;        &quot;&quot;&quot;Vytvoří Cosmos DB connection&quot;&quot;&quot;&#10;        from azure.cosmos.aio import CosmosClient&#10;        return CosmosClient(&#10;            self.config.database.url,&#10;            credential=self.config.ai.gemini_api_key  # Placeholder - should be cosmos key&#10;        )&#10;    &#10;    async def _create_sqlite_connection(self):&#10;        &quot;&quot;&quot;Vytvoří SQLite connection&quot;&quot;&quot;&#10;        return await aiosqlite.connect(self.config.database.url or &quot;:memory:&quot;)&#10;&#10;class BatchProcessor:&#10;    &quot;&quot;&quot;Optimalizuje databázové operace pomocí batchingu&quot;&quot;&quot;&#10;    &#10;    def __init__(self, batch_size: int = 25):&#10;        self.batch_size = batch_size&#10;        self._pending_operations = []&#10;        self._batch_timer = None&#10;        self.logger = logging.getLogger(__name__)&#10;    &#10;    async def add_operation(self, operation_type: str, data: Dict[str, Any]):&#10;        &quot;&quot;&quot;Přidá operaci do batch&quot;&quot;&quot;&#10;        self._pending_operations.append({&#10;            'type': operation_type,&#10;            'data': data,&#10;            'timestamp': datetime.utcnow()&#10;        })&#10;        &#10;        # Spusť batch pokud je plný&#10;        if len(self._pending_operations) &gt;= self.batch_size:&#10;            await self._execute_batch()&#10;        else:&#10;            # Nastav timer pro automatické spuštění&#10;            if self._batch_timer:&#10;                self._batch_timer.cancel()&#10;            self._batch_timer = asyncio.create_task(self._delayed_batch_execution())&#10;    &#10;    async def _delayed_batch_execution(self):&#10;        &quot;&quot;&quot;Spustí batch po timeoutu&quot;&quot;&quot;&#10;        await asyncio.sleep(5)  # 5 sekund delay&#10;        if self._pending_operations:&#10;            await self._execute_batch()&#10;    &#10;    async def _execute_batch(self):&#10;        &quot;&quot;&quot;Spustí všechny pending operace najednou&quot;&quot;&quot;&#10;        if not self._pending_operations:&#10;            return&#10;        &#10;        operations = self._pending_operations.copy()&#10;        self._pending_operations.clear()&#10;        &#10;        try:&#10;            # Seskup operace podle typu&#10;            grouped_operations = {}&#10;            for op in operations:&#10;                op_type = op['type']&#10;                if op_type not in grouped_operations:&#10;                    grouped_operations[op_type] = []&#10;                grouped_operations[op_type].append(op['data'])&#10;            &#10;            # Spusť batch operace&#10;            for op_type, data_list in grouped_operations.items():&#10;                await self._execute_batch_operation(op_type, data_list)&#10;                &#10;            self.logger.info(f&quot;✅ Executed batch with {len(operations)} operations&quot;)&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;❌ Batch execution failed: {e}&quot;)&#10;            # Vrať operace zpět do fronty pro retry&#10;            self._pending_operations.extend(operations)&#10;&#10;    async def _execute_batch_operation(self, operation_type: str, data_list: List[Dict]):&#10;        &quot;&quot;&quot;Spustí konkrétní batch operaci&quot;&quot;&quot;&#10;        # Implementace závisí na databázovém typu&#10;        pass&#10;&#10;class OptimizedDatabaseManager:&#10;    &quot;&quot;&quot;Optimalizovaný database manager s connection pooling a batch processing&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.config = get_config()&#10;        self.logger = logging.getLogger(__name__)&#10;        self.connection_pool = ConnectionPool(max_connections=self.config.database.connection_pool_size)&#10;        self.batch_processor = BatchProcessor()&#10;        self._initialized = False&#10;    &#10;    async def initialize(self):&#10;        &quot;&quot;&quot;Inicializace databázového managera&quot;&quot;&quot;&#10;        if self._initialized:&#10;            return&#10;        &#10;        try:&#10;            if COSMOS_AVAILABLE and self.config.database.type == &quot;cosmos&quot;:&#10;                await self._initialize_cosmos()&#10;            else:&#10;                await self._initialize_sqlite()&#10;            &#10;            self._initialized = True&#10;            self.logger.info(&quot;✅ Optimized Database Manager initialized&quot;)&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;❌ Database initialization failed: {e}&quot;)&#10;            raise&#10;    &#10;    @asynccontextmanager&#10;    async def get_connection(self):&#10;        &quot;&quot;&quot;Context manager pro connection&quot;&quot;&quot;&#10;        connection = await self.connection_pool.get_connection()&#10;        try:&#10;            yield connection&#10;        finally:&#10;            await self.connection_pool.return_connection(connection)&#10;    &#10;    async def save_research_query_optimized(self, query_record: QueryRecord) -&gt; str:&#10;        &quot;&quot;&quot;Optimalizované uložení research query pomocí batch processing&quot;&quot;&quot;&#10;        data = asdict(query_record)&#10;        data['timestamp'] = data['timestamp'].isoformat()&#10;        &#10;        await self.batch_processor.add_operation('save_query', data)&#10;        return query_record.id&#10;    &#10;    async def save_research_result_optimized(self, result_record: ResultRecord) -&gt; str:&#10;        &quot;&quot;&quot;Optimalizované uložení research result&quot;&quot;&quot;&#10;        data = asdict(result_record)&#10;        data['timestamp'] = data['timestamp'].isoformat()&#10;        &#10;        await self.batch_processor.add_operation('save_result', data)&#10;        return result_record.id&#10;    &#10;    async def get_cached_result(self, query_hash: str, max_age_hours: int = 24) -&gt; Optional[Dict]:&#10;        &quot;&quot;&quot;Optimalizované získání cached výsledku&quot;&quot;&quot;&#10;        async with self.get_connection() as connection:&#10;            if COSMOS_AVAILABLE and self.config.database.type == &quot;cosmos&quot;:&#10;                return await self._get_cosmos_cached_result(connection, query_hash, max_age_hours)&#10;            else:&#10;                return await self._get_sqlite_cached_result(connection, query_hash, max_age_hours)&#10;    &#10;    async def _initialize_cosmos(self):&#10;        &quot;&quot;&quot;Inicializace Cosmos DB&quot;&quot;&quot;&#10;        # Implementace Cosmos DB setup&#10;        self.logger.info(&quot;Initializing Cosmos DB...&quot;)&#10;    &#10;    async def _initialize_sqlite(self):&#10;        &quot;&quot;&quot;Inicializace SQLite&quot;&quot;&quot;&#10;        async with self.get_connection() as connection:&#10;            await connection.execute('''&#10;                CREATE TABLE IF NOT EXISTS research_queries (&#10;                    id TEXT PRIMARY KEY,&#10;                    query TEXT NOT NULL,&#10;                    domain TEXT,&#10;                    strategy TEXT,&#10;                    sources TEXT,&#10;                    user_id TEXT,&#10;                    timestamp TEXT,&#10;                    cost_estimate REAL&#10;                )&#10;            ''')&#10;            &#10;            await connection.execute('''&#10;                CREATE TABLE IF NOT EXISTS research_results (&#10;                    id TEXT PRIMARY KEY,&#10;                    query_id TEXT,&#10;                    strategy TEXT,&#10;                    sources_found INTEGER,&#10;                    total_tokens INTEGER,&#10;                    cost REAL,&#10;                    execution_time REAL,&#10;                    summary TEXT,&#10;                    key_findings TEXT,&#10;                    timestamp TEXT,&#10;                    FOREIGN KEY (query_id) REFERENCES research_queries (id)&#10;                )&#10;            ''')&#10;            &#10;            await connection.execute('''&#10;                CREATE TABLE IF NOT EXISTS research_cache (&#10;                    query_hash TEXT PRIMARY KEY,&#10;                    query TEXT,&#10;                    domain TEXT,&#10;                    result_data TEXT,&#10;                    timestamp TEXT,&#10;                    expires_at TEXT&#10;                )&#10;            ''')&#10;            &#10;            await connection.commit()&#10;            self.logger.info(&quot;✅ SQLite tables created/verified&quot;)&#10;    &#10;    async def _get_sqlite_cached_result(self, connection, query_hash: str, max_age_hours: int) -&gt; Optional[Dict]:&#10;        &quot;&quot;&quot;Získá cached výsledek z SQLite&quot;&quot;&quot;&#10;        cutoff_time = (datetime.utcnow() - timedelta(hours=max_age_hours)).isoformat()&#10;        &#10;        cursor = await connection.execute(&#10;            'SELECT result_data FROM research_cache WHERE query_hash = ? AND timestamp &gt; ?',&#10;            (query_hash, cutoff_time)&#10;        )&#10;        row = await cursor.fetchone()&#10;        &#10;        if row:&#10;            return json.loads(row[0])&#10;        return None&#10;    &#10;    async def flush_batch(self):&#10;        &quot;&quot;&quot;Manuálně spustí pending batch operace&quot;&quot;&quot;&#10;        await self.batch_processor._execute_batch()&#10;    &#10;    async def cleanup(self):&#10;        &quot;&quot;&quot;Cleanup resources&quot;&quot;&quot;&#10;        await self.flush_batch()&#10;        # Close connections if needed&#10;&#10;# Factory function&#10;def create_optimized_db_manager() -&gt; OptimizedDatabaseManager:&#10;    &quot;&quot;&quot;Factory pro optimalizovaný database manager&quot;&quot;&quot;&#10;    return OptimizedDatabaseManager()&#10;&#10;# Async context manager&#10;@asynccontextmanager&#10;async def database_session():&#10;    &quot;&quot;&quot;Context manager pro database session&quot;&quot;&quot;&#10;    db_manager = create_optimized_db_manager()&#10;    await db_manager.initialize()&#10;    try:&#10;        yield db_manager&#10;    finally:&#10;        await db_manager.cleanup()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/requirements_optimized.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/requirements_optimized.txt" />
              <option name="updatedContent" value="# Core dependencies for ResearchTool - Optimized for cost efficiency&#10;# Web framework&#10;Flask==3.1.1&#10;flask-cors==6.0.1&#10;&#10;# HTTP requests and scraping&#10;requests==2.31.0&#10;aiohttp==3.12.15&#10;beautifulsoup4==4.13.4&#10;&#10;# Data validation&#10;marshmallow==3.21.0&#10;&#10;# Academic sources&#10;scholarly==1.7.11&#10;&#10;# Database (lightweight)&#10;sqlite3  # Built-in Python module&#10;&#10;# Configuration and utilities&#10;python-dotenv==1.0.0&#10;pydantic==2.7.0&#10;&#10;# Development tools (optional)&#10;pytest==8.2.0&#10;black==25.1.0&#10;flake8==7.3.0&#10;&#10;# Total: ~15 core dependencies instead of 215" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_academic_scraper.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_academic_scraper.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Testy pro academic_scraper.py&#10;Testuje všechny scrapery s mock HTTP responses&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import pytest&#10;import requests_mock&#10;from unittest.mock import Mock, patch&#10;&#10;from academic_scraper import (&#10;    WikipediaScraper, OpenAlexScraper, SemanticScholarScraper, &#10;    GoogleScholarScraper, ScrapingOrchestrator, ScrapingResult&#10;)&#10;&#10;&#10;class TestWikipediaScraper:&#10;    &quot;&quot;&quot;Testy pro Wikipedia scraper&quot;&quot;&quot;&#10;    &#10;    @pytest.fixture&#10;    def scraper(self):&#10;        return WikipediaScraper()&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_successful_scrape(self, scraper):&#10;        &quot;&quot;&quot;Test úspěšného scrapingu Wikipedia&quot;&quot;&quot;&#10;        mock_html = &quot;&quot;&quot;&#10;        &lt;html&gt;&#10;            &lt;head&gt;&lt;title&gt;Test Article&lt;/title&gt;&lt;/head&gt;&#10;            &lt;body&gt;&#10;                &lt;h1 class=&quot;firstHeading&quot;&gt;Machine Learning&lt;/h1&gt;&#10;                &lt;div id=&quot;mw-content-text&quot;&gt;&#10;                    &lt;div class=&quot;mw-parser-output&quot;&gt;&#10;                        &lt;p&gt;Machine learning is a subset of artificial intelligence.&lt;/p&gt;&#10;                        &lt;p&gt;It involves algorithms that learn from data.&lt;/p&gt;&#10;                    &lt;/div&gt;&#10;                &lt;/div&gt;&#10;            &lt;/body&gt;&#10;        &lt;/html&gt;&#10;        &quot;&quot;&quot;&#10;        &#10;        with requests_mock.Mocker() as m:&#10;            m.get(requests_mock.ANY, text=mock_html, status_code=200)&#10;            &#10;            result = await scraper.scrape(&quot;machine learning&quot;)&#10;            &#10;            assert result.success is True&#10;            assert result.source == &quot;wikipedia&quot;&#10;            assert result.query == &quot;machine learning&quot;&#10;            assert &quot;Machine Learning&quot; in result.data['title']&#10;            assert &quot;artificial intelligence&quot; in result.data['summary']&#10;            assert result.response_time is not None&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_failed_request(self, scraper):&#10;        &quot;&quot;&quot;Test neúspěšného requestu&quot;&quot;&quot;&#10;        with requests_mock.Mocker() as m:&#10;            m.get(requests_mock.ANY, status_code=404)&#10;            &#10;            result = await scraper.scrape(&quot;nonexistent&quot;)&#10;            &#10;            assert result.success is False&#10;            assert result.error is not None&#10;            assert &quot;404&quot; in result.error or &quot;Request failed&quot; in result.error&#10;&#10;&#10;class TestOpenAlexScraper:&#10;    &quot;&quot;&quot;Testy pro OpenAlex scraper&quot;&quot;&quot;&#10;    &#10;    @pytest.fixture&#10;    def scraper(self):&#10;        return OpenAlexScraper()&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_successful_api_call(self, scraper):&#10;        &quot;&quot;&quot;Test úspěšného API volání&quot;&quot;&quot;&#10;        mock_response = {&#10;            &quot;results&quot;: [&#10;                {&#10;                    &quot;title&quot;: &quot;Deep Learning for Medical Diagnosis&quot;,&#10;                    &quot;authorships&quot;: [&#10;                        {&quot;author&quot;: {&quot;display_name&quot;: &quot;John Doe&quot;}},&#10;                        {&quot;author&quot;: {&quot;display_name&quot;: &quot;Jane Smith&quot;}}&#10;                    ],&#10;                    &quot;publication_year&quot;: 2023,&#10;                    &quot;doi&quot;: &quot;https://doi.org/10.1000/test&quot;,&#10;                    &quot;abstract&quot;: &quot;This paper explores deep learning applications...&quot;,&#10;                    &quot;cited_by_count&quot;: 42,&#10;                    &quot;id&quot;: &quot;https://openalex.org/W123456&quot;&#10;                }&#10;            ],&#10;            &quot;meta&quot;: {&quot;count&quot;: 1}&#10;        }&#10;        &#10;        with requests_mock.Mocker() as m:&#10;            m.get(requests_mock.ANY, json=mock_response, status_code=200)&#10;            &#10;            result = await scraper.scrape(&quot;deep learning&quot;)&#10;            &#10;            assert result.success is True&#10;            assert result.data['total_results'] == 1&#10;            assert len(result.data['works']) == 1&#10;            assert result.data['works'][0]['title'] == &quot;Deep Learning for Medical Diagnosis&quot;&#10;            assert len(result.data['works'][0]['authors']) == 2&#10;&#10;&#10;class TestSemanticScholarScraper:&#10;    &quot;&quot;&quot;Testy pro Semantic Scholar scraper&quot;&quot;&quot;&#10;    &#10;    @pytest.fixture&#10;    def scraper(self):&#10;        return SemanticScholarScraper()&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_api_with_results(self, scraper):&#10;        &quot;&quot;&quot;Test API s výsledky&quot;&quot;&quot;&#10;        mock_response = {&#10;            &quot;data&quot;: [&#10;                {&#10;                    &quot;title&quot;: &quot;Neural Networks in Healthcare&quot;,&#10;                    &quot;authors&quot;: [&#10;                        {&quot;name&quot;: &quot;Dr. Alice Brown&quot;},&#10;                        {&quot;name&quot;: &quot;Prof. Bob Wilson&quot;}&#10;                    ],&#10;                    &quot;year&quot;: 2022,&#10;                    &quot;abstract&quot;: &quot;Neural networks show promise in healthcare applications...&quot;,&#10;                    &quot;citationCount&quot;: 15,&#10;                    &quot;doi&quot;: &quot;10.1000/healthcare-ai&quot;,&#10;                    &quot;url&quot;: &quot;https://semanticscholar.org/paper/123&quot;&#10;                }&#10;            ],&#10;            &quot;total&quot;: 1&#10;        }&#10;        &#10;        with requests_mock.Mocker() as m:&#10;            m.get(requests_mock.ANY, json=mock_response, status_code=200)&#10;            &#10;            result = await scraper.scrape(&quot;neural networks healthcare&quot;)&#10;            &#10;            assert result.success is True&#10;            assert result.data['total_results'] == 1&#10;            assert len(result.data['papers']) == 1&#10;            assert &quot;Neural Networks&quot; in result.data['papers'][0]['title']&#10;&#10;&#10;class TestGoogleScholarScraper:&#10;    &quot;&quot;&quot;Testy pro Google Scholar scraper&quot;&quot;&quot;&#10;    &#10;    @pytest.fixture&#10;    def scraper(self):&#10;        return GoogleScholarScraper()&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_html_parsing(self, scraper):&#10;        &quot;&quot;&quot;Test parsování HTML z Google Scholar&quot;&quot;&quot;&#10;        mock_html = &quot;&quot;&quot;&#10;        &lt;html&gt;&#10;            &lt;body&gt;&#10;                &lt;div class=&quot;gs_r gs_or gs_scl&quot;&gt;&#10;                    &lt;div class=&quot;gs_rt&quot;&gt;&#10;                        &lt;h3&gt;&lt;a href=&quot;https://example.com/paper1&quot;&gt;AI in Medicine: A Review&lt;/a&gt;&lt;/h3&gt;&#10;                    &lt;/div&gt;&#10;                    &lt;div class=&quot;gs_rs&quot;&gt;This review examines the current state of AI in medical applications...&lt;/div&gt;&#10;                    &lt;div class=&quot;gs_fl&quot;&gt;&#10;                        &lt;a href=&quot;#&quot;&gt;Cited by 25&lt;/a&gt;&#10;                    &lt;/div&gt;&#10;                &lt;/div&gt;&#10;                &lt;div class=&quot;gs_r gs_or gs_scl&quot;&gt;&#10;                    &lt;div class=&quot;gs_rt&quot;&gt;&#10;                        &lt;h3&gt;&lt;a href=&quot;https://example.com/paper2&quot;&gt;Machine Learning Diagnostics&lt;/a&gt;&lt;/h3&gt;&#10;                    &lt;/div&gt;&#10;                    &lt;div class=&quot;gs_rs&quot;&gt;ML techniques for medical diagnostics show promising results...&lt;/div&gt;&#10;                    &lt;div class=&quot;gs_fl&quot;&gt;&#10;                        &lt;a href=&quot;#&quot;&gt;Cited by 12&lt;/a&gt;&#10;                    &lt;/div&gt;&#10;                &lt;/div&gt;&#10;            &lt;/body&gt;&#10;        &lt;/html&gt;&#10;        &quot;&quot;&quot;&#10;        &#10;        with requests_mock.Mocker() as m:&#10;            m.get(requests_mock.ANY, text=mock_html, status_code=200)&#10;            &#10;            result = await scraper.scrape(&quot;AI medicine&quot;)&#10;            &#10;            assert result.success is True&#10;            assert result.data['total_results'] == 2&#10;            assert len(result.data['results']) == 2&#10;            assert &quot;AI in Medicine&quot; in result.data['results'][0]['title']&#10;            assert &quot;https://example.com/paper1&quot; in result.data['results'][0]['url']&#10;&#10;&#10;class TestScrapingOrchestrator:&#10;    &quot;&quot;&quot;Testy pro orchestrátor scrapingu&quot;&quot;&quot;&#10;    &#10;    @pytest.fixture&#10;    def orchestrator(self):&#10;        return ScrapingOrchestrator()&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_scrape_all_sources_success(self, orchestrator):&#10;        &quot;&quot;&quot;Test scrapingu všech zdrojů s úspěchem&quot;&quot;&quot;&#10;        # Mock všechny scrapery&#10;        with patch.object(orchestrator.scrapers['wikipedia'], 'scrape') as mock_wiki, \&#10;             patch.object(orchestrator.scrapers['openalex'], 'scrape') as mock_openalex:&#10;            &#10;            mock_wiki.return_value = ScrapingResult(&#10;                source=&quot;wikipedia&quot;, query=&quot;test&quot;, success=True, data={&quot;title&quot;: &quot;Test&quot;}&#10;            )&#10;            mock_openalex.return_value = ScrapingResult(&#10;                source=&quot;openalex&quot;, query=&quot;test&quot;, success=True, data={&quot;works&quot;: []}&#10;            )&#10;            &#10;            results = await orchestrator.scrape_all_sources(&quot;test&quot;, [&quot;wikipedia&quot;, &quot;openalex&quot;])&#10;            &#10;            assert len(results) == 2&#10;            assert all(r.success for r in results)&#10;            assert results[0].source == &quot;wikipedia&quot;&#10;            assert results[1].source == &quot;openalex&quot;&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_scrape_single_source(self, orchestrator):&#10;        &quot;&quot;&quot;Test scrapingu jednoho zdroje&quot;&quot;&quot;&#10;        with patch.object(orchestrator.scrapers['wikipedia'], 'scrape') as mock_scraper:&#10;            mock_scraper.return_value = ScrapingResult(&#10;                source=&quot;wikipedia&quot;, query=&quot;test&quot;, success=True, data={&quot;title&quot;: &quot;Test&quot;}&#10;            )&#10;            &#10;            result = await orchestrator.scrape_single_source(&quot;test&quot;, &quot;wikipedia&quot;)&#10;            &#10;            assert result.success is True&#10;            assert result.source == &quot;wikipedia&quot;&#10;    &#10;    def test_get_available_sources(self, orchestrator):&#10;        &quot;&quot;&quot;Test získání dostupných zdrojů&quot;&quot;&quot;&#10;        sources = orchestrator.get_available_sources()&#10;        &#10;        assert isinstance(sources, list)&#10;        assert &quot;wikipedia&quot; in sources&#10;        assert &quot;openalex&quot; in sources&#10;        assert &quot;semantic_scholar&quot; in sources&#10;        assert &quot;google_scholar&quot; in sources&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_unknown_source(self, orchestrator):&#10;        &quot;&quot;&quot;Test neznámého zdroje&quot;&quot;&quot;&#10;        result = await orchestrator.scrape_single_source(&quot;test&quot;, &quot;unknown_source&quot;)&#10;        &#10;        assert result.success is False&#10;        assert &quot;Unknown source&quot; in result.error&#10;&#10;&#10;class TestRateLimiting:&#10;    &quot;&quot;&quot;Testy pro rate limiting&quot;&quot;&quot;&#10;    &#10;    @pytest.mark.asyncio&#10;    async def test_rate_limiter_delay(self):&#10;        &quot;&quot;&quot;Test že rate limiter skutečně čeka&quot;&quot;&quot;&#10;        from academic_scraper import RateLimiter&#10;        import time&#10;        &#10;        limiter = RateLimiter()&#10;        &#10;        # První request&#10;        start_time = time.time()&#10;        await limiter.wait_if_needed(&quot;test_source&quot;, 0.1)&#10;        first_duration = time.time() - start_time&#10;        &#10;        # Druhý request hned poté (měl by čekat)&#10;        start_time = time.time()&#10;        await limiter.wait_if_needed(&quot;test_source&quot;, 0.1)&#10;        second_duration = time.time() - start_time&#10;        &#10;        # První by měl být rychlý, druhý pomalejší kvůli čekání&#10;        assert first_duration &lt; 0.05  # Rychlý&#10;        assert second_duration &gt; 0.05  # Čekal&#10;&#10;&#10;@pytest.mark.asyncio&#10;async def test_convenience_functions():&#10;    &quot;&quot;&quot;Test convenience funkcí&quot;&quot;&quot;&#10;    from academic_scraper import scrape_all_sources, scrape_source&#10;    &#10;    with patch('academic_scraper.create_scraping_orchestrator') as mock_factory:&#10;        mock_orchestrator = Mock()&#10;        mock_orchestrator.scrape_all_sources.return_value = [&#10;            ScrapingResult(source=&quot;test&quot;, query=&quot;test&quot;, success=True, data={})&#10;        ]&#10;        mock_orchestrator.scrape_single_source.return_value = ScrapingResult(&#10;            source=&quot;test&quot;, query=&quot;test&quot;, success=True, data={}&#10;        )&#10;        mock_factory.return_value = mock_orchestrator&#10;        &#10;        # Test scrape_all_sources&#10;        results = await scrape_all_sources(&quot;test query&quot;)&#10;        assert len(results) == 1&#10;        &#10;        # Test scrape_source&#10;        result = await scrape_source(&quot;test query&quot;, &quot;wikipedia&quot;)&#10;        assert result.success is True" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>