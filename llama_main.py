Llama 3.1 8B Main - PokroÄilÃ½ research tool optimalizovanÃ½ pro MacBook Air M1
VyuÅ¾Ã­vÃ¡ Llama 3.1 8B pro vysokou kvalitu vÃ½zkumu s inteligentnÃ­m fallbackem
"""

import asyncio
import argparse
import sys
import time
import json
from pathlib import Path
from typing import Optional, Dict, Any

# Llama 3.1 8B optimized imports
try:
    from unified_config import get_config
    from unified_research_engine import M1OptimizedResearchEngine, M1OptimizedResearchRequest
    from local_ai_adapter import M1OptimizedLlamaClient, quick_ai_query
    from cache_manager import get_cache_manager
    IMPORTS_OK = True
except ImportError as e:
    print(f"âŒ Chyba pÅ™i importu: {e}")
    IMPORTS_OK = False
    sys.exit(1)

class LlamaResearchTool:
    """PokroÄilÃ½ research tool s Llama 3.1 8B a inteligentnÃ­m model managementem"""

    def __init__(self):
        self.config = get_config()
        print(f"ğŸ§  Llama 3.1 8B Research Tool inicializovÃ¡n v {self.config.environment.value} mÃ³du")

        # OvÄ›Å™ konfigurace
        errors = self.config.validate()
        if errors:
            print("âš ï¸  VarovÃ¡nÃ­ konfigurace:")
            for error in errors:
                print(f"   â€¢ {error}")

    async def research_query(
        self,
        query: str,
        strategy: str = "balanced",
        sources: Optional[list] = None,
        require_quality: bool = False,
        output_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """Provede pokroÄilÃ½ research s Llama 3.1 8B"""

        print(f"ğŸ” Zahajuji high-quality research: '{query}'")
        print(f"ğŸ“Š Strategie: {strategy}")
        print(f"ğŸ¯ High-quality mode: {'Ano' if require_quality else 'Auto-detect'}")

        start_time = time.time()

        # AutomatickÃ¡ detekce kvality podle dotazu
        auto_quality = self._should_use_quality_mode(query, require_quality)

        # VytvoÅ™ research request s enhanced parametry
        request = M1OptimizedResearchRequest(
            query=query,
            strategy=strategy,
            sources=sources,
            use_ai_analysis=True,
            max_sources=3 if strategy == "fast" else 4,
            timeout=45 if auto_quality else 30
        )

        # SpusÅ¥ research engine s Llama optimalizacemi
        async with M1OptimizedResearchEngine() as engine:
            result = await engine.research(request)

        execution_time = time.time() - start_time

        # Enhanced vÃ½sledky s model info
        await self._display_enhanced_results(result, execution_time, auto_quality)

        # UloÅ¾ do souboru pokud poÅ¾adovÃ¡no
        if output_file:
            await self._save_enhanced_results(result, output_file, auto_quality)
            print(f"ğŸ’¾ VÃ½sledky uloÅ¾eny do: {output_file}")

        return result.to_dict()

    def _should_use_quality_mode(self, query: str, force_quality: bool) -> bool:
        """InteligentnÄ› rozhodne o pouÅ¾itÃ­ quality mÃ³du"""
        if force_quality:
            return True

        # AutomatickÃ¡ detekce podle komplexity dotazu
        quality_indicators = [
            len(query) > 100,
            any(word in query.lower() for word in [
                'analyze', 'compare', 'explain', 'research', 'detailed',
                'comprehensive', 'academic', 'scientific', 'technical',
                'trends', 'future', 'implications', 'impact'
            ]),
            '?' in query and len(query.split('?')) > 1,  # VÃ­ce otÃ¡zek
            query.count(',') > 2,  # SloÅ¾itÄ›jÅ¡Ã­ struktura
        ]

        return sum(quality_indicators) >= 2

    async def _display_enhanced_results(self, result, execution_time: float, quality_mode: bool):
        """ZobrazÃ­ enhanced vÃ½sledky s model statistikami"""

        print(f"\nâœ… Research dokonÄen za {execution_time:.2f}s")
        print(f"ğŸ“š PouÅ¾itÃ© zdroje: {', '.join(result.sources_used)}")
        print(f"ğŸ“ Nalezeno vÃ½sledkÅ¯: {len(result.results)}")
        print(f"ğŸ’¾ Cache hit: {'Ano' if result.cache_hit else 'Ne'}")
        print(f"ğŸ§  SpotÅ™eba pamÄ›ti: {result.memory_usage_mb:.1f}MB")
        print(f"ğŸ¯ Quality mode: {'AktivnÃ­' if quality_mode else 'Standard'}")

        if result.ai_analysis:
            print(f"\nğŸ¤– AI AnalÃ½za (Llama 3.1 8B):")
            print("=" * 60)
            print(f"{result.ai_analysis}")
            print("=" * 60)

        # Zobraz top vÃ½sledky s enhanced formÃ¡tovÃ¡nÃ­m
        print(f"\nğŸ“– NejlepÅ¡Ã­ vÃ½sledky:")
        for i, res in enumerate(result.results[:5], 1):
            if isinstance(res, dict) and 'title' in res:
                title = res.get('title', 'Bez nÃ¡zvu')[:70]
                source = res.get('source', 'NeznÃ¡mÃ½')
                print(f"   {i}. [{source}] {title}")

                # Zobraz summary pokud existuje
                summary = res.get('summary', res.get('abstract', ''))
                if summary:
                    summary_short = summary[:150] + "..." if len(summary) > 150 else summary
                    print(f"      ğŸ“„ {summary_short}")

    async def _save_enhanced_results(self, result, output_file: str, quality_mode: bool):
        """UloÅ¾Ã­ enhanced vÃ½sledky s metadaty"""
        output_path = Path(output_file)

        enhanced_data = {
            **result.to_dict(),
            'enhanced_metadata': {
                'quality_mode_used': quality_mode,
                'model_info': await self._get_model_info(),
                'optimization_level': 'llama_3_1_8b',
                'saved_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'tool_version': '2.0.0'
            }
        }

        if output_path.suffix.lower() == '.json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
        else:
            # Enhanced text format
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(f"ğŸ§  LLAMA 3.1 8B RESEARCH REPORT\n")
                f.write(f"=" * 50 + "\n\n")
                f.write(f"Query: {result.query}\n")
                f.write(f"Datum: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Strategie: {result.strategy_used}\n")
                f.write(f"Quality Mode: {'AktivnÃ­' if quality_mode else 'Standard'}\n")
                f.write(f"Zdroje: {', '.join(result.sources_used)}\n")
                f.write(f"Doba zpracovÃ¡nÃ­: {result.execution_time:.2f}s\n")
                f.write(f"SpotÅ™eba pamÄ›ti: {result.memory_usage_mb:.1f}MB\n\n")

                if result.ai_analysis:
                    f.write(f"ğŸ¤– AI ANALÃZA (Llama 3.1 8B):\n")
                    f.write(f"-" * 40 + "\n")
                    f.write(f"{result.ai_analysis}\n\n")

                f.write("ğŸ“š DETAILNÃ VÃSLEDKY:\n")
                f.write(f"-" * 40 + "\n")
                for i, res in enumerate(result.results, 1):
                    if isinstance(res, dict):
                        f.write(f"{i}. {res.get('title', 'Bez nÃ¡zvu')}\n")
                        if 'source' in res:
                            f.write(f"   Zdroj: {res['source']}\n")
                        if 'url' in res:
                            f.write(f"   URL: {res['url']}\n")
                        if 'summary' in res:
                            f.write(f"   Souhrn: {res['summary']}\n")
                        f.write("\n")

    async def _get_model_info(self) -> Dict:
        """ZÃ­skÃ¡ informace o aktuÃ¡lnÄ› pouÅ¾Ã­vanÃ½ch modelech"""
        try:
            async with M1OptimizedLlamaClient() as client:
                return client.get_model_stats()
        except Exception:
            return {"error": "Model info nedostupnÃ©"}

    async def test_llama_connection(self):
        """Otestuje pÅ™ipojenÃ­ k Llama 3.1 8B a fallback modelÅ¯m"""
        print("ğŸ§  Testuji Llama 3.1 8B ecosystem...")

        tests = [
            ("Llama 3.1 8B (high-quality)", "Analyzuj souÄasnÃ© trendy v AI", 100, True),
            ("Auto-select model", "Co je machine learning?", 50, False),
            ("Fallback test", "RychlÃ¡ odpovÄ›Ä", 30, False)
        ]

        results = []

        for test_name, query, max_tokens, require_quality in tests:
            print(f"\nğŸ§ª Test: {test_name}")
            try:
                start_time = time.time()
                response = await quick_ai_query(
                    query,
                    max_tokens=max_tokens,
                    require_quality=require_quality
                )

                test_time = time.time() - start_time

                if response and "âŒ" not in response:
                    print(f"âœ… {test_name}: {test_time:.2f}s")
                    print(f"   ğŸ“ OdpovÄ›Ä: {response[:100]}...")
                    results.append(True)
                else:
                    print(f"âŒ {test_name}: NeplatnÃ¡ odpovÄ›Ä")
                    results.append(False)

            except Exception as e:
                print(f"âŒ {test_name}: {e}")
                results.append(False)

        success_rate = sum(results) / len(results) * 100
        print(f"\nğŸ“Š CelkovÃ¡ ÃºspÄ›Å¡nost: {success_rate:.0f}%")

        if success_rate >= 66:
            print("âœ… Llama 3.1 8B ecosystem je funkÄnÃ­!")
            return True
        else:
            print("âš ï¸ NÄ›kterÃ© testy selhaly. Zkontroluj konfiguraci.")
            return False

    def show_enhanced_system_info(self):
        """ZobrazÃ­ enhanced systÃ©movÃ© informace vÄetnÄ› model stats"""
        print("ğŸ–¥ï¸  Enhanced SystÃ©movÃ© Informace - Llama 3.1 8B Edition")
        print("=" * 60)

        try:
            import psutil

            # CPU info s M1 optimalizacemi
            cpu_count = psutil.cpu_count()
            cpu_usage = psutil.cpu_percent(interval=1)
            print(f"   ğŸ–¥ï¸  CPU: {cpu_count} jader, {cpu_usage}% vyuÅ¾itÃ­")

            # Memory info s doporuÄenÃ­mi pro Llama
            memory = psutil.virtual_memory()
            memory_total = memory.total / (1024**3)
            memory_available = memory.available / (1024**3)
            memory_percent = memory.percent

            print(f"   ğŸ’¾ RAM: {memory_available:.1f}GB volnÃ© z {memory_total:.1f}GB ({memory_percent}% vyuÅ¾ito)")

            # DoporuÄenÃ­ podle RAM
            if memory_available >= 4.5:
                print(f"   âœ… Dostatek RAM pro Llama 3.1 8B")
            elif memory_available >= 3.0:
                print(f"   âš ï¸  OmezenÃ¡ RAM - doporuÄuji menÅ¡Ã­ modely")
            else:
                print(f"   âŒ NÃ­zkÃ¡ RAM - pouÅ¾ij pouze Phi-3 Mini")

            # Disk info
            disk = psutil.disk_usage('/')
            disk_free = disk.free / (1024**3)
            disk_total = disk.total / (1024**3)
            print(f"   ğŸ’¿ Disk: {disk_free:.1f}GB volnÃ© z {disk_total:.1f}GB")

        except Exception as e:
            print(f"   âŒ Nelze zÃ­skat systÃ©movÃ© informace: {e}")

        # Cache info
        try:
            cache_manager = get_cache_manager()
            cache_stats = cache_manager.get_stats()
            print(f"   ğŸ’¾ Cache: {cache_stats['size']} poloÅ¾ek, {cache_stats['hit_rate']} hit rate")
            print(f"   ğŸ“Š Cache memory: {cache_stats.get('memory_usage_mb', 0):.1f}MB")
        except Exception as e:
            print(f"   ğŸ’¾ Cache: NedostupnÃ¡ ({e})")

        # Model info (async - zobrazÃ­me placeholder)
        print(f"   ğŸ§  AI Models: Llama 3.1 8B + fallbacks (spusÅ¥ --test-ai pro detaily)")

async def main():
    """HlavnÃ­ funkce s enhanced CLI rozhranÃ­m"""
    parser = argparse.ArgumentParser(
        description="Llama 3.1 8B MacBook Research Tool - High-Quality AI Research"
    )

    parser.add_argument("query", nargs="?", help="Research dotaz")
    parser.add_argument("--strategy", choices=["fast", "balanced", "thorough"],
                       default="balanced", help="Strategie vÃ½zkumu")
    parser.add_argument("--sources", nargs="+",
                       choices=["wikipedia", "pubmed", "openalex"],
                       help="KonkrÃ©tnÃ­ zdroje")
    parser.add_argument("--require-quality", action="store_true",
                       help="Vynutit high-quality mode (Llama 3.1 8B)")
    parser.add_argument("--output", help="Soubor pro uloÅ¾enÃ­ vÃ½sledkÅ¯")
    parser.add_argument("--test-ai", action="store_true",
                       help="Otestovat Llama 3.1 8B ecosystem")
    parser.add_argument("--system-info", action="store_true",
                       help="Zobrazit enhanced systÃ©movÃ© informace")
    parser.add_argument("--interactive", action="store_true",
                       help="InteraktivnÃ­ mÃ³d s pokroÄilÃ½mi funkcemi")

    args = parser.parse_args()

    if not IMPORTS_OK:
        print("âŒ Nelze pokraÄovat kvÅ¯li chybÃ¡m pÅ™i importu")
        return 1

    # VytvoÅ™ tool
    tool = LlamaResearchTool()

    # Test AI pokud poÅ¾adovÃ¡no
    if args.test_ai:
        success = await tool.test_llama_connection()
        return 0 if success else 1

    # Enhanced systÃ©movÃ© info
    if args.system_info:
        tool.show_enhanced_system_info()
        return 0

    # Research dotaz
    if args.query:
        try:
            await tool.research_query(
                query=args.query,
                strategy=args.strategy,
                sources=args.sources,
                require_quality=args.require_quality,
                output_file=args.output
            )
            return 0
        except Exception as e:
            print(f"âŒ Chyba pÅ™i research: {e}")
            return 1
    elif args.interactive:
        # Enhanced interaktivnÃ­ mÃ³d
        print("ğŸ¯ Llama 3.1 8B Research Tool - Enhanced InteraktivnÃ­ MÃ³d")
        print("ğŸ’¡ PÅ™Ã­kazy: 'exit', 'help', 'stats', 'clear', 'models'")

        while True:
            try:
                query = input("\nğŸ§  Research dotaz (Llama 3.1 8B): ").strip()

                if query.lower() in ['exit', 'quit', 'q']:
                    print("ğŸ‘‹ UkonÄuji...")
                    break
                elif query.lower() in ['help', 'h']:
                    print("""
ğŸ†˜ Enhanced NÃ¡povÄ›da:
   â€¢ Zadej jakÃ½koli vÃ½zkumnÃ½ dotaz pro high-quality analÃ½zu
   â€¢ PÅ™Ã­klady: "AI trendy 2024", "quantum computing applications"
   â€¢ PÅ™Ã­kazy: 
     - 'exit' (ukonÄit)
     - 'stats' (systÃ©movÃ© info)
     - 'clear' (vyÄistit cache)
     - 'models' (model statistiky)
     - 'quality on/off' (zapnout/vypnout quality mode)
   â€¢ Pro komplexnÃ­ dotazy je automaticky aktivovÃ¡n Llama 3.1 8B
                    """)
                    continue
                elif query.lower() == 'stats':
                    tool.show_enhanced_system_info()
                    continue
                elif query.lower() == 'models':
                    await tool.test_llama_connection()
                    continue
                elif query.lower() == 'clear':
                    cache_manager = get_cache_manager()
                    cache_manager.clear()
                    print("ğŸ—‘ï¸  Cache vyÄiÅ¡tÄ›na")
                    continue
                elif not query:
                    continue

                # SpusÅ¥ enhanced research
                await tool.research_query(
                    query,
                    strategy="balanced",
                    require_quality=False  # Auto-detect
                )

            except KeyboardInterrupt:
                print("\nğŸ‘‹ UkonÄuji...")
                break
            except Exception as e:
                print(f"âŒ Chyba: {e}")

        return 0
    else:
        # Zobraz help a model info
        parser.print_help()
        print(f"\nğŸ§  DostupnÃ© AI modely:")
        print(f"   â€¢ Llama 3.1 8B - High-quality research a analÃ½zy")
        print(f"   â€¢ Qwen2 7B - KÃ³dovÃ¡nÃ­ a technickÃ© dotazy")
        print(f"   â€¢ Phi-3 Mini - RychlÃ© odpovÄ›di a fallback")
        print(f"\nğŸ’¡ Quick start: python {sys.argv[0]} 'your research query'")
        return 0

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ UkonÄeno uÅ¾ivatelem")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ KritickÃ¡ chyba: {e}")
        sys.exit(1)
